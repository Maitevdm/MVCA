{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation confound strategies\n",
    "This notebook evaluates the effectiveness of \"confound strategies\", i.e. their effectiveness of removing the influence of a known confound on machine learning analyses of (simulations of) mutivoxel pattern data. This notebook is meant as a supplement to the [article](link). Together with the notebook with [empirical analyses](empirical_analysis_gender_classification.ipynb), it hopefully complements the article and clarifies our methods and results.\n",
    "\n",
    "## Contents\n",
    "- Part A: Generic simulation\n",
    "- Part B: Counterbalancing follow-up simulation\n",
    "- Part C: WDCR/FwCR follow-up simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Bunch of scikit-learn stuff\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.externals import joblib as jl\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Specific statistics-functions\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Misc.\n",
    "from tqdm import tqdm_notebook\n",
    "from copy import deepcopy\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom code! (install skbold by `pip install skbold`; counterbalance.py is in cwd)\n",
    "from skbold.preproc import ConfoundRegressor\n",
    "from counterbalance import CounterbalancedStratifiedSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A: generic simulation\n",
    "In this notebook's simulations, we are going to simulate three variables, with $N$ denoting the number of samples, and $K$ denoting the number of features:\n",
    "\n",
    "- **y**: an $N\\times 1$ vector with values {0, 1}\n",
    "- **c**: an $N\\times 1$ vector with values {0, 1} if continuous, else $c \\sim \\mathcal{N}(0,\\,1)$\n",
    "- **X**: an $N\\times K$ matrix, in each each column $j$ is generated as $X_{j} = \\beta_{1} c + \\beta_{2} y + \\epsilon,\\ \\epsilon \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "By tweaking the parameters $\\beta c$ and $\\beta y$, we can specifically \"tune\" how much variance of $y$ is explained by the \"true\" (i.e., unconfounded) signal (which we call $signal\\ R^2$) and how much variance of $y$ is explained by the \"confounded signal\" (i.e., signal in $X$ which is confounded by $c$; we call this $confound\\ R^2$). \n",
    "\n",
    "Basically, in our data-generator (function is defined below) we generate data ($X$, $y$, and $c$) with a specified correlation between $y$ and $c$, $\\rho(cy)$, and subsequently calculate the $confound\\ R^2$ and $signal\\ R^2$ terms. We then check whether these terms correspond to the desired values for these terms; if not, we adjust the generative parameters ($\\beta_{1}$ and $\\beta_{2}$) slightly and try the entire process again until we get the terms that we want.\n",
    "\n",
    "Below, we visualized the $confound\\ R^2$ and $signal\\ R^2$ terms in a Venn-diagram. On the right of the diagram, we describe the steps to get these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAAI9CAYAAAA5EXwqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe47307e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mappings = {'100': 'A', '110': 'D', '010': '',\n",
    "            '101': \"B\\n($signal\\ R^2$)\", '111': 'C\\n($confound\\ R^2$)', '011': 'E',\n",
    "            '001': ''}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "v = venn3(subsets=(1, 1, 0.1, 1, 0.3, 0.1, 0.6), set_labels=('y', 'c', 'X'))\n",
    "\n",
    "for key, value in mappings.items():\n",
    "    v.get_label_by_id(key).set_text(value)\n",
    "    v.get_label_by_id(key).set_size(15)\n",
    "    \n",
    "for label in ['A', 'B', 'C']:\n",
    "    v.get_label_by_id(label).set_size(50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions\n",
    "Below, we define our main data-generation class - `DataGenerator` - which generates three variables - $X$, $y$, $c$ - corresponding to our predictors, target, and confound, respectively. (But when setting `c=None`, you can also only simulate $X$ and $y$ *without* $c$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_r2\n",
    "import time\n",
    "\n",
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self, N, K, corr_cy, signal_r2, confound_r2=None,\n",
    "                 c_type='continuous', y_type='binary', tolerance=0.01,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int\n",
    "            Number of samples (N) in the data (X, y, and c)\n",
    "        K : int\n",
    "            Number of features (K) in the data (X)\n",
    "        c_type : str\n",
    "            Type of confound; either \"continuous\" or \"binary\". If binary,\n",
    "            the data a balanced vector with ones and zeros\n",
    "        y_type : str\n",
    "            Type of target; either \"continuous\" or \"binary\".\n",
    "        corr_cy : float\n",
    "            Number between -1 and 1, specifying the correlation\n",
    "            between the confound (c) and the target (y)\n",
    "        signal_r2 : float\n",
    "            Number between 0 and 1, specifying the explained variance\n",
    "            of y using X, independent of the confound contained in X;\n",
    "            (technically, the semipartial correlation rho(xy.c)\n",
    "        confound_r2 : float or None\n",
    "            Number between 0 and 1 (or None), specifying the shared variance \n",
    "            explained of y of x and c (i.e. the explained variance \n",
    "            of the confound-related information in x). If None,\n",
    "            no confound R2 will be left unspecified (which can be used\n",
    "            to specify a baseline).\n",
    "        tolerance : float\n",
    "            How much an observed statistic (corr_cy, signal_r2, confound_r2) may\n",
    "            deviate from the desired value.\n",
    "        verbose : bool\n",
    "            Whether to print (extra) relevant information\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.corr_cy = corr_cy\n",
    "        self.signal_r2 = signal_r2\n",
    "        self.confound_r2 = confound_r2\n",
    "        self.c_type = c_type\n",
    "        self.y_type = y_type\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "            \n",
    "    def generate(self):\n",
    "        \"\"\" Generates X, y, and (optionally) c. \"\"\"\n",
    "        self._check_settings()\n",
    "        self._init_y_and_c()\n",
    "    \n",
    "        # Define X as a matrix of N-samples by K-features\n",
    "        X = np.zeros((self.N, self.K))\n",
    "\n",
    "        # Pre-allocate arrays for average signal_r2 values and confound_r2 values\n",
    "        signal_r2_values = np.zeros(self.K)\n",
    "        confound_r2_values = np.zeros(self.K)\n",
    "\n",
    "        icept = np.ones((self.N, 1))\n",
    "        iterator = tqdm_notebook(np.arange(self.K)) if self.verbose else np.arange(self.K)\n",
    "        \n",
    "        for i in iterator:\n",
    "            \n",
    "            should_continue = False\n",
    "            # Define generative parameters (gen_beta_y = beta-parameter for y in model of X)\n",
    "            gen_beta_y, gen_beta_c = 1, 1\n",
    "            noise_factor = 1\n",
    "            this_c = 0 if self.confound_r2 is None else self.c\n",
    "            tmp_confound_r2 = 0 if self.confound_r2 is None else self.confound_r2\n",
    "            \n",
    "            c_iter = 0\n",
    "            start_time = time.time()\n",
    "            while True:\n",
    "                \n",
    "                this_time = time.time()\n",
    "                if c_iter > 100000:\n",
    "                    gen_beta_y, gen_beta_c, noise_factor = 1, 1, 1\n",
    "                    c_iter = 0\n",
    "                    \n",
    "                if (start_time * 1000 - this_time * 1000) > 10:\n",
    "                    print(\"Something's wrong\")\n",
    "                    print(\"C: %.3f, y: %.3f, noise: %.3f\" % (gen_beta_y, gen_beta_c, noise_factor))\n",
    "                # Generate X as a linear combination of y, c, and random noise\n",
    "                this_X = (gen_beta_y * self.y + gen_beta_c * this_c + np.random.randn(self.N) * noise_factor)\n",
    "                r2_X = pearsonr(this_X, self.y)[0] ** 2\n",
    "    \n",
    "                difference_obs_vs_desired = r2_X - (self.signal_r2 + tmp_confound_r2)\n",
    "                if np.abs(difference_obs_vs_desired) > self.tolerance:  # should be even more strict\n",
    "                    # If correlation too small/big, adjust noise factor and CONTINUE\n",
    "                    if difference_obs_vs_desired < 0:\n",
    "                        noise_factor -= 0.01\n",
    "                    else:\n",
    "                        noise_factor += 0.01\n",
    "                    c_iter += 1\n",
    "                    continue\n",
    "                \n",
    "                if self.confound_r2 is None and not should_continue:\n",
    "                    signal_r2_values[i] = r2_X\n",
    "                    X[:, i] = this_X\n",
    "                    break\n",
    "\n",
    "                c_tmp = np.hstack((icept, this_c[:, np.newaxis]))\n",
    "                X_not_c = this_X - c_tmp.dot(np.linalg.lstsq(c_tmp, this_X, rcond=None)[0])\n",
    "                this_signal_r2 = pearsonr(X_not_c, self.y)[0] ** 2\n",
    "                this_confound_r2 = r2_X - this_signal_r2\n",
    "\n",
    "                difference_obs_vs_desired = this_confound_r2 - self.confound_r2\n",
    "                if np.abs(difference_obs_vs_desired) > self.tolerance:\n",
    "                    if difference_obs_vs_desired < 0:\n",
    "                        gen_beta_c += 0.01\n",
    "                    else:\n",
    "                        gen_beta_c -= 0.01\n",
    "                    should_continue = True\n",
    "                else:\n",
    "                    should_continue = False\n",
    "\n",
    "                difference_obs_vs_desired = this_signal_r2 - self.signal_r2\n",
    "                if np.abs(difference_obs_vs_desired) > self.tolerance:\n",
    "                    if difference_obs_vs_desired < 0:\n",
    "                        gen_beta_y += 0.01\n",
    "                    else:\n",
    "                        gen_beta_y -= 0.01\n",
    "                    should_continue = True\n",
    "                else:\n",
    "                    should_continue = False\n",
    "                    \n",
    "                if should_continue:\n",
    "                    c_iter += 1\n",
    "                    continue\n",
    "                else:  # We found it!\n",
    "                    X[:, i] = this_X\n",
    "                    signal_r2_values[i] = this_signal_r2\n",
    "                    confound_r2_values[i] = this_confound_r2\n",
    "                    break\n",
    "        self.X = X\n",
    "        self.signal_r2_values = signal_r2_values\n",
    "        self.confound_r2_values = confound_r2_values\n",
    "        if self.verbose:\n",
    "            self._generate_report()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def return_vals(self):\n",
    "        \"\"\" Returns X, y, and (optionally) c. \"\"\"\n",
    "        if self.confound_r2 is not None:\n",
    "            return self.X, self.y, self.c\n",
    "        else:\n",
    "            return self.X, self.y\n",
    "\n",
    "    def _generate_report(self):\n",
    "        \"\"\" If verbose, prints some stuff to check. \"\"\"\n",
    "        print(\"Signal r2: %.3f\" % self.signal_r2_values.mean())\n",
    "        \n",
    "        if self.confound_r2 is not None:\n",
    "            print(\"Confound r2: %.3f\" % self.confound_r2_values.mean())\n",
    "        \n",
    "        if self.confound_r2 is not None:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(np.corrcoef(self.X.T), aspect='auto', cmap='RdBu')\n",
    "            plt.title(\"Correlations between features\")\n",
    "            plt.colorbar()\n",
    "            plt.grid('off')\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.title(\"Signal R2 values\")\n",
    "            plt.hist(self.signal_r2_values, bins='auto')\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.title(\"Confound R2 values\")\n",
    "            plt.hist(self.confound_r2_values, bins='auto')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def _check_settings(self):\n",
    "        \"\"\" Some checks of sensible parameters. \"\"\"\n",
    "        if self.N % 2 != 0:\n",
    "            raise ValueError(\"Please select an even number of samples \"\n",
    "                             \"(Makes things easier.)\")\n",
    "\n",
    "        if self.confound_r2 is not None:\n",
    "            if np.abs(self.corr_cy) < np.sqrt(self.confound_r2):\n",
    "                raise ValueError(\"The desired corr_cy value is less than the square \"\n",
    "                                 \"root of the desired confound R-squared ... This is \"\n",
    "                                 \"impossible to generate.\")\n",
    "                \n",
    "        VAR_TYPES = ['binary', 'continuous']\n",
    "        if self.y_type not in VAR_TYPES:\n",
    "            raise ValueError(\"y_type must be one of %r\" % VAR_TYPES)\n",
    "            \n",
    "        if self.c_type not in VAR_TYPES:\n",
    "            raise ValueError(\"c_type must be one of %r\" % VAR_TYPES)\n",
    "            \n",
    "    def _init_y_and_c(self):\n",
    "        \"\"\" Initializes y and c. \"\"\" \n",
    "        if self.y_type == 'binary':\n",
    "            y = np.repeat([0, 1], repeats=self.N / 2)\n",
    "        else:  # assume continuous\n",
    "            y = np.random.normal(0, 1, self.N)\n",
    "        \n",
    "        if self.c_type == 'binary':\n",
    "            if self.y_type == 'binary':\n",
    "                # Simply shift (\"roll\") y to create correlation using the \"formula\":\n",
    "                # to-shift = N / 4 * (1 - corr_cy)\n",
    "                to_roll = int((self.N / 4) * (1 - self.corr_cy))\n",
    "                c = np.roll(y, to_roll)\n",
    "            else:  # y is continuous\n",
    "                c = y.copy()\n",
    "                this_corr_cy = pearsonr(c, y)[0]\n",
    "                i = 0\n",
    "                while np.abs(this_corr_cy - self.corr_cy) > self.tolerance:\n",
    "                    np.shuffle(c)\n",
    "                    this_corr_cy = pearsonr(c, y)\n",
    "                    i += 1\n",
    "                    \n",
    "                    if i > 10000:\n",
    "                        raise ValueError(\"Probably unable to find good corr_cy value\")\n",
    "        else:\n",
    "            # If c is continuous, just sample y + random noise\n",
    "            noise_factor = 10\n",
    "            c = y + np.random.randn(self.N) * noise_factor\n",
    "            this_corr_cy = pearsonr(c, y)[0]\n",
    "\n",
    "            i = 0\n",
    "            while np.abs(this_corr_cy - self.corr_cy) > self.tolerance:\n",
    "                # Decrease noise if the difference is too big\n",
    "                noise_factor -= 0.01\n",
    "                c = y + np.random.randn(self.N) * noise_factor\n",
    "                this_corr_cy = pearsonr(c, y)[0]\n",
    "                i += 1\n",
    "\n",
    "                if i > 10000:\n",
    "                    # Reset noise factor\n",
    "                    noise_factor = 10\n",
    "                    i = 0\n",
    "\n",
    "        self.y = y\n",
    "        self.c = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our data generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efe75abf32e42efaf0f3e3d37f1b99b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Signal r2: 0.004\n",
      "Confound r2: 0.198\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABC4AAAFgCAYAAAB0RTgdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMzMzMzMassDF2ZmZmZmZmZWW/8D/ampXBeln64AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe448028940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (200, 5)\n",
      "y shape: (200,)\n",
      "c shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "data_gen = DataGenerator(N=200, K=5, corr_cy=0.65, signal_r2=0.003,\n",
    "                         confound_r2=0.2, c_type='continuous', y_type='binary',\n",
    "                         tolerance=0.003, verbose=True)\n",
    "data_gen.generate()\n",
    "\n",
    "X, y, c = data_gen.return_vals()\n",
    "print(\"X shape: %r\" % (X.shape,))\n",
    "print(\"y shape: %r\" % (y.shape,))\n",
    "print(\"c shape: %r\" % (c.shape,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define several functions to quickly run classification analyses using different confound-strategies we're evaluating in this notebook:\n",
    "\n",
    "- **None**: just run a classification analysis (predict $y$ from $X$) without treating the confound ($c$)\n",
    "- **WDCR** (whole-dataset confound regression): regress out $c$ from $X$ on the whole dataset, then run a (cross-validated) classification analysis\n",
    "- **FwCR** (foldwise confound regression): first regress out $c$ from $X$ per fold, then run a classification analysis (predict $y$ from $X$)\n",
    "- **CB** (counterbalance): first subsample $X$ and $y$ until $\\rho(yc) \\approx 0$, then run a classification analysis (predict $y$ from $X$; also, it's made sure that $\\rho(yc) = 0$ also in *each fold*!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import vectorized_semipartial_corr\n",
    "\n",
    "def run_without_confound_control(X, y, c, pipeline, cv, arg_dict):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['None'] * n_splits\n",
    "    results['score'] = cross_val_score(estimator=pipeline, X=X, y=y, cv=cv, scoring='f1_macro')\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_with_ipw(X, y, c, pipeline, cv, arg_dict):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['IPW'] * n_splits\n",
    "    \n",
    "    y_ohe = OneHotEncoder(sparse=False).fit_transform(y[:, np.newaxis])\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    lr = LogisticRegression(class_weight='balanced')\n",
    "    \n",
    "    if c.ndim == 1:\n",
    "        c = c[:, np.newaxis]\n",
    "    \n",
    "    tmp_scores = np.zeros(n_splits)\n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        lr.fit(c[train_idx], y[train_idx])\n",
    "        probas = lr.predict_proba(c[train_idx])\n",
    "        weights = 1 / (probas * y_ohe[train_idx]).sum(axis=1)\n",
    "        pipeline.fit(X[train_idx], y[train_idx], clf__sample_weight=weights)\n",
    "        preds = pipeline.predict(X[test_idx])\n",
    "        tmp_scores[i] = f1_score(y[test_idx], preds, average='macro')\n",
    "    \n",
    "    results['score'] = tmp_scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_with_counterbalancing(X, y, c, pipeline, cv, arg_dict, verbose=False,\n",
    "                              c_type='categorical', metric='corr', threshold=0.05, use_pval=True):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    "\n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['CB'] * n_splits\n",
    "\n",
    "    results_corr = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(arg_dict['K']*2)])\n",
    "    results_corr['ki'] = np.tile(np.arange(arg_dict['K']), reps=2)\n",
    "    results_corr['before_after'] = ['before'] * arg_dict['K'] + ['after'] * arg_dict['K']\n",
    "    \n",
    "    corrs_xc_before = vectorized_semipartial_corr(c, y, X, which='2D')\n",
    "    corrs_xy_before = vectorized_semipartial_corr(y, c, X, which='2D')\n",
    "    \n",
    "    skf = CounterbalancedStratifiedSplit(X, y, c, n_splits=cv, c_type=c_type, verbose=verbose)\n",
    "    try:\n",
    "        skf.check_counterbalance_and_subsample()\n",
    "    except ValueError as ve:\n",
    "        results['score'] = np.zeros(n_splits)\n",
    "        corrs_xc_after = np.zeros_like(corrs_xc_before)\n",
    "        corrs_xy_after = np.zeros_like(corrs_xy_before)\n",
    "        results_corr['cx'] = np.concatenate((corrs_xc_before, corrs_xc_after))\n",
    "        results_corr['xy'] = np.concatenate((corrs_xy_before, corrs_xy_after))\n",
    "        return results, results_corr\n",
    "\n",
    "    Xn, yn, cn = X[skf.subsample_idx], y[skf.subsample_idx], c[skf.subsample_idx]    \n",
    "    corrs_xc_after = vectorized_semipartial_corr(cn, yn, Xn, which='2D')\n",
    "    corrs_xy_after = vectorized_semipartial_corr(yn, cn, Xn, which='2D')\n",
    "    \n",
    "    results_corr['cx'] = np.concatenate((corrs_xc_before, corrs_xc_after))\n",
    "    results_corr['xy'] = np.concatenate((corrs_xy_before, corrs_xy_after))\n",
    "    results_corr['subsample_perc'] = [(Xn.shape[0] - X.shape[0]) / X.shape[0]] * X.shape[1] * 2\n",
    "    results['score'] = cross_val_score(estimator=pipeline, X=Xn, y=yn, cv=skf, scoring='f1_macro')\n",
    "    return results, results_corr\n",
    "\n",
    "\n",
    "def run_with_wholedataset_confound_regression(X, y, c, pipeline, cv, arg_dict):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['WDCR'] * n_splits\n",
    "    \n",
    "    # Regress out c from X\n",
    "    cr = ConfoundRegressor(X=X, confound=c, cross_validate=True)\n",
    "    X_corr = cr.fit_transform(X)\n",
    "    results['score'] = cross_val_score(estimator=pipeline, X=X_corr, y=y, cv=cv, scoring='f1_macro')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_with_foldwise_confound_regression(X, y, c, pipeline, cv, arg_dict):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : nu1mpn_sampy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : intn_sampn_samp\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    " \n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['FwCR'] * n_splits\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    scores = np.zeros(n_splits)\n",
    "    \n",
    "    cfr = ConfoundRegressor(X=X, confound=c, cross_validate=True)\n",
    "    this_pipe = deepcopy(pipeline).steps\n",
    "    this_pipe.insert(0, ('regress', cfr))\n",
    "    this_pipe = Pipeline(this_pipe)\n",
    "    results['score'] = cross_val_score(estimator=this_pipe, X=X, y=y, cv=cv, scoring='f1_macro')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed parameters / settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "n_splits = 10  # i.e. 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: what happens when we vary *confound $R^2$* and *$\\rho(cy)$*?\n",
    "... or, in other words, what happens when we increase the amount of confounded \"signal\" in X (area *C* in the venn-diagram) and the initial correlation between the confound (c) and the target (y)?\n",
    "\n",
    "We evaluate the following values for the parameter $\\rho(cy)$:\n",
    "- $\\rho(cy) \\in \\{0.25, 0.3, 0.35..., 0.85\\}$\n",
    "\n",
    "We set the range of values (in steps of 0.05) for $confound\\ R^2$ dynamically, because the maximum value for $confound\\ R^2$ depends on the $\\rho(cy)$ parameter. For example, we cannot set $confound\\ R^2 = 0.5$ when $\\rho(cy) = 0.6$, because $\\sqrt{0.5} > 0.6$. Thus:\n",
    "\n",
    "- $confound\\ R^2 \\in \\{0.00, 0.05, ..., \\rho(cy)^2 - (\\rho(cy)^2\\ mod\\ 0.05)\\}$\n",
    "\n",
    "And we'll use the following fixed parameters:\n",
    "- $N = 200$\n",
    "- $K = 5$\n",
    "- $signal\\ R^2=0.1$\n",
    "\n",
    "Importantly, we also track the correlations $\\rho(xy)$ and $\\rho(xc)$ before and after counterbalancing, because there is something weird going on (as we'll explain later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do it three times for robustness\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "simulations = 5\n",
    "\n",
    "# Specify arguments for data generations (we'll set corr_cy and confound_r2 later)\n",
    "data_args = dict(N=200, K=5, c_type='continuous', y_type='binary',\n",
    "                 corr_cy=None, signal_r2=0.004, confound_r2=None,\n",
    "                 verbose=False, tolerance=0.001)\n",
    "\n",
    "# The values for corr_cy to loop over\n",
    "corr_cy_vec = [0.65]\n",
    "\n",
    "# The confound_r2 values to loop over\n",
    "confound_r2_vecs = [np.arange(0, corr_cy**2 - 0.05, 0.05)\n",
    "                    for corr_cy in corr_cy_vec]\n",
    "\n",
    "signal_r2_vec = [0.004, 0.1]\n",
    "\n",
    "# The 'reference performance' to keep track of (scores on data generated without\n",
    "# any influence of the confound)\n",
    "reference_performance = np.zeros((simulations, len(signal_r2_vec)))\n",
    "\n",
    "results_gen_sim = []\n",
    "results_corr_gen_sim = []\n",
    "\n",
    "# Loop over simulations\n",
    "for sim in np.arange(simulations):\n",
    "\n",
    "    print(\"Simulation: %i\" % (sim + 1))\n",
    "    cv = StratifiedKFold(n_splits=n_splits)\n",
    "    # Loop over values for corr_cy\n",
    "    for i, signal_r2 in enumerate(signal_r2_vec):\n",
    "        data_args.update(signal_r2=signal_r2)\n",
    "        data_args.update(confound_r2=None)\n",
    "        data_args.update(corr_cy=0)\n",
    "        \n",
    "        dgen = DataGenerator(**data_args)\n",
    "        Xref, yref = dgen.generate().return_vals() \n",
    "        reference_performance[sim, i] = cross_val_score(pipeline, Xref, yref, cv=cv).mean()\n",
    "        \n",
    "        print(\"Signal r2: %.3f\" % signal_r2)\n",
    "        for ii, corr_cy in enumerate(corr_cy_vec):\n",
    "            data_args.update(corr_cy=corr_cy)\n",
    "            data_args.update(confound_r2=None)\n",
    "            \n",
    "            confound_r2_vec = confound_r2_vecs[ii]\n",
    "            print(\"corr_cy: %.3f, confound_r2: [ \" % corr_cy, end='')\n",
    "\n",
    "            # Loop over values for confound_r2\n",
    "            for iii, confound_r2 in enumerate(confound_r2_vec):\n",
    "                print('%.3f' % confound_r2 + ' ', end='')\n",
    "                data_args.update(confound_r2=confound_r2)\n",
    "                dgen = DataGenerator(**data_args)\n",
    "                X, y, c = dgen.generate().return_vals()\n",
    "                results_gen_sim.append(run_without_confound_control(X, y, c, pipeline, cv, data_args))\n",
    "                results_gen_sim.append(run_with_wholedataset_confound_regression(X, y, c, pipeline, cv, data_args))\n",
    "                results_gen_sim.append(run_with_foldwise_confound_regression(X, y, c, pipeline, cv, data_args))    \n",
    "                # results_sce1.append(run_with_ipw(X, y, c, pipeline, n_splits, data_args))    \n",
    "                \n",
    "                res, corrs = run_with_counterbalancing(X, y, c, pipeline, n_splits, c_type='continuous',\n",
    "                                                       arg_dict=data_args)\n",
    "                results_gen_sim.append(res)\n",
    "                results_corr_gen_sim.append(corrs)\n",
    "            print(']')\n",
    "            \n",
    "results_gen_sim_df = pd.concat(results_gen_sim)\n",
    "results_corrs_gen_sim_df = pd.concat(results_corr_gen_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks')\n",
    "\"\"\"\n",
    "color_baseline = sns.color_palette(\"Set2\", 10)[7]\n",
    "color_cb = sns.color_palette(\"Set2\")[2]\n",
    "color_wdcr = sns.color_palette(\"Set2\")[3]\n",
    "color_fwcr = sns.color_palette(\"Set2\")[4]\n",
    "palette = [color_baseline, color_cb, color_fwcr, color_wdcr]\n",
    "\n",
    "custom_legend_labels = ['None', 'CB', 'FwCR', 'WDCR', 'Chance level performance', 'Brain size only performance']\n",
    "custom_legend_handles = [plt.Line2D((0, 1), (0, 0), color=color_baseline, marker='o', lw=3),\n",
    "                         plt.Line2D((0, 1), (0, 0), color=color_cb, marker='o', lw=3),\n",
    "                         plt.Line2D((0, 1), (0, 0), color=color_wdcr, marker='o', lw=3),\n",
    "                         plt.Line2D((0, 1), (0, 0), color=color_fwcr, marker='o', lw=3),\n",
    "                         plt.Line2D((0, 1), (0, 0), color='k', ls='--', lw=3),\n",
    "                         plt.Line2D((0, 1), (0, 0), color='orange', ls='--', lw=3)]\n",
    "\"\"\"\n",
    "custom_legend_handles = [plt.Line2D((0, 1), (0, 0), color='k', ls='--', lw=1),\n",
    "                         plt.Line2D((0, 1), (0, 0), color='orange', ls='--', lw=1)]\n",
    "custom_legend_labels = ['Chance level', 'Confound ignored']\n",
    "\n",
    "g = sns.factorplot(x='confound_r2', y='score', hue='method', col='signal_r2',\n",
    "                   data=results_gen_sim_df, palette='colorblind', aspect=2, size=4,\n",
    "                   legend=False, hue_order=['None', 'CB', 'WDCR', 'FwCR'], dodge=0.2)\n",
    "g.axes[0][0].set_title(r\"$Signal\\ R^2: 0.004$\")\n",
    "g.axes[0][1].set_title(r\"$Signal\\ R^2: 0.1$\")\n",
    "legend1 = g.axes[0][0].legend(loc=0)\n",
    "legend2 = g.axes[0][0].legend(custom_legend_handles, custom_legend_labels, loc=1)\n",
    "g.axes[0][0].add_artist(legend1)\n",
    "g.set_yticklabels(np.round(np.arange(0.2, 1.1, 0.1), 1))\n",
    "g.set_xticklabels(np.round(confound_r2_vec, 3), rotation=35)\n",
    "\n",
    "for i in [0, 1]:\n",
    "    g.axes[0][i].axhline(0.5, ls='--', c='black', lw=1)\n",
    "    g.axes[0][i].axhline(reference_performance.mean(axis=0)[i], ls='--', c='orange', lw=1)\n",
    "    g.axes[0][i].set_ylim(0.2, 1)\n",
    "\n",
    "g.axes[0][0].grid(ls='--', lw=0.5)\n",
    "g.axes[0][1].grid(ls='--', lw=0.5)\n",
    "\n",
    "g.set_axis_labels(r'$Confound\\ R^2$', r'$F_{1}\\ score$')\n",
    "g.despine()\n",
    "fig = plt.gcf()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrs_sce1 = pd.concat(results_corr_sce1)\n",
    "g = sns.FacetGrid(df_corrs_sce1, col=\"signal_r2\", hue=\"before_after\",\n",
    "                  palette='colorblind', aspect=2, size=3)\n",
    "g = g.map(sns.barplot, 'confound_r2', 'xy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thisdf = df_sce1[df_sce1.signal_r2 == 0.1]\n",
    "a = thisdf[thisdf.method == 'cb'].groupby('confound_r2').mean()['score'] - thisdf[thisdf.method == 'fwcr'].groupby('confound_r2').mean()['score']\n",
    "thisdf = df_corrs_sce1[df_corrs_sce1.signal_r2 == 0.1]\n",
    "b = thisdf[thisdf.before_after == 'before'].groupby('confound_r2').mean()['xy'] - thisdf[thisdf.before_after == 'before'].groupby('confound_r2').mean()['cx']\n",
    "\n",
    "plt.plot((a - a.mean()) / a.std())\n",
    "plt.plot((b - b.mean()) / b.std())\n",
    "plt.legend(['Overclassification (CB - FwCR)', 'partial(Xy) - partial(Xc)'])\n",
    "\n",
    "pearsonr(a - a.mean(), b - b.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdf = df_corrs_sce1[df_corrs_sce1.signal_r2 == 0.1]\n",
    "a = thisdf[thisdf.before_after == 'before'].groupby('confound_r2').mean()['xy'] - thisdf[thisdf.before_after == 'after'].groupby('confound_r2').mean()['xy']\n",
    "b = thisdf[thisdf.before_after == 'before'].groupby('confound_r2').mean()['xy'] - thisdf[thisdf.before_after == 'before'].groupby('confound_r2').mean()['cx']\n",
    "\n",
    "plt.plot((a - a.mean()) / a.std())\n",
    "plt.plot((b - b.mean()) / b.std())\n",
    "plt.legend(['cor(xy) before - cor(xy) after', 'partial(Xy) - partial(Xc)'])\n",
    "\n",
    "pearsonr(a - a.mean(), b - b.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: counterbalancing follow-up simulation\n",
    "Something weird is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.array([\n",
    "    [1, -.4, +.65],\n",
    "    [-.4, 1, +.4],\n",
    "    [+.65, +.4, 1]\n",
    "])\n",
    "\n",
    "N = 1000\n",
    "iters = 1000\n",
    "orig_cb_diffs = np.zeros(iters)\n",
    "highest_diff = 0\n",
    "\n",
    "for i in tqdm_notebook(range(iters)):\n",
    "\n",
    "    data = np.random.multivariate_normal(np.zeros(3), cov=cov, size=N)\n",
    "    y, X, c = data[:, 0], data[:, 1, np.newaxis], data[:, 2]\n",
    "    y = (y > y.mean()).astype(int)\n",
    "\n",
    "    cbss = CounterbalancedStratifiedSplit(X=X, y=y, c=c, n_splits=n_splits, c_type='continuous',\n",
    "                                          metric='corr', use_pval=True, threshold=0.1)\n",
    "    cbss.check_counterbalance_and_subsample()\n",
    "    idx = cbss.subsample_idx\n",
    "    nidx = list(set(range(y.size)) - set(idx))\n",
    "    nonsel = np.zeros(y.size)\n",
    "    nonsel[nidx] = 1\n",
    "\n",
    "    Xss, yss, css = X[idx], y[idx], c[idx]\n",
    "    o_score = cross_val_score(pipeline, X=X, y=y, cv=10).mean()\n",
    "    cb_score = cross_val_score(pipeline, X=Xss, y=yss, cv=10).mean()\n",
    "    orig_cb_diffs[i] = cb_score - o_score\n",
    "    \n",
    "    if orig_cb_diffs[i] > highest_diff:\n",
    "        best_X, best_y, best_c = X, y, c\n",
    "        best_idx, best_nidx = idx, nidx \n",
    "        highest_diff = orig_cb_diffs[i]\n",
    "        print(\"Highest diff: %.3f\" % highest_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_score = cross_val_score(pipeline, X=best_X, y=best_y, cv=n_splits).mean()\n",
    "cb_score = cross_val_score(pipeline, X=best_X[best_idx], y=best_y[best_idx], cv=n_splits).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "def plot_contours_and_decision_boundary(ax, X, y, clf, cmap, xlim=(-4, 4), ylim=(-4, 4)):\n",
    "    clf.fit(X, y)\n",
    "    w = clf.coef_[0]\n",
    "    x_0 = -clf.intercept_[0] / w[0]\n",
    "    yy = np.linspace(*ylim)\n",
    "    XX, YY = np.mgrid[xlim[0]:xlim[1]:200j, ylim[0]:ylim[1]:200j]\n",
    "    Z = clf.predict(XX.ravel()[:, np.newaxis]).reshape(XX.shape)\n",
    "    ax.pcolormesh(XX, YY, Z, cmap=cmap)\n",
    "    ax.axvline(x_0, c='k', ls='--', lw=3)\n",
    "    return ax, x_0\n",
    "\n",
    "sns_map = sns.color_palette('colorblind')\n",
    "cmap = matplotlib.colors.ListedColormap([sns_map[5], sns_map[1]])\n",
    "y0_col, y1_col = cmap(0), cmap(1)\n",
    "\n",
    "custom_legend_handles = [\n",
    "    plt.Line2D((0,1),(0,0), color=cmap(0), marker='o', linestyle='', mec='k', mew=.5, markersize=20),\n",
    "    plt.Line2D((0,1),(0,0), color=cmap(1), marker='o', linestyle='', mec='k', mew=.5, markersize=20),\n",
    "    plt.Line2D((0,1),(0,0), color=cmap(0), marker='o', linestyle='', mec='r', mew=3, markersize=20),\n",
    "    plt.Line2D((0,1),(0,0), color='k', ls='--', lw=3)\n",
    "]\n",
    "\n",
    "custom_legend_labels = ['y = 0', 'y = 1', 'Rejected after subsampling', 'Decision boundary']\n",
    "\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "xlim, ylim = (-4, 4), (-4, 4)\n",
    "\n",
    "ax0 = plt.subplot2grid((13, 3), (0, 0), colspan=2, rowspan=2)\n",
    "ax0 = sns.kdeplot(data=best_X[best_y == 1].squeeze(), shade=True, color=y1_col, ax=ax0)\n",
    "ax0 = sns.kdeplot(data=best_X[best_y == 0].squeeze(), shade=True, color=y0_col, ax=ax0)\n",
    "\n",
    "ax0.set_xlim(xlim)\n",
    "ax0.axis('off')\n",
    "\n",
    "ax1 = plt.subplot2grid((13, 3), (2, 0), rowspan=4, colspan=2)\n",
    "ax1, x_0 = plot_contours_and_decision_boundary(ax=ax1, X=best_X, y=best_y, cmap=cmap, clf=SVC(kernel='linear'))\n",
    "ax1.scatter(best_X[best_idx], best_c[best_idx], c=best_y[best_idx],\n",
    "            cmap=cmap, edgecolor='black', linewidth=.8, s=175)\n",
    "ax1.scatter(best_X[best_nidx], best_c[best_nidx], c=best_y[best_nidx],\n",
    "            cmap=cmap, edgecolor='red', linewidth=0.8, s=175)\n",
    "ax1.text(2, -3.5, \"Accuracy: %.3f\" % orig_score, fontsize=30)\n",
    "ax1.set_ylabel(\"C\", fontsize=40)\n",
    "ax1.set_ylim(ylim)\n",
    "ax1.set_xlim(xlim)\n",
    "ax1.legend(custom_legend_handles, custom_legend_labels, fontsize=25, loc=2)\n",
    "ax1.tick_params(labelsize=20)\n",
    "ax0.axvline(x_0, c='k', ls='--')\n",
    "\n",
    "ax2 = plt.subplot2grid((13, 3), (7, 0), rowspan=4, colspan=2)\n",
    "ax2, x_0 = plot_contours_and_decision_boundary(ax=ax2, X=best_X[best_idx], y=best_y[best_idx], cmap=cmap,\n",
    "                                               clf=SVC(kernel='linear'))\n",
    "ax2.scatter(best_X[best_idx], best_c[best_idx], c=best_y[best_idx],\n",
    "            cmap=cmap, edgecolor='black', linewidth=.8, s=175)\n",
    "ax2.text(2, -3.5, \"Accuracy: %.3f\" % cb_score, fontsize=30)\n",
    "ax2.set_ylabel(\"C\", fontsize=40)\n",
    "ax2.set_ylim(ylim)\n",
    "ax2.set_xlim(xlim)\n",
    "ax2.xaxis.set_ticks_position('top')\n",
    "ax2.tick_params(labelsize=20)\n",
    "\n",
    "ax3 = plt.subplot2grid((13, 3), (11, 0), colspan=2, rowspan=2)\n",
    "ax3 = sns.kdeplot(data=best_X[best_idx][best_y[best_idx] == 1].squeeze(), shade=True, color=y1_col, ax=ax3)\n",
    "ax3 = sns.kdeplot(data=best_X[best_idx][best_y[best_idx] == 0].squeeze(), shade=True, color=y0_col, ax=ax3)\n",
    "ax3.axvline(x_0, c='k', ls='--')\n",
    "ax3.invert_yaxis()\n",
    "ax3.set_xlim(xlim)\n",
    "\n",
    "ax3.axis('off')\n",
    "\n",
    "ax4 = plt.subplot2grid((13, 3), (2, 2), rowspan=4)\n",
    "ax4 = sns.kdeplot(data=best_c[best_y == 1], shade=True, color=y1_col, ax=ax4, vertical=True)\n",
    "ax4 = sns.kdeplot(data=best_c[best_y == 0], shade=True, color=y0_col, ax=ax4, vertical=True)\n",
    "ax4.axis('off')\n",
    "\n",
    "ax5 = plt.subplot2grid((13, 3), (7, 2), rowspan=4)\n",
    "ax5 = sns.kdeplot(data=best_c[best_idx][best_y[best_idx] == 1], vertical=True, color=y1_col, ax=ax5, shade=True)\n",
    "ax5 = sns.kdeplot(data=best_c[best_idx][best_y[best_idx] == 0], vertical=True, color=y0_col, ax=ax5, shade=True)\n",
    "ax5.axis('off')\n",
    "\n",
    "fig.tight_layout(w_pad=-2, h_pad=-1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we quantify this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, chisquare\n",
    "dtb = np.zeros(N)\n",
    "preds = np.zeros(N)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "clf = SVC(kernel='linear')\n",
    "scaler = StandardScaler()\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(best_X, best_y)):\n",
    "    X_train, X_test = best_X[train_idx], best_X[test_idx]\n",
    "    y_train, y_test = best_y[train_idx], best_y[test_idx]\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    clf.fit(X_train, y_train)\n",
    "    dtb[test_idx] = clf.decision_function(X_test)\n",
    "    preds[test_idx] = clf.predict(X_test)\n",
    "    \n",
    "correct = (preds == best_y)\n",
    "\n",
    "nonsel = np.zeros(N)\n",
    "nonsel[best_nidx] = 1\n",
    "y_tmp = best_y.copy()\n",
    "y_tmp[best_y == 0] = -1\n",
    "\n",
    "print(\"Mean retained samples: %.3f (%.3f)\" % ((dtb*y_tmp)[nonsel == 0].mean(), (dtb*y_tmp)[nonsel == 0].std()))\n",
    "print(\"Mean rejected samples: %.3f (%.3f)\" % ((dtb*y_tmp)[nonsel == 1].mean(), (dtb*y_tmp)[nonsel == 1].std()))\n",
    "\n",
    "print(ttest_ind((dtb*y_tmp)[nonsel == 0], (dtb*y_tmp)[nonsel == 1]))\n",
    "observed_count = (correct[nonsel == 0].sum(), correct[nonsel == 1].sum())\n",
    "print(1 - np.array(observed_count) / np.array([(nonsel == 0).sum(), (nonsel == 1).sum()]))\n",
    "print(chisquare(observed_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: WDCR/FwCR follow-up simulations\n",
    "Now, we delve into the issue of below-chance classification we observe when regressing out the confound from the entire dataset at once. We'll demonstrate this issue by some simulations on random (normal) data.\n",
    "\n",
    "Suppose you generate random (normal) data of shape $N\\ (samples) \\times K\\ (features)$  - let's call this $X$ - and a vector with binary values (0 or 1) of shape $N \\times K$ - let's call this $y$. Suppose you then implement a cross-validated analysis, in which you fit a linear model (e.g. a linear SVM) iteratively in a (let's say) 10-fold cross-validation scheme. Now, suppose you simulate this 1000 times. (Note that this is very similar to the permutation statistics people employ to calculate non-parametric p-values for their classification scores.) You expect the distribution of your (e.g.) accuracy scores to be centered around 50%. However, you'll undoubtedly observe some scores below 50%. Can we characterize *why* some random generations lead to below chance classifications while others don't, beyond just saying it's just occuring randomly? Let's try.\n",
    "\n",
    "Remember, linear classifiers are capitalizing on some linear statistical dependence between the features in X and y. This can be (per feature) expressed as the correlation between X and y. When randomly generating data (or, perhaps equivalently, when doing permutation analyses), you will observe a *distribution* of correlations between your features and y. In the following simulations, we'll show you that **relatively narrow distributions of these correlations** (i.e., narrower than you'd expect by chance) **lead to relatively low accuracy scores**. Data corresponding to these relatively narrow distributions are, then, more prone to yield \"oppsing statistical dependencies\" (i.e. flipped signs of correlations) between the train and test-set, resulting in below-chance accuracies. \n",
    "\n",
    "To show this association between (below chance) accuracy and the standard deviation of the distribution of correlations between features and y *in general*, we've written a simple simulation below. We generate random (normal) data for our $X$ matrix and a vector with random (0, 1) values for y for a given `n_sims` simulations. For each simulation, we keep track of accuracy (averaged over `n_folds`), the initial standard deviation of the correlations between $X$ and $y$ (averaged over `k_feat` in X), and the proportion of \"sign-flips\" we observe between the correlation of features and y in the train and test-set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_classify_random_data(n_sims, n_samp, k_feat, n_folds):\n",
    "    \"\"\" Simulate classification of random data.\n",
    "    \n",
    "    Simulate classification of random data, in which X is of\n",
    "    shape (n_samp, k_feat) and consists of values from a random\n",
    "    normal distribution with mean 0 and unit variance.\n",
    "    The target, y, is a vector with binary (values )\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_sims : int\n",
    "        Number of (iterations of) simulations to run.\n",
    "    n_samp : int\n",
    "        Number of samples (preferably an even number to keep the classes in\n",
    "        y balanced).\n",
    "    k_feat : int\n",
    "        Number of features in X.\n",
    "    n_folds : int\n",
    "        Number of folds in a (Repeated) KFold cross-validation scheme \n",
    "        (in which the number of repetitions is set to 10 for robustness).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    acc : dict\n",
    "        A dictionary with keys 'accuracy', 'std_corr' (stdev of initial correlation\n",
    "        between features and y), and 'sign_change' (the proportion of features that \n",
    "        change sign of their correlation with y between train and test)\n",
    "    \"\"\"\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='linear'))\n",
    "    ])\n",
    "\n",
    "    skf = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=10)\n",
    "\n",
    "    results = {'accuracy': [],\n",
    "               'std_corrs': [],\n",
    "               'prop_sign_changes': [],\n",
    "               'weighted_diff_train_test_corrs': []}\n",
    "\n",
    "    acc = np.zeros((n_sims, n_folds * 10))\n",
    "    for i in tqdm_notebook(range(n_sims)):\n",
    "        X = np.random.normal(0, 1, size=(n_samp, k_feat))\n",
    "        y = np.repeat([0, 1], repeats=int(n_samp / 2))\n",
    "        \n",
    "        mean_abs_corr = np.abs(np.corrcoef(np.hstack((X, y[:, np.newaxis])).T))[-1, :-1].mean()\n",
    "        std_corr = np.corrcoef(np.hstack((X, y[:, np.newaxis])).T)[-1, :-1].std()\n",
    "        results['std_corrs'].append(std_corr)\n",
    "        \n",
    "        sign_changes = np.zeros(n_folds * 10)\n",
    "        weighted_diffs = np.zeros(n_folds * 10)\n",
    "        \n",
    "        for ii, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            pipe.fit(X_train, y_train)\n",
    "            acc[i, ii] = pipe.score(X_test, y_test)\n",
    "            \n",
    "            fitted_clf = pipe.get_params(deep=False)['steps'][1][1]\n",
    "            corr_train = np.corrcoef(np.hstack((X_train, y_train[:, np.newaxis])).T)[-1, :-1]\n",
    "            corr_test = np.corrcoef(np.hstack((X_test, y_test[:, np.newaxis])).T)[-1, :-1]\n",
    "            sign_changes[ii] = (np.sign(corr_train) != np.sign(corr_test)).sum() / k_feat\n",
    "            weighted_diffs[ii] = ((corr_train - corr_test) * fitted_clf.coef_).mean()\n",
    "            \n",
    "        results['accuracy'].append(acc[i, :].mean())\n",
    "        results['prop_sign_changes'].append(sign_changes.mean())\n",
    "        results['weighted_diff_train_test_corrs'].append(weighted_diffs.mean())\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we simulate 500 datasets with 100 samples and 50 features, which are analyzed with a linear SVM in a 10-fold cross-validation scheme (repeated 10 times for robustness using [RepeatedStratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold)). We then plot a scatterplot (and linear fit) between accuracy and standard deviation of correlations (left plot), between accuracy and the proportion of sign-flips we observe (middle plot), and the average (across features) difference in correlation between train-set and test-set, weighted by the coefficient from the fit (as a more sophisticated measure of \"sign flipping\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = simulate_and_classify_random_data(n_sims=500, n_samp=100, k_feat=200, n_folds=10)\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1, pval1 = pearsonr(results_df.std_corrs, results_df.accuracy)\n",
    "corr2, pval2 = pearsonr(results_df.weighted_diff_train_test_corrs, results_df.accuracy)\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.regplot(y='accuracy', x='std_corrs', data=results_df)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('St. dev. correlation distribution')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_ylim(.2, 0.8)\n",
    "#ax.text(0.115, 0.7, r'$r(499) = %.3f$' % corr1, fontsize=15)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.regplot(y='accuracy', x='weighted_diff_train_test_corrs', data=results_df)\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('Weighted train/test difference')\n",
    "ax.set_ylabel('')\n",
    "ax.set_ylim(.2, 0.8)\n",
    "#ax.text(0.01, 0.7, r'$r(499) = %.3f$' % corr2, fontsize=15)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, higher initial correlations lead to above chance accuracy, but importantly (absolute) correlations close(r) to zero lead to below chance classifications (upper left plot). \n",
    "\n",
    "This is fine in this simulation, because we can simulate this many times, across which the mean accuracy is about 50% (y-coordinates of centroid in scatterplots). But you also see that below an average standard deviation of correlation across features between X and y of 0.01, below-chance accuracies tend to occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When exactly do you get below-chance accuracy?\n",
    "In the left plot above, you can see that around a standard deviation of 0.1 we get 50% correct (chance-level) and below that standard deviation you get below-chance accuracy. It turns out that this observation makes sense when we relate it the the analytic correlation distribution. The probability density function (PDF; $f(r)$) of any given correlation population parameter $r$ between bivariate normal data of shape $(n)$ is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "f(r) = \\frac{(n - 2)\\mathbf{\\Gamma}(n - 1)(1 - \\rho^2)^{\\frac{n-1}{2}}(1 - r^2)^{\\frac{n-4}{2}}}{\\sqrt{2\\pi}\\mathbf{\\Gamma}(n - \\frac{1}{2})(1 - \\rho r)^{n - \\frac{3}{2}}}\\mathbf{{_2}F_{1}}(\\frac{1}{2}, \\frac{1}{2};\\frac{2n - 1}{2}; \\frac{\\rho r + 1}{2})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement this pdf as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rpdf\n",
    "rpdf??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "rs = np.arange(-1, 1, .001)\n",
    "plt.plot(rpdf(rho=0, n=N, rs=rs))\n",
    "plt.xticks(np.arange(0, 2000, 200), np.round(rs[::200], 1))\n",
    "plt.xlabel(r'$\\rho$')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to determine the expected standard deviation of the pdf of a given value for $\\rho$, we can sample from the corresponding inverted CDF (also known as quantile function) by inputting numbers from a uniform distribution. We can then calculte the standard deviation of the output of the inverted CDF to determine the expected standard deviation for that particular value for $\\rho$. \n",
    "\n",
    "Let's first define and plot the inverted CDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "xs = np.random.uniform(0, 1, 10000)\n",
    "rs = np.arange(-1, 1, .001)\n",
    "ys = rpdf(0, N, rs)\n",
    "cdf = ECDF(ys)50\n",
    "inverted_edf = interp1d(np.cumsum(ys[ys>0])/np.sum(ys[ys>0]), rs[ys>0], bounds_error=False)\n",
    "\n",
    "x = np.arange(-1, 1, .01)\n",
    "y = inverted_edf(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's sample from this inverted CDF with random uniform numbers as inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.uniform(0, 1, 100000)\n",
    "samples_transformed = inverted_edf(samples)\n",
    "\n",
    "plt.hist(samples_transformed)\n",
    "plt.xlabel(\"Observed correlation\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can approximate the expected standard deviation for a given $\\rho$ (here: $\\rho = 0$) by calculating the standard deviation of the observed samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_rho_0 = np.std(samples_transformed)\n",
    "print('Standard deviation when rho = 0: %.3f' % std_rho_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to check out for our randomly generated (normally distributed) data, which indeed was at chance when the standard deviation was 0.1. Given that the pdf of the correlation distribution depends on $N$ (number of samples), let simulate some new data with another value for $N$, let's say 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = simulate_and_classify_random_data(n_sims=500, n_samp=100, k_feat=50, n_folds=10)\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.title('Accuracy & std correlation')\n",
    "sns.regplot(y='std_corrs', x='accuracy', data=results_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = np.linalg.lstsq(np.hstack((np.ones((len(results_df), 1)), results_df.accuracy[:, np.newaxis])),\n",
    "                        results_df.std_corrs)[0]\n",
    "betas[0] + betas[1] * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the expected standard deviation again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "xs = np.random.uniform(0, 1, 10000)\n",
    "rs = np.arange(-1, 1, .001)\n",
    "ys = rpdf(0, N, rs)\n",
    "cdf = ECDF(ys)\n",
    "inverted_edf = interp1d(np.cumsum(ys[ys>0])/np.sum(ys[ys>0]), rs[ys>0], bounds_error=False)\n",
    "\n",
    "samples = np.random.uniform(0, 1, 100000)\n",
    "samples_transformed = inverted_edf(samples)\n",
    "\n",
    "std_rho_0_N50 = np.std(samples_transformed)\n",
    "print('Standard deviation when rho = 0 (and N = 50): %.3f' % std_rho_0_N50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this seems to correspond to the standard deviation in the plot above (x-axis) for when accuracy is at chance (0.5, y-axis)! \n",
    "\n",
    "So, in conclusion: below chance accuracy is expected when (in random normal data) the standard deviation of the correlations between your features and the target (y) is lower than you would expect based on the correlation distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificially narrowing the correlation distribution leads to below-chance accuracy\n",
    "We think exactly this is happening when you regress out your confound on the entire dataset: you artificially narrow the correlation distribution, which leads to an increase in sign-flips and thus negative bias. If the correlation distribution becomes more narrow than expected by chance, below-chance accuracy will follow.\n",
    "\n",
    "We follow-up on this idea with another simulation. Here, we actively control the standard deviation of the correlations between X and y. We test a range of these correlations and show that lower average absolute correlations yield (relatively) more negative bias (i.e. dip more below chance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_classify_data_constrained(n_sims, n_samp, k_feat, n_folds, \n",
    "                                           std_corr=None, verbose=False):\n",
    "    \"\"\" Simulate classification of random data.\n",
    "    \n",
    "    Simulate classification of random data, in which X is of\n",
    "    shape (n_samp, k_feat) and consists of values from a random\n",
    "    normal distribution with mean 0 and unit variance. The data generation\n",
    "    is constrained by 'mean_corr' (the average absolute correlation between\n",
    "    features and y) or 'std_corr' (the standard deviation of correlations between\n",
    "    features and y). The target, y, is a vector with binary values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_sims : int\n",
    "        Number of (iterations of) simulations to run.\n",
    "    n_samp : int\n",
    "        Number of samples (preferably an even number to keep the classes in\n",
    "        y balanced).\n",
    "    k_feat : int\n",
    "        Number of features in X.\n",
    "    n_folds : int\n",
    "        Number of folds in a (Repeated) KFold cross-validation scheme \n",
    "        (in which the number of repetitions is set to 10 for robustness).\n",
    "    std_corr : float or None\n",
    "        If float, the standard deviation of correlations between features and\n",
    "        y will be constrained by this number (+- 0.001 margin of error).\n",
    "        If None, the number for mean_corr will be used to constrain\n",
    "        data generation. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    acc : numpy array (of length n_sims)\n",
    "        Array with average scores (across folds) for n_sims.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If both mean_corr and std_corr are set to None.\n",
    "    \"\"\"\n",
    "     \n",
    "    if verbose:\n",
    "        iterator = tqdm_notebook(range(n_sims), desc='%.3f/%i' % (std_corr, k_feat))\n",
    "    else:\n",
    "        iterator = range(n_sims)\n",
    "\n",
    "    # Pipeline\n",
    "    pipe = Pipeline([('scale', StandardScaler()), \n",
    "                     ('svc', SVC(kernel='linear'))])\n",
    "\n",
    "    acc = np.zeros(n_sims)\n",
    "    for i in iterator:\n",
    "        \n",
    "        break_out = False\n",
    "        \n",
    "        # Generate y first\n",
    "        y = np.repeat([0, 1], repeats=int(n_samp / 2))\n",
    "        \n",
    "        attempt = 0\n",
    "        while True:\n",
    "            X = np.random.normal(0, 1, size=(n_samp, k_feat))\n",
    "            \n",
    "            corrs = np.corrcoef(np.hstack((X, y[:, np.newaxis])).T)[:-1, -1]\n",
    "            this_std = np.std(corrs)\n",
    "            std_found = np.abs(this_std - std_corr) < 0.001\n",
    "            \n",
    "            if std_found:\n",
    "                break\n",
    "                \n",
    "            attempt += 1\n",
    "            \n",
    "            if (attempt + 1) % 10000 == 0:\n",
    "                \n",
    "                # If it's stuck (likely for many features), let's help it a little bit\n",
    "                while True:\n",
    "                \n",
    "                    upper, lower = np.mean(corrs) + np.std(corrs), np.mean(corrs) - np.std(corrs)\n",
    "                    if (this_std - std_corr) < 0:\n",
    "                        # Std must be increased    \n",
    "                        idx = np.logical_and(corrs < upper, corrs > lower)\n",
    "                    else:\n",
    "                        # Std must be decreased\n",
    "                        idx = np.logical_or(corrs > upper, corrs < lower)\n",
    "                    \n",
    "                    new_X = np.random.normal(0, 1, size=(n_samp, idx.sum()))\n",
    "                    X[:, idx] = new_X\n",
    "                    \n",
    "                    corrs = np.corrcoef(np.hstack((X, y[:, np.newaxis])).T)[:-1, -1]\n",
    "                    this_std = np.std(corrs)\n",
    "                    std_found = np.abs(this_std - std_corr) < 0.001\n",
    "                    \n",
    "                    if std_found:\n",
    "                        break_out = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out:\n",
    "                break\n",
    "                \n",
    "        # 10-fold stratified CV\n",
    "        cv = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=10)\n",
    "\n",
    "        # Cross-validate and predict\n",
    "        scores = cross_val_score(pipe, X, y, cv=cv)\n",
    "        acc[i] = scores.mean()\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call the function from above with different values `std_corr` and, while we're at it, diffferent amounts of features (`k_feat`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "std_corrs = [0.3, 0.25, 0.2, 0.15, 0.1, 0.05, 0.01]\n",
    "\n",
    "k_feats = [1, 10, 100, 1000]\n",
    "n_sims = 10\n",
    "n_samp = 100\n",
    "n_folds = 10\n",
    "\n",
    "results_1 = dict(std_corr=[], k_feat=[], acc=[])\n",
    "\n",
    "for k_feat in k_feats:\n",
    "    \n",
    "    results_1['k_feat'].extend([k_feat] * n_sims * len(std_corrs))\n",
    "    results_1['std_corr'].extend(np.repeat(std_corrs, n_sims))\n",
    "    \n",
    "    accs = jl.Parallel(n_jobs=-1, verbose=1)(\n",
    "        jl.delayed(simulate_and_classify_data_constrained)(n_sims, n_samp, k_feat, n_folds, scorr, True)\n",
    "                   for scorr in std_corrs)\n",
    "    results_1['acc'].extend([a for this_acc in accs for a in this_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', context='poster')\n",
    "rc = {\"font.size\":25, \"axes.titlesize\":25, \"axes.labelsize\":25, \n",
    "      \"legend.fontsize\":15, \"xtick.labelsize\": 15,\n",
    "      \"ytick.labelsize\": 15, \"text.usetex\": True}\n",
    "\n",
    "sns.factorplot(data=pd.DataFrame(results), x='std_corr', y='acc', hue='k_feat', size=5, aspect=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can clearly see, artificially low standard deviations lead to stronger below-chance classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole-dataset confound regression leads to artificially low correlations\n",
    "We'll now demonstrate this using a simulation in which we regress \"confounds\" of various strengths (that is, how much they're correlated to y) from the data (X). [some more explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_classify_data_with_confound(n_sims, n_samp, k_feat, n_folds, corr_cy, cross_validate=False,\n",
    "                                             verbose=False):\n",
    "    \"\"\" Simulate classification of random data.\n",
    "    \n",
    "    Simulate classification of random data, in which X is of\n",
    "    shape (n_samp, k_feat) and consists of values from a random\n",
    "    normal distribution with mean 0 and unit variance. The data generation\n",
    "    is constrained by 'mean_corr' - the average absolute correlation between\n",
    "    features and y (to show that artificially reducing this mean will lead to\n",
    "    below chance classification). The target, y, is a vector with binary values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_sims : int\n",
    "        Number of (iterations of) simulations to run.\n",
    "    n_samp : int\n",
    "        Number of samples (preferably an even number to keep the classes in\n",
    "        y balanced).\n",
    "    k_feat : int\n",
    "        Number of features in X.\n",
    "    n_folds : int\n",
    "        Number of folds in a (Repeated) KFold cross-validation scheme \n",
    "        (in which the number of repetitions is set to 10 for robustness).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    acc : numpy array (of length n_sims)\n",
    "        Array with average scores (across folds) for n_sims. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Pipeline\n",
    "    pipe = Pipeline([('scale', StandardScaler()), \n",
    "                     ('svc', SVC(kernel='linear'))])\n",
    "    \n",
    "    if verbose:\n",
    "        iterator = tqdm_notebook(range(n_sims), desc='%.3f/%i' % (corr_cy, k_feat))\n",
    "    else:\n",
    "        iterator = range(n_sims)\n",
    "    \n",
    "    acc = np.zeros(n_sims)\n",
    "    std_corr = np.zeros(n_sims)\n",
    "    for i in iterator:\n",
    "        \n",
    "        # Generate y first\n",
    "        y = np.repeat([0, 1], repeats=int(n_samp / 2))\n",
    "        \n",
    "        noise_factor = 100\n",
    "        c = y + np.random.randn(n_samp) * noise_factor\n",
    "        corr = pearsonr(c, y)[0]\n",
    "        \n",
    "        while np.abs(corr - corr_cy) > 0.01:\n",
    "            # Decrease noise if the difference is too big\n",
    "            noise_factor -= 0.01\n",
    "            c = y + np.random.randn(n_samp) * noise_factor\n",
    "            corr = pearsonr(c, y)[0]        \n",
    "        \n",
    "        X = np.random.normal(0, 1, size=(n_samp, k_feat))\n",
    "        \n",
    "        cv = RepeatedStratifiedKFold(n_splits=n_folds, n_repeats=10)\n",
    "            \n",
    "        if not cross_validate:\n",
    "            cr = ConfoundRegressor(confound=c, X=X, cross_validate=True)\n",
    "            X = cr.fit_transform(X)\n",
    "            std_corr[i] = np.corrcoef(np.hstack((X, y[:, np.newaxis])).T)[:-1, -1].std()\n",
    "            scores = cross_val_score(pipe, X, y, cv=cv)\n",
    "            acc[i] = scores.mean()\n",
    "        else:\n",
    "            std_corr[i] = np.corrcoef(np.hstack((X, y[:, np.newaxis])).T)[:-1, -1].std()\n",
    "            tmp_scores = np.zeros(n_folds * 10)\n",
    "            for ii, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "                cr = ConfoundRegressor(confound=c, X=X, cross_validate=True)\n",
    "                pipe2 = deepcopy(pipe.steps)\n",
    "                pipe2.insert(0, ('cr', cr))\n",
    "                pipe2 = Pipeline(pipe2)\n",
    "                pipe2.fit(X[train_idx], y[train_idx])\n",
    "                tmp_scores[ii] = pipe2.score(X[test_idx], y[test_idx])\n",
    "            \n",
    "            acc[i] = tmp_scores.mean()\n",
    "        \n",
    "    return acc, std_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it for different correlations(c, y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrs_cy = np.arange(0, 1.1, 0.1)\n",
    "k_feats = [1, 10, , 100, 1000]\n",
    "n_sims = 10\n",
    "n_samp = 100\n",
    "n_folds = 10\n",
    "\n",
    "results_2 = dict(corr_cy=[], k_feat=[], acc=[], std_corr=[])\n",
    "\n",
    "for k_feat in k_feats:\n",
    "    \n",
    "    results_2['k_feat'].extend([k_feat] * n_sims * len(corrs_cy))\n",
    "    results_2['corr_cy'].extend(np.repeat(corrs_cy, n_sims))\n",
    "    \n",
    "    res = jl.Parallel(n_jobs=6, verbose=10)(jl.delayed(simulate_and_classify_data_with_confound)(n_sims, n_samp, k_feat, n_folds, corr_cy, cross_validate=False) for corr_cy in corrs_cy)\n",
    "    accs, std_corrs = [r[0] for r in res], [r[1] for r in res]\n",
    "    results_2['acc'].extend([a for this_acc in accs for a in this_acc])\n",
    "    results_2['std_corr'].extend([s for this_std_corr in std_corrs for s in this_std_corr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "sns.factorplot(data=results_df, x='corr_cy', y='acc', hue='k_feat', size=5, aspect=2)\n",
    "plt.axhline(0.5, c='black', ls='--')\n",
    "plt.title('Effect of WDCR on null-data')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Correlation confound and target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(data=results_df, x='corr_cy', y='std_corr', hue='k_feat', size=5, aspect=2)\n",
    "pearsonr(results_df['std_corr'], results_df['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foldwise confound regression solves negative bias\n",
    "If we regress it out foldwise, everything stays nicely at 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_cy = np.arange(0, 1.1, 0.1)\n",
    "k_feats = [5, 10, 50, 100, 1000]\n",
    "n_sims = 25\n",
    "n_samp = 100\n",
    "n_folds = 10\n",
    "\n",
    "results = dict(corr_cy=[], k_feat=[], acc=[], std_corr=[])\n",
    "\n",
    "for k_feat in k_feats:\n",
    "    \n",
    "    results['k_feat'].extend([k_feat] * n_sims * len(corrs_cy))\n",
    "    results['corr_cy'].extend(np.repeat(corrs_cy, n_sims))\n",
    "    \n",
    "    res = jl.Parallel(n_jobs=6, verbose=10)(jl.delayed(simulate_and_classify_data_with_confound)(n_sims, n_samp, k_feat, n_folds, corr_cy, cross_validate=True) for corr_cy in corrs_cy)\n",
    "    accs, std_corrs = [r[0] for r in res], [r[1] for r in res]\n",
    "    results['acc'].extend([a for this_acc in accs for a in this_acc])\n",
    "    results['std_corr'].extend([s for this_std_corr in std_corrs for s in this_std_corr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "sns.factorplot(data=results_df, x='corr_cy', y='acc', hue='k_feat', size=5, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we have shown that below chance classification is due to a more narrow correlation distribution than expected for random normal data (i.e., when $\\rho = 0$). Moreover, we show that whole-dataset confound regression has exactly this effect: it narrows the correlation distribution and, importantly, this can be avoided by performing foldwise (i.e. crossvalidated) confound regression.\n",
    "\n",
    "Now, having established how to remove confounds (i.e., foldwise), part B will investigate how this method (foldwise confound regression) compares to counterbalancing, given a baseline of no correction!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
