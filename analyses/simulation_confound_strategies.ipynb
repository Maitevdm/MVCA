{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation confound strategies\n",
    "This notebook evaluates the effectiveness of \"confound strategies\", i.e. their effectiveness of removing the influence of a known confound on machine learning analyses of (simulations of) mutivoxel pattern data. This notebook is meant as a supplement to the [article](link). Together with the notebook with [empirical analyses](empirical_analysis_gender_classification.ipynb), it hopefully complements the article and clarifies our methods and results.\n",
    "\n",
    "Note that this notebook is organized slightly differently than the article. In this notebook, we'll first show you the problem with regressing out a confound from the entire dataset (i.e. not foldwise). We then show you how the correct confound-regression method and the counter-balance method compare in their ability to control for confounding influences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Bunch of scikit-learn stuff\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Specific statistics-functions\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Misc.\n",
    "from tqdm import tqdm_notebook\n",
    "from copy import deepcopy\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom code! (install skbold by `pip install skbold`; counterbalance.py is in cwd)\n",
    "from skbold.preproc import ConfoundRegressor\n",
    "from counterbalance import CounterbalancedStratifiedSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why confound regression the entire dataset at once is a bad idea\n",
    "We first delve into the issue of below-chance classification we observe when regressing out the confound from the entire dataset at once. As described in the article, we argue that this is narrowing the correlation distribution you'd expect by chance. If you generate random (normal) data matrix of shape $N\\ (samples) \\times K\\ (features)$  - let's call this X - and a vector with binary values (0 or 1) of shape $N \\times K$ - let's call this y.\n",
    "\n",
    "Suppose you implement a cross-validated analysis, in which you fit a linear model (e.g. logistic regression) iteratively in a (let's say) 10-fold cross-validation scheme. Now, suppose you simulate this 1000 times. (Note that this is very similar to the permutation statistics people employ to calculate non-parametric p-values for their classification scores.) You expect the distribution of your (e.g.) accuracy scores to be centered around 50%. However, you'll undoubtely observe some scores below 50%. \n",
    "\n",
    "Remember, linear classifiers are capitalizing on some linear statistical dependence between the features in X and y. This can be (per feature) expressed as the correlation between X and y. We argue that these scores are due a relatively low (absolute) initial correlation between features and y in the *entire* dataset, which subsequently causes opposing statistical dependencies (i.e. flipped signs of correlations) between the train and test-set. \n",
    "\n",
    "To show this association between (below chance) accuracy and the initial correlation between features and y, we've written a simple simulation below. We generate random (normal) data for our X matrix and a vector with random (0, 1) values for y for a given `n_sims` simulations. For each simulation, we keep track of accuracy (averaged over `n_folds`), the initial (absolute) correlation between X and y (averaged over `k_feat` in X), and the proportion of \"sign-flips\" we observe between the correlation of features and y in the train and test-set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_and_classify_random_data(n_sims, n_samp, k_feat, n_fold):\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='linear'))\n",
    "    ])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_fold)\n",
    "\n",
    "    results = {'accuracy': [],\n",
    "               'init_corr': [],\n",
    "               'sign_change': []}\n",
    "\n",
    "    acc = np.zeros((n_sims, n_fold))\n",
    "    for i in range(n_sims):\n",
    "        X = np.random.normal(0, 1, size=(n_samp, k_feat))\n",
    "        y = np.repeat([0, 1], repeats=int(n_samp / 2))\n",
    "        \n",
    "        results['init_corr'].append(np.abs(np.corrcoef(np.hstack((X, y[:, np.newaxis])).T))[-1, :-1].mean())\n",
    "        sign_changes = np.zeros(n_fold)\n",
    "        \n",
    "        for ii, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            corr_train = np.corrcoef(np.hstack((X_train, y_train[:, np.newaxis])).T)[-1, :-1]\n",
    "            corr_test = np.corrcoef(np.hstack((X_test, y_test[:, np.newaxis])).T)[-1, :-1]\n",
    "            sign_changes[ii] = (np.sign(corr_train) != np.sign(corr_test)).sum() / n_fold\n",
    "            \n",
    "            pipe.fit(X_train, y_train)\n",
    "            acc[i, ii] = pipe.score(X_test, y_test)\n",
    "        results['accuracy'].append(acc[i, :].mean())\n",
    "        results['sign_change'].append(sign_changes.mean())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we simulate 100 datasets with 100 samples, 5 features, which are analyzed with a linear SVM in a 10-fold cross-validation scheme. We then plot a scatterplot (and linear fit) between accuracy and the initial correlation and between accuracy and the proportion of sign-flips we observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCkAAAFOCAYAAACrP0ciAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3feb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = simulate_and_classify_random_data(n_sims=100, n_samp=100, k_feat=5, n_fold=10)\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Accuracy & init correlation')\n",
    "sns.regplot(y='accuracy', x='init_corr', data=pd.DataFrame(results))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Accuracy & proportion sign changes')\n",
    "sns.regplot(y='accuracy', x='sign_change', data=pd.DataFrame(results))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, higher initial correlations lead to above chance accuracy, but importantly (absolute) correlations close(r) to zero lead to below chance classifications! This is fine in this simulation, because we can simulate this many times, across which the mean accuracy is about 50% (y-coordinates of centroid in scatterplots). But you also see that below an average (absolute) correlation across features between X and y of 0.08, below-chance accuracies tend to occur. \n",
    "\n",
    "We think this is happening when you regress out your confound on the entire dataset: you artificially narrow the (absolute) correlation distribution, which leads to an increase in sign-flips and thus negative bias.\n",
    "\n",
    "We show this in another simulation, in which we control the average (absolute) correlation between X and y. We test a range of these correlations and show that lower correlations yield (relatively) more negative bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_and_classify_data_constrained(n_sims, n_samp, k_feat, n_splits, mean_corr):\n",
    "\n",
    "    acc = np.zeros(n_sims)\n",
    "    for i in tqdm_notebook(range(n_sims), desc='%.2f/%i' % (mean_corr, k_feat)):\n",
    "        \n",
    "        # Generate y first\n",
    "        y = np.repeat([0, 1], repeats=int(n_samp / 2))\n",
    "        \n",
    "        attempt = 0\n",
    "        while True:\n",
    "            X = np.random.normal(0, 1, size=(n_samp, k_feat))\n",
    "            \n",
    "            corrs = np.corrcoef(np.hstack((X, y[:, np.newaxis])).T)[:-1, -1]\n",
    "            if (attempt + 1) % 10000 == 0:\n",
    "                \n",
    "                # If it's stuck (likely for many features), let's help it a little bit\n",
    "                while True:\n",
    "                    idx = np.abs(corrs) > mean_corr\n",
    "                    new_X = np.random.normal(0, 1, size=(n_samp, idx.sum()))\n",
    "                    X[:, idx] = new_X\n",
    "                    corrs = np.corrcoef(np.hstack((X, y[:, np.newaxis])).T)[:-1, -1]\n",
    "                    if np.mean(np.abs(corrs)) < mean_corr:\n",
    "                        break\n",
    "            \n",
    "            if np.mean(np.abs(corrs)) < mean_corr:\n",
    "                break\n",
    "            else:\n",
    "                attempt += 1\n",
    "            \n",
    "        # 10-fold stratified CV\n",
    "        cv = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "        # Pipeline\n",
    "        pipe = Pipeline([('scale', StandardScaler()), \n",
    "                         ('svc', SVC(kernel='linear'))])\n",
    "\n",
    "        # Cross-validate and predict\n",
    "        scores = cross_val_score(pipe, X, y, cv=n_splits)\n",
    "        acc[i] = scores.mean()\n",
    "        \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7767aea8a47f43c299cbed78b5830fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd5d3caf24c40ad8a306aa6f231dc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19f3b0b73164d84af4f3b9ca7c239a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07c47d5fafd4be2a6994efece7210bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb23e1c532d0412f987fa98076dd7476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e07b5df3d445ddacaeb5614349fc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4d9a02cb514c5e813e1430f6292e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e798356ca9b046c1a7c3ca18a6008f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b143f82e5b434db61139f5d0b8f74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961fa82fbc8d4178b42f9fd9567a0ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718b035057d7446aaede8d4939969c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929d4b4763314166ac9f27fd059f0444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993e5faebd724786b7bfa6f8229cc54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423ba98524294fb2829c855fecb5012a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6764e81356843d59eabd7ffbacd07b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9ae9851bdc4355b2bf2e1d84f7da9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5125fdfc474ae691c4aa4165cc8b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ac9b3ff1d74d738bbb42234135d758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0517388283875\n",
      "0.0423746163123\n",
      "0.035828837643\n",
      "0.0299291808832\n",
      "0.0429176256463\n",
      "0.0131537717855\n",
      "0.0442048236027\n",
      "0.0570878190935\n",
      "0.0743420809252\n",
      "0.0513399578908\n",
      "0.0250779460981\n",
      "0.031032273984\n",
      "0.0245116196486\n",
      "0.0251859330548\n",
      "0.0291215012162\n",
      "0.0199082680434\n",
      "0.0490580884188\n",
      "0.0378406039067\n",
      "0.0150787821413\n",
      "0.0259295505147\n",
      "0.0379285286098\n",
      "0.0297917869936\n",
      "0.0242693937737\n",
      "0.0258901544903\n",
      "0.0341648368537\n",
      "0.0300155252367\n",
      "0.0196715622713\n",
      "0.0647513420228\n",
      "0.065136149827\n",
      "0.070845088134\n",
      "0.0397824847448\n",
      "0.0370024190714\n",
      "0.0163308538755\n",
      "0.0645280751696\n",
      "0.0631909733779\n",
      "0.0610424237983\n",
      "0.0362742702464\n",
      "0.0393797388551\n",
      "0.0217379514919\n",
      "0.0577355519699\n",
      "0.0638599277586\n",
      "0.0339772891122\n",
      "0.0183589205913\n",
      "0.0789690673211\n",
      "0.0610517538911\n",
      "0.0205313872013\n",
      "0.0267876942841\n",
      "0.0157700071173\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74dec602a9154a6a84030b2f0b626f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.123479632818\n",
      "0.107472337989\n",
      "0.0564877390247\n",
      "0.0637727027499\n",
      "0.0469230502387\n",
      "0.072938214077\n",
      "0.0381538339213\n",
      "0.0411430981717\n",
      "0.0215441988998\n",
      "0.0168364954093\n",
      "0.0358788884986\n",
      "0.0114766524967\n",
      "0.00878715160041\n",
      "0.0510674379083\n",
      "0.0378437788234\n",
      "0.0322953415253\n",
      "0.0314368755065\n",
      "0.0184102316996\n",
      "0.0168397591774\n",
      "0.0330634873\n",
      "0.01806358057\n",
      "0.0125911666667\n",
      "0.0167494153536\n",
      "0.0100631914718\n",
      "0.0293742689053\n",
      "0.0179370033258\n",
      "0.0303771029922\n",
      "0.0108263906403\n",
      "0.0308121587119\n",
      "0.00681194568717\n",
      "0.0831004821631\n",
      "0.0970947620379\n",
      "0.0814719623946\n",
      "0.0348684617167\n",
      "0.0210936725988\n",
      "0.0332322355361\n",
      "0.0344652179674\n",
      "0.0340814368783\n",
      "0.0593158004835\n",
      "0.0568141112627\n",
      "0.0417965721015\n",
      "0.0788006339961\n",
      "0.0267644880621\n",
      "0.0297102034023\n",
      "0.0148076748367\n",
      "0.006716048252\n",
      "0.141110313311\n",
      "0.0404360702096\n",
      "0.0723481390283\n",
      "0.0544609130804\n",
      "0.0657768516066\n",
      "0.0458288861462\n",
      "0.035122280822\n",
      "0.0487858744202\n",
      "0.0250035104458\n",
      "0.0196652872125\n",
      "0.0306676664608\n",
      "0.0296092710984\n",
      "0.0386075746662\n",
      "0.0136374799601\n",
      "0.034437996\n",
      "0.0595127725408\n",
      "0.0305273698239\n",
      "0.0405306259267\n",
      "0.0602446024058\n",
      "0.0506677126261\n",
      "0.0274019392499\n",
      "0.0243236845601\n",
      "0.0273398771172\n",
      "0.0307268472704\n",
      "0.00900128479785\n",
      "0.0953300394886\n",
      "0.0668572086886\n",
      "0.0956172282216\n",
      "0.08163424035\n",
      "0.0864757479325\n",
      "0.142740923962\n",
      "0.0559578987883\n",
      "0.0891206867093\n",
      "0.0315284118602\n",
      "0.0283895023059\n",
      "0.021114888846\n",
      "0.0141226385416\n",
      "0.0170385891451\n",
      "0.0123150755616\n",
      "0.0104252401472\n",
      "0.0342574463314\n",
      "0.0293552620599\n",
      "0.00847105057366\n",
      "0.0888459631924\n",
      "0.0812531417796\n",
      "0.0874306957\n",
      "0.082927465146\n",
      "0.052521634788\n",
      "0.0633801476019\n",
      "0.0638173337043\n",
      "0.0588905699135\n",
      "0.0479280318655\n",
      "0.0282261602766\n",
      "0.0177945657558\n",
      "0.0182859820135\n",
      "0.0470763952964\n",
      "0.0114770653697\n",
      "0.0120139361311\n",
      "0.0184680767239\n",
      "0.0190940612121\n",
      "0.00575567140633\n",
      "0.126598647528\n",
      "0.0345062663825\n",
      "0.047793151109\n",
      "0.0426618779139\n",
      "0.0321810047205\n",
      "0.0625607523135\n",
      "0.0956429610759\n",
      "0.0622114857668\n",
      "0.0582605817149\n",
      "0.0705851539686\n",
      "0.0904296697062\n",
      "0.0653479900596\n",
      "0.0374437146881\n",
      "0.0503640347094\n",
      "0.0709424619518\n",
      "0.0533361368592\n",
      "0.0560850360505\n",
      "0.0136404935882\n",
      "0.00859135469379\n",
      "0.0526793429994\n",
      "0.0498115959723\n",
      "0.0837649267425\n",
      "0.066970772376\n",
      "0.0279324741888\n",
      "0.066467803127\n",
      "0.0368264562489\n",
      "0.036433437307\n",
      "0.042963921563\n",
      "0.0450195648771\n",
      "0.0123917074388\n",
      "0.0565963391344\n",
      "0.0526733919839\n",
      "0.0299190974834\n",
      "0.0615769726332\n",
      "0.0290644916519\n",
      "0.0463736072162\n",
      "0.0573652619881\n",
      "0.00786224384091\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08379c79f1934873b2f12ccb98a0df84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0931316766269\n",
      "0.0653630980735\n",
      "0.0867789099574\n",
      "0.0980588801325\n",
      "0.0496354375599\n",
      "0.0816256130741\n",
      "0.0491542887246\n",
      "0.101188170132\n",
      "0.0603483521944\n",
      "0.0630787073572\n",
      "0.059822955504\n",
      "0.0446325909842\n",
      "0.0433640238773\n",
      "0.0351155587269\n",
      "0.036930993393\n",
      "0.0540948451803\n",
      "0.0602093124087\n",
      "0.0529214505162\n",
      "0.0566225276684\n",
      "0.0501587561218\n",
      "0.059592173134\n",
      "0.0438191195499\n",
      "0.056661247199\n",
      "0.0500164288095\n",
      "0.0574008798509\n",
      "0.0757351648047\n",
      "0.0355091218614\n",
      "0.0181739467307\n",
      "0.024553015278\n",
      "0.016651556387\n",
      "0.0113921676815\n",
      "0.028188979112\n",
      "0.0108587800028\n",
      "0.0235646477245\n",
      "0.0313994553018\n",
      "0.0343635008022\n",
      "0.0262702606157\n",
      "0.0150742923757\n",
      "0.0199010446183\n",
      "0.0068880089197\n",
      "0.0592774353369\n",
      "0.0830330659255\n",
      "0.0613725505483\n",
      "0.0728343657703\n",
      "0.0497650228284\n",
      "0.0523883553198\n",
      "0.0740718018703\n",
      "0.0587799228782\n",
      "0.0358913034627\n",
      "0.0485625423116\n",
      "0.0526883481607\n",
      "0.043262973532\n",
      "0.0120689475883\n",
      "0.0257839966707\n",
      "0.011095909061\n",
      "0.011953699667\n",
      "0.0147756252256\n",
      "0.034223779151\n",
      "0.0158227035869\n",
      "0.00511493179002\n",
      "0.0458455862804\n",
      "0.0725671493724\n",
      "0.0643042878489\n",
      "0.0701061223151\n",
      "0.030694855546\n",
      "0.038586016743\n",
      "0.0443250659464\n",
      "0.0216543686347\n",
      "0.0471796170887\n",
      "0.0295923490286\n",
      "0.0282201557915\n",
      "0.0338043516887\n",
      "0.0512951576428\n",
      "0.0205548138743\n",
      "0.0305892461211\n",
      "0.0176793110271\n",
      "0.0101042522724\n",
      "0.0220080920437\n",
      "0.0209322100314\n",
      "0.0291095795138\n",
      "0.0201000645593\n",
      "0.00874358096731\n",
      "0.0683198636677\n",
      "0.0417868833605\n",
      "0.0354647145293\n",
      "0.0330239741491\n",
      "0.0408068736174\n",
      "0.046646828432\n",
      "0.0514920242855\n",
      "0.0370644729019\n",
      "0.0282979133264\n",
      "0.0243021665441\n",
      "0.0247710703307\n",
      "0.0294205486569\n",
      "0.0133699936554\n",
      "0.0149938482442\n",
      "0.00804698798522\n",
      "0.10298578602\n",
      "0.0730252196756\n",
      "0.0473964181999\n",
      "0.0638238050098\n",
      "0.0602826412977\n",
      "0.0338818998995\n",
      "0.0243902948187\n",
      "0.0443802670919\n",
      "0.0214412436583\n",
      "0.0268750289184\n",
      "0.0446472813107\n",
      "0.0143099356361\n",
      "0.0262137760577\n",
      "0.0135877724228\n",
      "0.0109871164158\n",
      "0.0141129624601\n",
      "0.0320402227984\n",
      "0.019754517278\n",
      "0.00755196729786\n",
      "0.0601244066982\n",
      "0.058269192426\n",
      "0.0204134706181\n",
      "0.0286716308435\n",
      "0.0448537566452\n",
      "0.0452598024315\n",
      "0.0268931912586\n",
      "0.018237645635\n",
      "0.0177878382592\n",
      "0.00647961086673\n",
      "0.0580458183148\n",
      "0.0442940738421\n",
      "0.0433823353525\n",
      "0.0389112255955\n",
      "0.0244223488691\n",
      "0.0356454160807\n",
      "0.0395464078614\n",
      "0.0509957124721\n",
      "0.040913615314\n",
      "0.0565607557246\n",
      "0.0368951302377\n",
      "0.0140593649976\n",
      "0.0424955919059\n",
      "0.0342895090239\n",
      "0.0280549129339\n",
      "0.0208903588064\n",
      "0.025537495059\n",
      "0.0235695020771\n",
      "0.00472066556765\n",
      "0.0897478834914\n",
      "0.0789828600218\n",
      "0.0730146314254\n",
      "0.0841615000684\n",
      "0.0571389136009\n",
      "0.0881339203528\n",
      "0.0294756885519\n",
      "0.0421811214822\n",
      "0.066658925448\n",
      "0.0567141418507\n",
      "0.0421974605299\n",
      "0.0428327853953\n",
      "0.0461319702898\n",
      "0.0286257678893\n",
      "0.0137763914001\n",
      "0.0149702246079\n",
      "0.00619294580694\n",
      "0.0581281960503\n",
      "0.102029920507\n",
      "0.0551752342541\n",
      "0.0622837448698\n",
      "0.0643060916688\n",
      "0.0447480183873\n",
      "0.0453854307857\n",
      "0.0433260383856\n",
      "0.0552600925832\n",
      "0.0716484693033\n",
      "0.0325094206429\n",
      "0.0305767520301\n",
      "0.0337784723527\n",
      "0.0355339832602\n",
      "0.0254919768158\n",
      "0.030541550378\n",
      "0.0281159302417\n",
      "0.0279467093368\n",
      "0.0155259627948\n",
      "0.0296619807367\n",
      "0.0242744245657\n",
      "0.0308421561115\n",
      "0.0203672035686\n",
      "0.0176094482771\n",
      "0.0179679269461\n",
      "0.0158379166658\n",
      "0.0165048352245\n",
      "0.010502169741\n",
      "0.0260432331133\n",
      "0.0120663352566\n",
      "0.0110314625463\n",
      "0.0159971886605\n",
      "0.0129574291343\n",
      "0.0264480046729\n",
      "0.0065397933943\n",
      "0.0734669352713\n",
      "0.0553812843405\n",
      "0.0420732734949\n",
      "0.0695836507053\n",
      "0.0515980586982\n",
      "0.0261879009106\n",
      "0.0499609860872\n",
      "0.0570550922504\n",
      "0.0409085818029\n",
      "0.0131714365409\n",
      "0.0428184298087\n",
      "0.0265424620813\n",
      "0.0283660121664\n",
      "0.0342619209609\n",
      "0.0297071544504\n",
      "0.0339044563314\n",
      "0.0486700104756\n",
      "0.00684112030109\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500bc4d2ac194c12abc06866be6088a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0525360716836\n",
      "0.122007872953\n",
      "0.0562548543078\n",
      "0.0826630352398\n",
      "0.164180049915\n",
      "0.100883413599\n",
      "0.0479323602778\n",
      "0.0425690339403\n",
      "0.0904935937135\n",
      "0.0526198757314\n",
      "0.0690611020629\n",
      "0.0266601776925\n",
      "0.050538248909\n",
      "0.0413799064105\n",
      "0.0352016970505\n",
      "0.0246252364996\n",
      "0.00782692249661\n",
      "0.051836354493\n",
      "0.0549502663386\n",
      "0.0206431690556\n",
      "0.0103565603058\n",
      "0.0509722587869\n",
      "0.017696675281\n",
      "0.013330146639\n",
      "0.0104522823541\n",
      "0.007856960239\n",
      "0.0328344813338\n",
      "0.0223290667694\n",
      "0.025951518652\n",
      "0.0243956265558\n",
      "0.00912382168749\n",
      "0.0185709174396\n",
      "0.0257453102803\n",
      "0.0217324721166\n",
      "0.00723820974389\n",
      "0.00942061418804\n",
      "0.023840209689\n",
      "0.0111389035053\n",
      "0.0101856471377\n",
      "0.0100047596802\n",
      "0.0213566674321\n",
      "0.011994230019\n",
      "0.0245722032082\n",
      "0.0231234450687\n",
      "0.0231388068566\n",
      "0.0156121243637\n",
      "0.00695698650412\n",
      "0.00873628329041\n",
      "0.0361745447712\n",
      "0.0182051201667\n",
      "0.0411367593552\n",
      "0.0172164330382\n",
      "0.0108599057831\n",
      "0.011314089466\n",
      "0.00969598622291\n",
      "0.00993704153729\n",
      "0.0200817798888\n",
      "0.0232958064586\n",
      "0.0124001265167\n",
      "0.0298779405417\n",
      "0.00906054497742\n",
      "0.0164299980651\n",
      "0.00662077425407\n",
      "0.0287701794201\n",
      "0.00306785014013\n",
      "0.0479327253102\n",
      "0.0946317161851\n",
      "0.142579742072\n",
      "0.0457766894489\n",
      "0.108549088079\n",
      "0.0877860895526\n",
      "0.116877146297\n",
      "0.128412700621\n",
      "0.0493952910355\n",
      "0.0578699385505\n",
      "0.0913214412668\n",
      "0.107167877452\n",
      "0.0740681406602\n",
      "0.0252960469255\n",
      "0.0334754453969\n",
      "0.0381431747346\n",
      "0.0815802258158\n",
      "0.0785976434494\n",
      "0.0278054411626\n",
      "0.0512467580901\n",
      "0.0212674657222\n",
      "0.0527308382108\n",
      "0.00769189921351\n",
      "0.0436045473477\n",
      "0.0556779413701\n",
      "0.047774126075\n",
      "0.0469303796942\n",
      "0.0457153753382\n",
      "0.0442678230412\n",
      "0.0573444402534\n",
      "0.0282338788054\n",
      "0.00478301321066\n",
      "0.083647989633\n",
      "0.111701233825\n",
      "0.0626150728103\n",
      "0.0279429127974\n",
      "0.0433505241961\n",
      "0.0601786615511\n",
      "0.0305546096415\n",
      "0.0605933934674\n",
      "0.0492142903807\n",
      "0.040273988288\n",
      "0.0293926009712\n",
      "0.0317027567271\n",
      "0.0689366555451\n",
      "0.041464581791\n",
      "0.0319204973211\n",
      "0.0278062279502\n",
      "0.0505003232216\n",
      "0.0565711284427\n",
      "0.00646716751863\n",
      "0.00674933747614\n",
      "0.0204638673036\n",
      "0.00414015630927\n",
      "0.06618255394\n",
      "0.125514515262\n",
      "0.0540996274473\n",
      "0.0899071242196\n",
      "0.0894813777312\n",
      "0.0754983776058\n",
      "0.0483206808787\n",
      "0.0754737329018\n",
      "0.0680105342097\n",
      "0.0489471484871\n",
      "0.0376812404554\n",
      "0.052961002524\n",
      "0.0974923907537\n",
      "0.065419240106\n",
      "0.0542604767595\n",
      "0.0476511055761\n",
      "0.00548295180929\n",
      "0.0386992220652\n",
      "0.0290869166302\n",
      "0.0142333055415\n",
      "0.00464683475554\n",
      "0.0749263514911\n",
      "0.0640869092504\n",
      "0.0940081068488\n",
      "0.0546899672186\n",
      "0.130228301227\n",
      "0.0405787840445\n",
      "0.0646532793425\n",
      "0.0573351763226\n",
      "0.0470371613416\n",
      "0.0384914720068\n",
      "0.0509214464551\n",
      "0.0497096781407\n",
      "0.0800973069465\n",
      "0.0707103892935\n",
      "0.0629033971145\n",
      "0.0605125882581\n",
      "0.0370240336285\n",
      "0.0550004283689\n",
      "0.10300915234\n",
      "0.0405324270111\n",
      "0.0950001823503\n",
      "0.0578030662317\n",
      "0.0270482242275\n",
      "0.0258439476866\n",
      "0.106985054606\n",
      "0.092378597509\n",
      "0.0478426162059\n",
      "0.0413935584251\n",
      "0.0165018514556\n",
      "0.0445182604059\n",
      "0.0524677036383\n",
      "0.0265718595511\n",
      "0.0298572199662\n",
      "0.0135584505392\n",
      "0.0604709415904\n",
      "0.0427939502928\n",
      "0.0320809214599\n",
      "0.0290191510851\n",
      "0.0453801434905\n",
      "0.0550445309922\n",
      "0.049443095115\n",
      "0.0426003369496\n",
      "0.030485130787\n",
      "0.0655262000664\n",
      "0.047568495955\n",
      "0.0151539356446\n",
      "0.0275103641829\n",
      "0.0295433013288\n",
      "0.0101924739233\n",
      "0.0038909214656\n",
      "0.0764953914136\n",
      "0.0590605338278\n",
      "0.154485576631\n",
      "0.0604052052512\n",
      "0.0612699790865\n",
      "0.0900849595196\n",
      "0.101314789988\n",
      "0.111304636484\n",
      "0.0437825635815\n",
      "0.0808379895912\n",
      "0.0639598434852\n",
      "0.0799138319346\n",
      "0.091398455163\n",
      "0.0445974772677\n",
      "0.0320402502728\n",
      "0.0703365018409\n",
      "0.0494606496266\n",
      "0.0519149849304\n",
      "0.0609080494137\n",
      "0.0446497225711\n",
      "0.0418173339377\n",
      "0.0662827206124\n",
      "0.0372382361347\n",
      "0.0488555909207\n",
      "0.0249858688006\n",
      "0.0556244583802\n",
      "0.0792982129825\n",
      "0.0207584029538\n",
      "0.0249574157554\n",
      "0.0191324230092\n",
      "0.029031607973\n",
      "0.0399656395255\n",
      "0.0118465369973\n",
      "0.0252123068935\n",
      "0.00883113245756\n",
      "0.0119119764396\n",
      "0.019944258545\n",
      "0.00278931786419\n",
      "0.0908241975336\n",
      "0.0778611225282\n",
      "0.0541057980003\n",
      "0.1149367523\n",
      "0.0610741466541\n",
      "0.040289557951\n",
      "0.0269591627233\n",
      "0.0415248521382\n",
      "0.039055117837\n",
      "0.0281405283669\n",
      "0.0217404047735\n",
      "0.0167715011668\n",
      "0.0303161444492\n",
      "0.0254026394264\n",
      "0.0515752382168\n",
      "0.0161472027215\n",
      "0.0311784900603\n",
      "0.0150410369634\n",
      "0.0236785904476\n",
      "0.0306041088212\n",
      "0.011051577887\n",
      "0.0341667069008\n",
      "0.0203591756886\n",
      "0.00847983786945\n",
      "0.00812548651493\n",
      "0.0236628496034\n",
      "0.0517158548564\n",
      "0.0289492956186\n",
      "0.0164615656707\n",
      "0.0197171746438\n",
      "0.00429425716781\n",
      "0.0650761710102\n",
      "0.0815797710425\n",
      "0.0646567245194\n",
      "0.0794913787624\n",
      "0.120241354143\n",
      "0.0880590840901\n",
      "0.0512484352678\n",
      "0.0847887171731\n",
      "0.080978144761\n",
      "0.0668730502923\n",
      "0.0543992107933\n",
      "0.101430608319\n",
      "0.0601200606754\n",
      "0.108886178662\n",
      "0.0670465968472\n",
      "0.0102032460627\n",
      "0.0643021848795\n",
      "0.0460323352035\n",
      "0.0886780736544\n",
      "0.0180623454944\n",
      "0.0131735420384\n",
      "0.0364322676626\n",
      "0.00703444310486\n",
      "0.00506420053767\n",
      "0.0160734395468\n",
      "0.0262151073233\n",
      "0.0241495640221\n",
      "0.0123506723658\n",
      "0.0117471988057\n",
      "0.0148395442284\n",
      "0.0245050804567\n",
      "0.0252444685302\n",
      "0.00664812130161\n",
      "0.00924347045766\n",
      "0.00259724260276\n",
      "0.0882844488672\n",
      "0.0454916297213\n",
      "0.0885833650523\n",
      "0.0704659468552\n",
      "0.0275364678935\n",
      "0.0494332785614\n",
      "0.0600528339083\n",
      "0.0299395253446\n",
      "0.0280926977439\n",
      "0.0672272302864\n",
      "0.0453341163794\n",
      "0.0573230621349\n",
      "0.0469597045431\n",
      "0.077808432729\n",
      "0.0774812163184\n",
      "0.0447946524446\n",
      "0.0534699452443\n",
      "0.0500336342512\n",
      "0.0447578161002\n",
      "0.0549866853233\n",
      "0.0517058184939\n",
      "0.0480696573395\n",
      "0.022338300173\n",
      "0.00769965039927\n",
      "0.04131826002\n",
      "0.0121778572901\n",
      "0.0026572882743\n",
      "0.0885640217545\n",
      "0.0807277094336\n",
      "0.0863679277367\n",
      "0.0433967847448\n",
      "0.0649083531414\n",
      "0.0911325592103\n",
      "0.0820468588518\n",
      "0.0552708856812\n",
      "0.0370144677927\n",
      "0.0569445530396\n",
      "0.0265285985556\n",
      "0.0164133717125\n",
      "0.0354978160374\n",
      "0.00615327881949\n",
      "0.0126095854297\n",
      "0.0173832529987\n",
      "0.0334012101209\n",
      "0.0142710550708\n",
      "0.00742979061917\n",
      "0.0225631737478\n",
      "0.0244572574735\n",
      "0.0343976960562\n",
      "0.0101576861167\n",
      "0.0379417509965\n",
      "0.0162779468869\n",
      "0.0175376509885\n",
      "0.0129446209502\n",
      "0.00385107903681\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bc2ad7265b4e6583b93b30ed931cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0737637105903\n",
      "0.0859739492861\n",
      "0.0807799117659\n",
      "0.0768212399235\n",
      "0.0784950682398\n",
      "0.07921817942\n",
      "0.067497379644\n",
      "0.0721966309508\n",
      "0.0378144921552\n",
      "0.0181910052808\n",
      "0.0355001306328\n",
      "0.0243790466269\n",
      "0.0401212154084\n",
      "0.0251072239384\n",
      "0.0305694208197\n",
      "0.0614074500569\n",
      "0.0219872242953\n",
      "0.028703170984\n",
      "0.0272688975071\n",
      "0.0467649987696\n",
      "0.0533070589994\n",
      "0.0245854644524\n",
      "0.0644298201935\n",
      "0.0251953070165\n",
      "0.0423754877667\n",
      "0.0370705434715\n",
      "0.0308220525598\n",
      "0.0479348218722\n",
      "0.0270258220728\n",
      "0.0458110875721\n",
      "0.0203030429961\n",
      "0.0279064331243\n",
      "0.0415856679335\n",
      "0.0311167832075\n",
      "0.0258205096883\n",
      "0.0248926446786\n",
      "0.0375597888781\n",
      "0.0231315690387\n",
      "0.0202800694188\n",
      "0.0375826794855\n",
      "0.0376701156942\n",
      "0.0308482177083\n",
      "0.0209939245779\n",
      "0.0170702064894\n",
      "0.0231727800398\n",
      "0.0244097727755\n",
      "0.0311637668683\n",
      "0.0305453744375\n",
      "0.0299327481239\n",
      "0.0250637605482\n",
      "0.0231853905472\n",
      "0.0201803104781\n",
      "0.0166493448119\n",
      "0.0202431677182\n",
      "0.0204329344624\n",
      "0.0127162530071\n",
      "0.0132645683085\n",
      "0.0266650518856\n",
      "0.0205894761728\n",
      "0.0204173564335\n",
      "0.0418999473715\n",
      "0.0251839901427\n",
      "0.0208266909696\n",
      "0.021521426019\n",
      "0.00544479028004\n",
      "0.0157495593969\n",
      "0.025579886341\n",
      "0.0309014620394\n",
      "0.0189917533589\n",
      "0.034657465313\n",
      "0.0299283320189\n",
      "0.0244851332799\n",
      "0.0253637086209\n",
      "0.0258944306248\n",
      "0.025240709336\n",
      "0.00771449390413\n",
      "0.0323096591653\n",
      "0.0121485984901\n",
      "0.0223716064091\n",
      "0.0097251264493\n",
      "0.0201918286298\n",
      "0.033412911739\n",
      "0.0321990647102\n",
      "0.00833134876461\n",
      "0.0139267861615\n",
      "0.0155212109572\n",
      "0.00567384354907\n",
      "0.0119272656417\n",
      "0.0189023282064\n",
      "0.00875350296707\n",
      "0.0220260863014\n",
      "0.0200704857365\n",
      "0.00587905473657\n",
      "0.00373244014132\n",
      "0.0573524569882\n",
      "0.0441674305759\n",
      "0.0678245167541\n",
      "0.0601531184537\n",
      "0.0717905403862\n",
      "0.0368906640266\n",
      "0.0804482594168\n",
      "0.0326983909502\n",
      "0.0591750079803\n",
      "0.0870543604378\n",
      "0.0656770612915\n",
      "0.0346723147419\n",
      "0.0391004166887\n",
      "0.0658320561921\n",
      "0.0395405696105\n",
      "0.0598966681752\n",
      "0.0508646340049\n",
      "0.0419326246264\n",
      "0.0448387579751\n",
      "0.0336490006189\n",
      "0.0322032076041\n",
      "0.0373755413626\n",
      "0.0555858693016\n",
      "0.0754087084324\n",
      "0.0462176811844\n",
      "0.0304181129119\n",
      "0.0442745863063\n",
      "0.0335563010986\n",
      "0.0299814830668\n",
      "0.0340532442025\n",
      "0.0367030368609\n",
      "0.0204785439373\n",
      "0.0234057270901\n",
      "0.0256934549807\n",
      "0.0183934524883\n",
      "0.0350562233773\n",
      "0.00607910718101\n",
      "0.00615870267752\n",
      "0.020568285457\n",
      "0.00868960798793\n",
      "0.0278512466168\n",
      "0.0249779456961\n",
      "0.021013416072\n",
      "0.0311347414916\n",
      "0.0239312799002\n",
      "0.0143677911557\n",
      "0.0200757434143\n",
      "0.0434718943138\n",
      "0.0144778364888\n",
      "0.0202147761203\n",
      "0.0204063643031\n",
      "0.00885038856343\n",
      "0.0167603100156\n",
      "0.0192450627633\n",
      "0.0184915696296\n",
      "0.0256622090154\n",
      "0.0184444073811\n",
      "0.0160162920583\n",
      "0.0301531261122\n",
      "0.0142327227954\n",
      "0.00958945321971\n",
      "0.00747587776761\n",
      "0.0150065020004\n",
      "0.0117417213788\n",
      "0.00455987483531\n",
      "0.0738724635596\n",
      "0.0615311871981\n",
      "0.074887235673\n",
      "0.0370865564337\n",
      "0.0447965698895\n",
      "0.0567119127222\n",
      "0.0618411450474\n",
      "0.0309655405714\n",
      "0.0536651741578\n",
      "0.0465525601566\n",
      "0.0381026300213\n",
      "0.0396744918582\n",
      "0.0507123278421\n",
      "0.0404331089665\n",
      "0.0474594123967\n",
      "0.0238463524294\n",
      "0.0429028732425\n",
      "0.034708541667\n",
      "0.03594691074\n",
      "0.030842008989\n",
      "0.0305263361338\n",
      "0.0297723454957\n",
      "0.029694768047\n",
      "0.0363490865346\n",
      "0.0424126776111\n",
      "0.0409164402863\n",
      "0.0257532923892\n",
      "0.0108459178284\n",
      "0.0355756167634\n",
      "0.00908442663724\n",
      "0.0255459238404\n",
      "0.025566863063\n",
      "0.0175723025204\n",
      "0.0197230539149\n",
      "0.0188397597797\n",
      "0.0419391469823\n",
      "0.0216417728425\n",
      "0.00531443293428\n",
      "0.010824347351\n",
      "0.00743882279638\n",
      "0.0128865509742\n",
      "0.00506929967495\n",
      "0.0244713145945\n",
      "0.015741312016\n",
      "0.00606334498393\n",
      "0.00714878652576\n",
      "0.00233412193455\n",
      "0.0939634839305\n",
      "0.0885919993665\n",
      "0.072488116034\n",
      "0.0436152983495\n",
      "0.0540247457126\n",
      "0.0802228985739\n",
      "0.0519937358442\n",
      "0.0474584089545\n",
      "0.0668009253549\n",
      "0.0356267242604\n",
      "0.0302918908932\n",
      "0.0366783429156\n",
      "0.0437245297329\n",
      "0.0320755487367\n",
      "0.0456094567449\n",
      "0.0392066910888\n",
      "0.0598145452408\n",
      "0.0427891235314\n",
      "0.0476476816562\n",
      "0.0303462075037\n",
      "0.0384449419097\n",
      "0.0274737295498\n",
      "0.0299392600344\n",
      "0.0301746933391\n",
      "0.0324094514998\n",
      "0.0344531945037\n",
      "0.0228724774416\n",
      "0.0440889583991\n",
      "0.0201932244504\n",
      "0.0270026042678\n",
      "0.0405879404979\n",
      "0.0257836808862\n",
      "0.0325650898545\n",
      "0.0103207815297\n",
      "0.0378954024899\n",
      "0.0418465107825\n",
      "0.0338898191772\n",
      "0.0225725058666\n",
      "0.0300663727464\n",
      "0.0238307306671\n",
      "0.00689298757261\n",
      "0.0116142369613\n",
      "0.013057236313\n",
      "0.0249704006953\n",
      "0.0239772717495\n",
      "0.02409957833\n",
      "0.0214819733513\n",
      "0.00961607262535\n",
      "0.016078218162\n",
      "0.00916491263633\n",
      "0.00292952809397\n",
      "0.102825363456\n",
      "0.0998340202882\n",
      "0.0696701928086\n",
      "0.100111915744\n",
      "0.0732210226107\n",
      "0.0412047847888\n",
      "0.0782653551776\n",
      "0.0841097238167\n",
      "0.0750041352837\n",
      "0.0930905209443\n",
      "0.0986967338981\n",
      "0.0992004571162\n",
      "0.0551096116968\n",
      "0.0520570384\n",
      "0.0332375067403\n",
      "0.0542756416422\n",
      "0.0440792908261\n",
      "0.0683641321591\n",
      "0.0574304108502\n",
      "0.0282554159586\n",
      "0.0552437030436\n",
      "0.0366645588752\n",
      "0.044796315712\n",
      "0.0337738163122\n",
      "0.0300213211236\n",
      "0.0310178100381\n",
      "0.0542018175883\n",
      "0.01943940803\n",
      "0.0419253314138\n",
      "0.0288336157825\n",
      "0.0410868860659\n",
      "0.027696767255\n",
      "0.0342583904739\n",
      "0.0353323546293\n",
      "0.0250903988396\n",
      "0.0257727856246\n",
      "0.0360619387481\n",
      "0.0284457769646\n",
      "0.024068313644\n",
      "0.0223604527245\n",
      "0.0234414310402\n",
      "0.0149427921008\n",
      "0.00509176200529\n",
      "0.0234498980219\n",
      "0.00409253286853\n",
      "0.128749857913\n",
      "0.130005651161\n",
      "0.0896922485816\n",
      "0.094393607117\n",
      "0.0707563779538\n",
      "0.0558201968698\n",
      "0.0667718122423\n",
      "0.0552374407276\n",
      "0.0297986518035\n",
      "0.077938404784\n",
      "0.0726520857994\n",
      "0.057591709253\n",
      "0.0499238595757\n",
      "0.0413967828207\n",
      "0.0618957628386\n",
      "0.065312060772\n",
      "0.050195596382\n",
      "0.0406474528489\n",
      "0.0744364502385\n",
      "0.0490798018256\n",
      "0.0357020943195\n",
      "0.0708926324429\n",
      "0.0310653610492\n",
      "0.0400174823388\n",
      "0.0240923800137\n",
      "0.037564841599\n",
      "0.0194718348114\n",
      "0.030147639756\n",
      "0.0198824447847\n",
      "0.0107269048206\n",
      "0.0531118602537\n",
      "0.0447302323508\n",
      "0.0347953240889\n",
      "0.0131539731134\n",
      "0.054201114088\n",
      "0.0327360355487\n",
      "0.0297165042637\n",
      "0.0233930500796\n",
      "0.0279269807625\n",
      "0.0184528010627\n",
      "0.024202541009\n",
      "0.0163841342025\n",
      "0.00508190089002\n",
      "0.0236585350688\n",
      "0.00489843449692\n",
      "0.0932307870127\n",
      "0.0505827810923\n",
      "0.0434142049905\n",
      "0.0526263753859\n",
      "0.0590765280757\n",
      "0.0632335677098\n",
      "0.0358313231647\n",
      "0.0186765307488\n",
      "0.0272452618615\n",
      "0.0548334890348\n",
      "0.0320093952254\n",
      "0.0617240994648\n",
      "0.0464642175184\n",
      "0.0586233557816\n",
      "0.0487054731395\n",
      "0.0397378328561\n",
      "0.0711734791089\n",
      "0.0388358330489\n",
      "0.0390884476718\n",
      "0.0441231563174\n",
      "0.043591926626\n",
      "0.0239496803047\n",
      "0.0352201190745\n",
      "0.0342823335987\n",
      "0.0243446151578\n",
      "0.0156025245795\n",
      "0.0378423612478\n",
      "0.0259749858678\n",
      "0.0313456897517\n",
      "0.0246791270103\n",
      "0.00960269425428\n",
      "0.0249391952258\n",
      "0.0267695687568\n",
      "0.0346340520354\n",
      "0.0307542394416\n",
      "0.0173574509474\n",
      "0.0501475520224\n",
      "0.0132635432328\n",
      "0.0230310137168\n",
      "0.0313164607491\n",
      "0.0292882086283\n",
      "0.0485286788979\n",
      "0.0325503568223\n",
      "0.0180716454103\n",
      "0.0204175204733\n",
      "0.0208130934158\n",
      "0.00489470865428\n",
      "0.090711202299\n",
      "0.0498364177584\n",
      "0.0754413643655\n",
      "0.0532480349486\n",
      "0.0383075484866\n",
      "0.0683795064592\n",
      "0.0602365572154\n",
      "0.036547716756\n",
      "0.0397837573464\n",
      "0.0587578755203\n",
      "0.0322497617672\n",
      "0.031425181481\n",
      "0.0357345234494\n",
      "0.0342308098687\n",
      "0.0413432514373\n",
      "0.0318476147663\n",
      "0.0183574606494\n",
      "0.0105738521226\n",
      "0.0206273998652\n",
      "0.0431265672789\n",
      "0.0241859284577\n",
      "0.0288680259398\n",
      "0.0432820717166\n",
      "0.0290598652915\n",
      "0.0218219736486\n",
      "0.0420380443976\n",
      "0.0508672827666\n",
      "0.0209699624283\n",
      "0.0230465396371\n",
      "0.0333189543774\n",
      "0.022387332276\n",
      "0.0401581755805\n",
      "0.0361869064017\n",
      "0.0128813627574\n",
      "0.0325918726636\n",
      "0.0517355072894\n",
      "0.0176679180834\n",
      "0.0165304359737\n",
      "0.0147507493681\n",
      "0.00858204939039\n",
      "0.025744542523\n",
      "0.00694448769016\n",
      "0.0272622189539\n",
      "0.0289902579668\n",
      "0.0381094045268\n",
      "0.0204146053809\n",
      "0.0215703921381\n",
      "0.0162939570713\n",
      "0.0189669058514\n",
      "0.0164265778872\n",
      "0.0242210593318\n",
      "0.0103452888902\n",
      "0.00986495700911\n",
      "0.0242377856897\n",
      "0.00788904280132\n",
      "0.00766025143606\n",
      "0.0107490640858\n",
      "0.00468482847252\n",
      "0.0544045650397\n",
      "0.0263350255766\n",
      "0.0528319365855\n",
      "0.0430872049569\n",
      "0.0466717708871\n",
      "0.0408575873201\n",
      "0.0425501471017\n",
      "0.0263920326774\n",
      "0.0399951779954\n",
      "0.0206204411871\n",
      "0.0360415377857\n",
      "0.0511751171187\n",
      "0.0349214868011\n",
      "0.0303143513683\n",
      "0.0308687496941\n",
      "0.0355040939137\n",
      "0.0306164827393\n",
      "0.0132729727123\n",
      "0.023317745139\n",
      "0.0162800512459\n",
      "0.0102108864847\n",
      "0.0172632282246\n",
      "0.0113623147005\n",
      "0.0107377670689\n",
      "0.0108823304852\n",
      "0.0364388156842\n",
      "0.0206155947223\n",
      "0.00885896238469\n",
      "0.0156120545757\n",
      "0.0110527307893\n",
      "0.0265428594568\n",
      "0.00740966852983\n",
      "0.0094787369164\n",
      "0.0220119672879\n",
      "0.030113380489\n",
      "0.0294051152388\n",
      "0.00710546073911\n",
      "0.022112648837\n",
      "0.0210616180547\n",
      "0.00726334049871\n",
      "0.00380518599151\n",
      "0.0585746265301\n",
      "0.0645151818065\n",
      "0.0499973688738\n",
      "0.0902687615147\n",
      "0.056156654531\n",
      "0.0715069309531\n",
      "0.049824606231\n",
      "0.0955160804019\n",
      "0.0721465081934\n",
      "0.0617830711945\n",
      "0.0364551055282\n",
      "0.043995207616\n",
      "0.0277956045417\n",
      "0.0282628576775\n",
      "0.031328818451\n",
      "0.0282782813863\n",
      "0.0307975102516\n",
      "0.0471620478431\n",
      "0.0316792295545\n",
      "0.0444869482138\n",
      "0.050192766583\n",
      "0.0407156925682\n",
      "0.0447698663306\n",
      "0.0353167145341\n",
      "0.0316657523354\n",
      "0.0329546097868\n",
      "0.0497613120309\n",
      "0.0206601167235\n",
      "0.0296445241624\n",
      "0.0382170177268\n",
      "0.0474859992948\n",
      "0.0464127999656\n",
      "0.0387091867637\n",
      "0.0315381583263\n",
      "0.0267148570575\n",
      "0.0347769188161\n",
      "0.0140340629403\n",
      "0.0314796879456\n",
      "0.019814561117\n",
      "0.0277810989243\n",
      "0.0419080640183\n",
      "0.0113821461936\n",
      "0.0252066162055\n",
      "0.0192879528228\n",
      "0.00496702402792\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e397dfb3be4dfab64e5acc4a43bc60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0900672211976\n",
      "0.0998216194828\n",
      "0.089888618961\n",
      "0.0540838819282\n",
      "0.066669899464\n",
      "0.069805445531\n",
      "0.133192746215\n",
      "0.0650141348339\n",
      "0.0774120838075\n",
      "0.0177172290146\n",
      "0.0788869968092\n",
      "0.0686686068081\n",
      "0.0504217570082\n",
      "0.0505533087932\n",
      "0.0605064025309\n",
      "0.0555128682206\n",
      "0.0961542389594\n",
      "0.0735665638002\n",
      "0.0210051629046\n",
      "0.0667584272678\n",
      "0.0432405947078\n",
      "0.0337235933262\n",
      "0.0492156306918\n",
      "0.0384008298885\n",
      "0.0351124102327\n",
      "0.0393836502077\n",
      "0.0518138835502\n",
      "0.046192033083\n",
      "0.0626089281936\n",
      "0.0365014571652\n",
      "0.0570174415021\n",
      "0.0252830109377\n",
      "0.0762018298166\n",
      "0.021782927038\n",
      "0.0344846431643\n",
      "0.0497716051583\n",
      "0.0583895230752\n",
      "0.0276082507899\n",
      "0.0391720682064\n",
      "0.0339291758093\n",
      "0.0157975984195\n",
      "0.0291542057803\n",
      "0.0165193832858\n",
      "0.0418554441899\n",
      "0.0610577307584\n",
      "0.0459595434209\n",
      "0.0425091695723\n",
      "0.0338160346776\n",
      "0.0306765578415\n",
      "0.0485784732731\n",
      "0.046772640122\n",
      "0.0592785817718\n",
      "0.0401245405395\n",
      "0.033561594774\n",
      "0.0264184742214\n",
      "0.00558220516006\n",
      "0.0468413125428\n",
      "0.0189349870663\n",
      "0.0420732637734\n",
      "0.0761005334472\n",
      "0.0127392273029\n",
      "0.0160770702687\n",
      "0.0291556003476\n",
      "0.0224575299732\n",
      "0.0681988563467\n",
      "0.0444006859214\n",
      "0.0344009952206\n",
      "0.028014127017\n",
      "0.0292200105244\n",
      "0.0548502263841\n",
      "0.0362907661279\n",
      "0.0292271574291\n",
      "0.0182264498606\n",
      "0.0192155413784\n",
      "0.0305244468076\n",
      "0.0448629010929\n",
      "0.0138770257474\n",
      "0.0113843233848\n",
      "0.0553689881085\n",
      "0.0283510938869\n",
      "0.0423249264779\n",
      "0.0305217377703\n",
      "0.0221064785564\n",
      "0.0224570047049\n",
      "0.0144545081098\n",
      "0.0217365756585\n",
      "0.0170204377673\n",
      "0.0370977863514\n",
      "0.0318853535898\n",
      "0.0580780926203\n",
      "0.0121832955912\n",
      "0.00661393441947\n",
      "0.021480072036\n",
      "0.0396452139595\n",
      "0.0247564630942\n",
      "0.0160766838177\n",
      "0.0523280190947\n",
      "0.022478792782\n",
      "0.031461073047\n",
      "0.0491169761174\n",
      "0.0363995674584\n",
      "0.0227400042482\n",
      "0.0179949559613\n",
      "0.0257002615571\n",
      "0.0256395568881\n",
      "0.0381999707587\n",
      "0.0119271291352\n",
      "0.0272841470837\n",
      "0.0614473201219\n",
      "0.0659497667191\n",
      "0.0133059317951\n",
      "0.0210678605255\n",
      "0.0478241536246\n",
      "0.0212986551042\n",
      "0.0174770933135\n",
      "0.00961425663976\n",
      "0.0771893613755\n",
      "0.00571023325035\n",
      "0.0257469132464\n",
      "0.0309667263006\n",
      "0.0127073839665\n",
      "0.0514526834114\n",
      "0.0237359190821\n",
      "0.00896133267073\n",
      "0.0318309982478\n",
      "0.0484375355071\n",
      "0.0735937372924\n",
      "0.0215055534669\n",
      "0.0358291390632\n",
      "0.0244192020204\n",
      "0.0116022332536\n",
      "0.0498689002304\n",
      "0.0629975902338\n",
      "0.0779555677563\n",
      "0.041086773574\n",
      "0.0186783493313\n",
      "0.0489862947243\n",
      "0.00541025429577\n",
      "0.00065136979718\n",
      "0.0695806966662\n",
      "0.0803917281268\n",
      "0.0917731958507\n",
      "0.082864291491\n",
      "0.102279518566\n",
      "0.0899159113341\n",
      "0.0694016021935\n",
      "0.078466466849\n",
      "0.0692090789156\n",
      "0.0935437189633\n",
      "0.0614835631126\n",
      "0.128309816581\n",
      "0.0806047184036\n",
      "0.0472550331367\n",
      "0.0509876514257\n",
      "0.0976986794005\n",
      "0.0801264823821\n",
      "0.111419092703\n",
      "0.107755393932\n",
      "0.110052981594\n",
      "0.104963958406\n",
      "0.0472229576432\n",
      "0.0609833524132\n",
      "0.0843396729611\n",
      "0.0784106207085\n",
      "0.0728452235322\n",
      "0.0678111802786\n",
      "0.0605176201483\n",
      "0.0749935447668\n",
      "0.048889485837\n",
      "0.0448073634268\n",
      "0.0493063267827\n",
      "0.0571265981697\n",
      "0.0871849697062\n",
      "0.108675403964\n",
      "0.0728455933022\n",
      "0.0427251325383\n",
      "0.0955873053733\n",
      "0.0502910374082\n",
      "0.0566832639162\n",
      "0.0315467159117\n",
      "0.0940275913462\n",
      "0.0388270257955\n",
      "0.0833107295622\n",
      "0.0612080835135\n",
      "0.0455437847883\n",
      "0.0945205105941\n",
      "0.092450292564\n",
      "0.0734223995149\n",
      "0.0740782922173\n",
      "0.067983908211\n",
      "0.0898777637207\n",
      "0.174629546586\n",
      "0.0544274760025\n",
      "0.0646787876022\n",
      "0.0907079508645\n",
      "0.0988091708384\n",
      "0.0799426835\n",
      "0.0575705902298\n",
      "0.039987152545\n",
      "0.0585328589233\n",
      "0.0800854453589\n",
      "0.0722702231856\n",
      "0.0866387420706\n",
      "0.0522416372621\n",
      "0.0541816141786\n",
      "0.053831379609\n",
      "0.0609798858948\n",
      "0.0372089720521\n",
      "0.0948225835846\n",
      "0.0682888234486\n",
      "0.075284625389\n",
      "0.0661982339791\n",
      "0.0912117098485\n",
      "0.0489692758476\n",
      "0.0307258863881\n",
      "0.0217257700583\n",
      "0.0627968912857\n",
      "0.0742649510334\n",
      "0.140095124148\n",
      "0.0351720113744\n",
      "0.104074209098\n",
      "0.0604657523349\n",
      "0.0510357172018\n",
      "0.1025548864\n",
      "0.0203135141591\n",
      "0.10962112634\n",
      "0.0470131745366\n",
      "0.0891778335095\n",
      "0.0426281931637\n",
      "0.0601398457766\n",
      "0.0607439547861\n",
      "0.0879174084059\n",
      "0.0474100351172\n",
      "0.0745564333203\n",
      "0.0791462278935\n",
      "0.116246040957\n",
      "0.0529616177384\n",
      "0.0409093508429\n",
      "0.0371038162587\n",
      "0.118245865268\n",
      "0.0580157624208\n",
      "0.073488283185\n",
      "0.0413400753325\n",
      "0.0560332192161\n",
      "0.0445011539985\n",
      "0.0659100407256\n",
      "0.0872892072332\n",
      "0.0626591488379\n",
      "0.065613996863\n",
      "0.0495840902212\n",
      "0.0778771161439\n",
      "0.0452578159568\n",
      "0.0365820088597\n",
      "0.063556173806\n",
      "0.0697683318243\n",
      "0.0301571836714\n",
      "0.0961990326191\n",
      "0.0703403155868\n",
      "0.0630758367321\n",
      "0.0502950825816\n",
      "0.0250919006714\n",
      "0.053657694276\n",
      "0.0448695575096\n",
      "0.0543325638148\n",
      "0.0224117328214\n",
      "0.0518656968242\n",
      "0.0274713762585\n",
      "0.0443425128262\n",
      "0.0229460065578\n",
      "0.0348704578118\n",
      "0.0256647876125\n",
      "0.0772531926012\n",
      "0.0258388107817\n",
      "0.0488715855697\n",
      "0.032954178433\n",
      "0.0458702968992\n",
      "0.0753493448212\n",
      "0.0418103140158\n",
      "0.0661900469755\n",
      "0.130684423688\n",
      "0.045532693116\n",
      "0.0443185962526\n",
      "0.0569846751241\n",
      "0.0451201891816\n",
      "0.035014813605\n",
      "0.0720469214586\n",
      "0.043586627738\n",
      "0.0328965815031\n",
      "0.0350075125117\n",
      "0.0753251503314\n",
      "0.0252519285369\n",
      "0.067514809055\n",
      "0.0484121892461\n",
      "0.0402286711288\n",
      "0.0397820794221\n",
      "0.0793431869644\n",
      "0.0546820630996\n",
      "0.0601369191183\n",
      "0.0622386096801\n",
      "0.0448861463003\n",
      "0.0451950747181\n",
      "0.0511391626745\n",
      "0.0101300201557\n",
      "0.0644697145026\n",
      "0.0234068555122\n",
      "0.0823956172293\n",
      "0.0836885024393\n",
      "0.0659102343224\n",
      "0.039332736199\n",
      "0.0792867139027\n",
      "0.103598868698\n",
      "0.0519761648211\n",
      "0.0488815188679\n",
      "0.113189794177\n",
      "0.0212143004179\n",
      "0.0649252431964\n",
      "0.0451050424674\n",
      "0.0574738477952\n",
      "0.0646810960644\n",
      "0.0399463909126\n",
      "0.0668569986129\n",
      "0.0554345524579\n",
      "0.0363249027721\n",
      "0.0555031510999\n",
      "0.059852135166\n",
      "0.0576198818058\n",
      "0.0775497532175\n",
      "0.0616736571923\n",
      "0.0357875769597\n",
      "0.0423468398501\n",
      "0.0619959287677\n",
      "0.0514168543098\n",
      "0.0515115794757\n",
      "0.0922348537116\n",
      "0.0318854360051\n",
      "0.0446404530076\n",
      "0.0298545774773\n",
      "0.0350434632574\n",
      "0.0332268637131\n",
      "0.0682889545387\n",
      "0.0336322537199\n",
      "0.107833849825\n",
      "0.0661284002043\n",
      "0.0636035096791\n",
      "0.0353893369353\n",
      "0.0302778628564\n",
      "0.0480868883096\n",
      "0.0595487448498\n",
      "0.0551958156722\n",
      "0.0516968659673\n",
      "0.0534648220114\n",
      "0.0697657810589\n",
      "0.0330874546992\n",
      "0.0610019257055\n",
      "0.0252767866585\n",
      "0.0419507513834\n",
      "0.0216434740795\n",
      "0.0443688582657\n",
      "0.0489083524383\n",
      "0.040557180263\n",
      "0.0463148398484\n",
      "0.0415758730257\n",
      "0.0490196011879\n",
      "0.0586047195556\n",
      "0.0436349720527\n",
      "0.00808372828089\n",
      "0.0283144820158\n",
      "0.0103254614397\n",
      "0.0308429746901\n",
      "0.0336228925368\n",
      "0.0444942026815\n",
      "0.029823522786\n",
      "0.0344570376815\n",
      "0.066177816198\n",
      "0.0225994207027\n",
      "0.0314704176478\n",
      "0.0107538200661\n",
      "0.0156727670067\n",
      "0.0268946649558\n",
      "0.0170545149064\n",
      "0.0123021858391\n",
      "0.0293864721325\n",
      "0.016079641568\n",
      "0.0153721202774\n",
      "0.020678068651\n",
      "0.0374572982881\n",
      "0.0276729259617\n",
      "0.0132199063685\n",
      "0.0171426549779\n",
      "0.0285253899879\n",
      "0.0363977515762\n",
      "0.0528461521219\n",
      "0.0354156466248\n",
      "0.00566922310872\n",
      "0.0137193777619\n",
      "0.0442943519\n",
      "0.0258311563142\n",
      "0.0158871863002\n",
      "0.0181054125546\n",
      "0.0154281507808\n",
      "0.027166715056\n",
      "0.0356533685526\n",
      "0.0376414499269\n",
      "0.0416219600868\n",
      "0.0366793041015\n",
      "0.0194327035542\n",
      "0.0229868179827\n",
      "0.0351627278681\n",
      "0.0221394232922\n",
      "0.0147431958235\n",
      "0.0220946259307\n",
      "0.010756380799\n",
      "0.0093498590746\n",
      "0.0206866752403\n",
      "0.00916094183853\n",
      "0.00429866631041\n",
      "0.0143687200801\n",
      "0.00353354276941\n",
      "0.013013405176\n",
      "0.00907782721281\n",
      "0.011914932595\n",
      "0.0126384028439\n",
      "0.0112460476559\n",
      "0.00958025039173\n",
      "0.00847789790964\n",
      "0.0195232700519\n",
      "0.0461743526576\n",
      "0.0422813195003\n",
      "0.0108470057179\n",
      "0.000480899161205\n",
      "0.0718277727997\n",
      "0.0432589177511\n",
      "0.0623249676592\n",
      "0.0712511913911\n",
      "0.07643644544\n",
      "0.0934230996741\n",
      "0.0656758547179\n",
      "0.0851122298995\n",
      "0.0612454466499\n",
      "0.0628294900442\n",
      "0.100772236015\n",
      "0.0778723685149\n",
      "0.0962498272135\n",
      "0.0481744430012\n",
      "0.0406815348062\n",
      "0.0761482139756\n",
      "0.067906504121\n",
      "0.0384772937503\n",
      "0.0373318927174\n",
      "0.0483275763079\n",
      "0.0901834633229\n",
      "0.0358697638271\n",
      "0.0586615852861\n",
      "0.102280820319\n",
      "0.0669103167164\n",
      "0.0572095081589\n",
      "0.0538530565499\n",
      "0.0360670542154\n",
      "0.0711442950094\n",
      "0.0448577417146\n",
      "0.0543494229903\n",
      "0.0426390178343\n",
      "0.0674631500702\n",
      "0.0784727342989\n",
      "0.087424961269\n",
      "0.039050019392\n",
      "0.0496280256904\n",
      "0.052813267533\n",
      "0.049997919926\n",
      "0.0765645153714\n",
      "0.0242767009518\n",
      "0.0263600008506\n",
      "0.0477608014852\n",
      "0.0499632111538\n",
      "0.0646956285898\n",
      "0.0556749150557\n",
      "0.0229033931479\n",
      "0.0424381511864\n",
      "0.0325986440478\n",
      "0.0457216516934\n",
      "0.0402775374394\n",
      "0.0263325414634\n",
      "0.0378350099707\n",
      "0.0518421545361\n",
      "0.0564495344923\n",
      "0.0241066005588\n",
      "0.0786791836284\n",
      "0.0170801426757\n",
      "0.0406373866549\n",
      "0.038195076489\n",
      "0.00924688366834\n",
      "0.00915087518848\n",
      "0.0356234372881\n",
      "0.00645000432862\n",
      "0.011688810455\n",
      "0.0430598225843\n",
      "0.0477154163253\n",
      "0.0212410007193\n",
      "0.03745927599\n",
      "0.0456065523907\n",
      "0.0318490450044\n",
      "0.0204423379299\n",
      "0.0375396963193\n",
      "0.0196663195495\n",
      "0.0433544243755\n",
      "0.0509237632005\n",
      "0.101333483722\n",
      "0.0285105004141\n",
      "0.0491117433779\n",
      "0.0239338096982\n",
      "0.019251151265\n",
      "0.0203486850205\n",
      "0.0580355031897\n",
      "0.0830211623426\n",
      "0.0567164832321\n",
      "0.0107632438973\n",
      "0.020665657562\n",
      "0.0199925893651\n",
      "0.0587655454556\n",
      "0.027960459389\n",
      "0.0332019211495\n",
      "0.0136423614515\n",
      "0.0263811844785\n",
      "0.05239812737\n",
      "0.0427993088579\n",
      "0.0171860758496\n",
      "0.028047244555\n",
      "0.0408950381058\n",
      "0.0164072957297\n",
      "0.0258019409942\n",
      "0.00149189371125\n",
      "0.00613040280665\n",
      "0.0124330945291\n",
      "0.0260113390338\n",
      "0.0216925063305\n",
      "0.0235391773556\n",
      "0.0144387690026\n",
      "0.00482837037731\n",
      "0.00848341528205\n",
      "0.0284709972358\n",
      "0.0167576789654\n",
      "0.00487422254454\n",
      "0.00806079325381\n",
      "0.00203931607311\n",
      "0.0115928201757\n",
      "0.00109810317058\n",
      "0.00378834687291\n",
      "0.00076302535554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0841044866597\n",
      "0.051205804426\n",
      "0.0609556103113\n",
      "0.0557937071101\n",
      "0.12640382426\n",
      "0.0989985691458\n",
      "0.0400752854387\n",
      "0.0515267227093\n",
      "0.0519593187253\n",
      "0.0829397795999\n",
      "0.0705225561216\n",
      "0.087150185562\n",
      "0.0477177515653\n",
      "0.072345384364\n",
      "0.0557447573738\n",
      "0.0899849494325\n",
      "0.044686767401\n",
      "0.012475958915\n",
      "0.0521261155254\n",
      "0.0403914782369\n",
      "0.0380121170026\n",
      "0.0554553069242\n",
      "0.0275579015282\n",
      "0.0317952071988\n",
      "0.0460943776769\n",
      "0.0370225638411\n",
      "0.071352864054\n",
      "0.0233329988895\n",
      "0.0853745976429\n",
      "0.0696450372501\n",
      "0.0641538996394\n",
      "0.0307882175085\n",
      "0.0934976215703\n",
      "0.0396087713334\n",
      "0.125987709292\n",
      "0.062077377038\n",
      "0.0709479991614\n",
      "0.0409823018199\n",
      "0.0404250327892\n",
      "0.067021258466\n",
      "0.0795811683493\n",
      "0.0533689524821\n",
      "0.0301368844982\n",
      "0.0517737640442\n",
      "0.10278307503\n",
      "0.0353224631835\n",
      "0.0518627596926\n",
      "0.0720102807794\n",
      "0.0213948899212\n",
      "0.0530132459715\n",
      "0.04676154702\n",
      "0.0413141602606\n",
      "0.0480389681351\n",
      "0.0502629486968\n",
      "0.0284176323496\n",
      "0.0334284988327\n",
      "0.0300930763728\n",
      "0.0433122866538\n",
      "0.045505628353\n",
      "0.0280134879179\n",
      "0.102115668576\n",
      "0.0450419579308\n",
      "0.0443926320732\n",
      "0.0343979717039\n",
      "0.0533359518474\n",
      "0.050220065397\n",
      "0.0540890947906\n",
      "0.0357786064463\n",
      "0.0428829895842\n",
      "0.100285502149\n",
      "0.0492699281193\n",
      "0.0574202801226\n",
      "0.0753633995742\n",
      "0.030351055862\n",
      "0.0528453844599\n",
      "0.0474884994244\n",
      "0.0399485093547\n",
      "0.0180635706843\n",
      "0.0707666555655\n",
      "0.0378263890696\n",
      "0.0374092756819\n",
      "0.0732424481001\n",
      "0.0361719470072\n",
      "0.0183317525942\n",
      "0.0571200870817\n",
      "0.0283973101605\n",
      "0.0489257915826\n",
      "0.0328791000616\n",
      "0.0409811246022\n",
      "0.0533301478842\n",
      "0.0765370714218\n",
      "0.0836708850506\n",
      "0.0650624513657\n",
      "0.0599145698233\n",
      "0.0507534201843\n",
      "0.0488112729593\n",
      "0.00735906580558\n",
      "0.0360249270497\n",
      "0.048687481651\n",
      "0.025173709679\n",
      "0.0628361722705\n",
      "0.00586791293951\n",
      "0.0287752680729\n",
      "0.0301563425278\n",
      "0.0879152751052\n",
      "0.0291563340488\n",
      "0.0179525818742\n",
      "0.0547964965666\n",
      "0.0174320142294\n",
      "0.0149902544694\n",
      "0.0344455434198\n",
      "0.0224083923996\n",
      "0.0403857659237\n",
      "0.0341203132718\n",
      "0.0400540237845\n",
      "0.0279741111427\n",
      "0.0263519689163\n",
      "0.0300787999317\n",
      "0.024056687984\n",
      "0.0457586130478\n",
      "0.0259193373599\n",
      "0.0505736439133\n",
      "0.0108220863721\n",
      "0.060628349591\n",
      "0.0251630937686\n",
      "0.0472500113181\n",
      "0.0503228603917\n",
      "0.0154996673277\n",
      "0.0277853122892\n",
      "0.0578956029847\n",
      "0.0432857623577\n",
      "0.0518502844988\n",
      "0.00723840111592\n",
      "0.0316724065466\n",
      "0.0329094661162\n",
      "0.0326270261266\n",
      "0.0530955925438\n",
      "0.0143259667863\n",
      "0.0383464400396\n",
      "0.0231979063544\n",
      "0.0419170423006\n",
      "0.0229230880419\n",
      "0.0340762169896\n",
      "0.0247476056721\n",
      "0.0558603440902\n",
      "0.0332624318868\n",
      "0.0691362991458\n",
      "0.0161613184989\n",
      "0.0188329553742\n",
      "0.0488383985803\n",
      "0.0543199320398\n",
      "0.0236116426065\n",
      "0.0292382699605\n",
      "0.0206186288506\n",
      "0.0385838536348\n",
      "0.0154303083378\n",
      "0.0399414699657\n",
      "0.0350025734275\n",
      "0.0275307816669\n",
      "0.0532325667173\n",
      "0.0535628607632\n",
      "0.0755380002502\n",
      "0.00849729757771\n",
      "0.0241754199977\n",
      "0.0308365550311\n",
      "0.0543568270496\n",
      "0.0913274014853\n",
      "0.0582308515277\n",
      "0.0563243859863\n",
      "0.0230046345414\n",
      "0.0400288307735\n",
      "0.0246572276284\n",
      "0.0371026327672\n",
      "0.00510226561081\n",
      "0.0330167325788\n",
      "0.0255203494953\n",
      "0.020585565538\n",
      "0.0473915642639\n",
      "0.0579571502441\n",
      "0.0142562705193\n",
      "0.0498019164345\n",
      "0.0172693520519\n",
      "0.0467631743809\n",
      "0.0215520317107\n",
      "0.0368070845593\n",
      "0.0320818427596\n",
      "0.0657346184654\n",
      "0.0211546485201\n",
      "0.0406850855032\n",
      "0.0315880695584\n",
      "0.0183162826967\n",
      "0.00971536164173\n",
      "0.01920843555\n",
      "0.0349673944913\n",
      "0.0505378573395\n",
      "0.0170083927363\n",
      "0.0189893848298\n",
      "0.013585134499\n",
      "0.0120272145779\n",
      "0.0532445491725\n",
      "0.0354810579708\n",
      "0.0472773004036\n",
      "0.025690367065\n",
      "0.0396675415471\n",
      "0.0321991198479\n",
      "0.0634464801725\n",
      "0.0437977508022\n",
      "0.0287794017382\n",
      "0.0322157570052\n",
      "0.0473654645212\n",
      "0.0378206832483\n",
      "0.0427300647534\n",
      "0.0207372440085\n",
      "0.00948146909303\n",
      "0.0434260733306\n",
      "0.0141216983941\n",
      "0.05310549221\n",
      "0.0143506213601\n",
      "0.0385363769384\n",
      "0.0313696973599\n",
      "0.0782338469654\n",
      "0.0188597669377\n",
      "0.00455734000254\n",
      "0.0835868601644\n",
      "0.008405123391\n",
      "0.035328826176\n",
      "0.0351340514424\n",
      "0.0291061147839\n",
      "0.0281059567588\n",
      "0.008954345966\n",
      "0.0439469246268\n",
      "0.0362972411807\n",
      "0.0242839758901\n",
      "0.0365961507567\n",
      "0.054408792306\n",
      "0.0401228410533\n",
      "0.0683210125021\n",
      "0.0774201859234\n",
      "0.0220202402014\n",
      "0.0747757734693\n",
      "0.0298774934419\n",
      "0.064221175578\n",
      "0.0247772791756\n",
      "0.0371730936419\n",
      "0.0175022326322\n",
      "0.0225137769158\n",
      "0.0452935616664\n",
      "0.0673459398345\n",
      "0.0149829392726\n",
      "0.0343484025611\n",
      "0.0245088047031\n",
      "0.00201451437627\n",
      "0.0377127077274\n",
      "0.0219767759205\n",
      "0.0299021471177\n",
      "0.0444460696812\n",
      "0.0330810613842\n",
      "0.0528938569493\n",
      "0.0397041320356\n",
      "0.0255364696175\n",
      "0.068906751389\n",
      "0.0229441269349\n",
      "0.0298166141029\n",
      "0.0222711416103\n",
      "0.0276108600966\n",
      "0.0555849004163\n",
      "0.0347926162547\n",
      "0.0386452019006\n",
      "0.0176671104173\n",
      "0.0206678256499\n",
      "0.0315854568967\n",
      "0.0176109005548\n",
      "0.0790495764222\n",
      "0.0166089224689\n",
      "0.0266584812431\n",
      "0.0106637275563\n",
      "0.0218640601961\n",
      "0.0697355886157\n",
      "0.031746977429\n",
      "0.0565619508221\n",
      "0.0316825475339\n",
      "0.0230034598293\n",
      "0.0398840931181\n",
      "0.0446535067961\n",
      "0.0608796433123\n",
      "0.0215009634533\n",
      "0.0321484485926\n",
      "0.0237118632025\n",
      "0.00557883393821\n",
      "0.0127584225153\n",
      "0.0418100235152\n",
      "0.0708528092582\n",
      "0.0572185866283\n",
      "0.0248487434613\n",
      "0.0428674041675\n",
      "0.0247697288495\n",
      "0.0486735858949\n",
      "0.0354194049764\n",
      "0.0438669230557\n",
      "0.0119437859572\n",
      "0.0437370759521\n",
      "0.0431889542626\n",
      "0.0498672953589\n",
      "0.0343250307193\n",
      "0.0514346771389\n",
      "0.0182410393144\n",
      "0.0616138299962\n",
      "0.0380013846827\n",
      "0.0116063812799\n",
      "0.0337432510932\n",
      "0.0219756587391\n",
      "0.0170605146906\n",
      "0.0290364001806\n",
      "0.0342491877145\n",
      "0.0150056012607\n",
      "0.0234657263416\n",
      "0.028442166128\n",
      "0.0354721883371\n",
      "0.0292247383791\n",
      "0.0276436107375\n",
      "0.0136820420738\n",
      "0.0206031033381\n",
      "0.0385596943996\n",
      "0.0237211233537\n",
      "0.0115670686252\n",
      "0.0355725432584\n",
      "0.0582723267095\n",
      "0.0731263777236\n",
      "0.0202228604756\n",
      "0.0545654299197\n",
      "0.00718403240528\n",
      "0.0266536062759\n",
      "0.0386971994466\n",
      "0.0651297747378\n",
      "0.0110027167867\n",
      "0.0194835736623\n",
      "0.0368749539108\n",
      "0.035484184775\n",
      "0.0480857502534\n",
      "0.0421069541225\n",
      "0.0247185557416\n",
      "0.0199424763167\n",
      "0.0120567435005\n",
      "0.015032794121\n",
      "0.00464596088653\n",
      "0.0446925072311\n",
      "0.0395469454198\n",
      "0.043258848035\n",
      "0.0461882248997\n",
      "0.01935899823\n",
      "0.015057061071\n",
      "0.0200509467934\n",
      "0.0255124366604\n",
      "0.0187030853127\n",
      "0.0384295818028\n",
      "0.0402850953149\n",
      "0.0405340524354\n",
      "0.0185787395233\n",
      "0.0190808866039\n",
      "0.0324928296659\n",
      "0.0475189236085\n",
      "0.0236342891312\n",
      "0.0349808367795\n",
      "0.0263025460308\n",
      "0.0333471469958\n",
      "0.0343098470446\n",
      "0.0185439690779\n",
      "0.00492201247491\n",
      "0.00640311284046\n",
      "0.0374940210611\n",
      "0.0358979337082\n",
      "0.0143854277195\n",
      "0.0258105933935\n",
      "0.0147016405074\n",
      "0.029707487726\n",
      "0.0258328716074\n",
      "0.00583196785472\n",
      "0.0549701554685\n",
      "0.0023194362045\n",
      "0.0114602324745\n",
      "0.00528723112089\n",
      "0.0129034561653\n",
      "0.0424264742143\n",
      "0.0222421633089\n",
      "0.00900376763555\n",
      "0.00232224288002\n",
      "0.028825049558\n",
      "0.00456976686939\n",
      "0.0172768285556\n",
      "0.0289276083496\n",
      "0.00237224815053\n",
      "0.0102595012817\n",
      "0.0259078011338\n",
      "0.0191278013968\n",
      "0.00592369599585\n",
      "0.0487663641089\n",
      "0.0199526539674\n",
      "0.00293903624055\n",
      "0.00123007130862\n",
      "0.0303310582307\n",
      "0.0253220983356\n",
      "0.015722081197\n",
      "0.00253740357657\n",
      "0.0254115531877\n",
      "0.0065904649934\n",
      "0.00172214884104\n",
      "0.0346598684538\n",
      "0.0159258196782\n",
      "0.0324948477558\n",
      "0.0254739009004\n",
      "0.020754065749\n",
      "0.0408610960338\n",
      "0.030568128376\n",
      "0.0280418592268\n",
      "0.0373796615855\n",
      "0.0365512585671\n",
      "0.00149308411897\n",
      "0.0128177926157\n",
      "0.00259529620763\n",
      "0.0125802277431\n",
      "0.0149943862741\n",
      "0.00679778959967\n",
      "0.0444669925901\n",
      "0.0472393590639\n",
      "0.00506244453351\n",
      "0.00761911183443\n",
      "0.0363983491318\n",
      "0.00782449687708\n",
      "0.0531769836034\n",
      "0.0144381961324\n",
      "0.00357369456451\n",
      "0.0137423757351\n",
      "0.00732268929505\n",
      "0.0200775614549\n",
      "0.0290231619486\n",
      "0.0273116690232\n",
      "0.0494011707978\n",
      "0.0110219084711\n",
      "0.0344090329661\n",
      "0.0152992499006\n",
      "0.00288125173724\n",
      "0.0211372605369\n",
      "0.00321758783044\n",
      "0.0148081127752\n",
      "0.0177958348536\n",
      "0.00216411931744\n",
      "0.0180221861145\n",
      "0.008272129125\n",
      "0.0116732207395\n",
      "0.0185560161812\n",
      "0.0183815707023\n",
      "0.0226460349326\n",
      "0.0173945453676\n",
      "0.0176776000139\n",
      "0.0158047339598\n",
      "0.0326383255842\n",
      "0.0142710130793\n",
      "0.00580898006769\n",
      "0.0512531284462\n",
      "0.0112353874287\n",
      "0.0374320679794\n",
      "0.020820063345\n",
      "0.00788395018508\n",
      "0.00085666240973\n",
      "0.0510144118749\n",
      "0.0748468530369\n",
      "0.109157432233\n",
      "0.0272202293087\n",
      "0.0854723804522\n",
      "0.0398595285317\n",
      "0.121785398814\n",
      "0.0706992415091\n",
      "0.0339407935811\n",
      "0.0551256034071\n",
      "0.0679580618697\n",
      "0.0299693695908\n",
      "0.052583468295\n",
      "0.0291620683935\n",
      "0.0627636605748\n",
      "0.0514385681594\n",
      "0.0637150172972\n",
      "0.0555840256722\n",
      "0.0891553459914\n",
      "0.0436441634267\n",
      "0.0374603740215\n",
      "0.02946321782\n",
      "0.0549042530663\n",
      "0.0458620731208\n",
      "0.0550223362023\n",
      "0.0261396796214\n",
      "0.0295479040145\n",
      "0.0530087539404\n",
      "0.0461031873298\n",
      "0.0800555425953\n",
      "0.0832101125894\n",
      "0.0546743421394\n",
      "0.030915543549\n",
      "0.0573504798403\n",
      "0.0204879967298\n",
      "0.0586509224666\n",
      "0.0473064573077\n",
      "0.0307501485466\n",
      "0.0742185399873\n",
      "0.0504818733803\n",
      "0.0576650415471\n",
      "0.0262470960358\n",
      "0.066519940167\n",
      "0.0239280583507\n",
      "0.0623959919542\n",
      "0.0486696225823\n",
      "0.0352250137294\n",
      "0.0455527569483\n",
      "0.0555092798591\n",
      "0.0284605655045\n",
      "0.0901358037479\n",
      "0.0411459313912\n",
      "0.0325130105128\n",
      "0.0522070038499\n",
      "0.0499635316025\n",
      "0.0562793917863\n",
      "0.0568952272099\n",
      "0.0497258212198\n",
      "0.0287079007003\n",
      "0.0281464389098\n",
      "0.0309372971691\n",
      "0.0211927272322\n",
      "0.0527233607498\n",
      "0.0417121219395\n",
      "0.0344107136046\n",
      "0.056341933288\n",
      "0.0269998680017\n",
      "0.0370926089565\n",
      "0.0144247378014\n",
      "0.0919170452559\n",
      "0.0615112325293\n",
      "0.0191817211909\n",
      "0.044700398816\n",
      "0.0651829975367\n",
      "0.0598325434106\n",
      "0.0567406607621\n",
      "0.0621731014697\n",
      "0.0214029269079\n",
      "0.0482631537088\n",
      "0.0370223804541\n",
      "0.0519915789813\n",
      "0.0245008088852\n",
      "0.0776029611404\n",
      "0.0443836309255\n",
      "0.0442241210742\n",
      "0.0381297186777\n",
      "0.0380145885721\n",
      "0.0895060209225\n",
      "0.0280066406313\n",
      "0.0988614878335\n",
      "0.0517359670436\n",
      "0.0603651118383\n",
      "0.0463033043473\n",
      "0.0608261459903\n",
      "0.0487849849321\n",
      "0.0281429937627\n",
      "0.0968488024836\n",
      "0.055400140597\n",
      "0.0271916656388\n",
      "0.0949134580652\n",
      "0.0945354300947\n",
      "0.0767384509657\n",
      "0.0621617293471\n",
      "0.0461330249311\n",
      "0.0695045194114\n",
      "0.0462451118319\n",
      "0.0191454417916\n",
      "0.0520874757329\n",
      "0.0398472634541\n",
      "0.0447619723855\n",
      "0.0258201698542\n",
      "0.0460736326644\n",
      "0.0675867483432\n",
      "0.0751793194533\n",
      "0.0123599776302\n",
      "0.0423752721511\n",
      "0.0720146990982\n",
      "0.111503188156\n",
      "0.062096831147\n",
      "0.0327877911062\n",
      "0.0477064141829\n",
      "0.0205049419044\n",
      "0.0754288609119\n",
      "0.0352557232769\n",
      "0.0379486651474\n",
      "0.0522708302821\n",
      "0.0313208450154\n",
      "0.0560088218827\n",
      "0.0245352657292\n",
      "0.0473426441267\n",
      "0.00680144055885\n",
      "0.0215402208882\n",
      "0.0163688156934\n",
      "0.0523639287463\n",
      "0.0479578433711\n",
      "0.050899997098\n",
      "0.0274520928328\n",
      "0.0380003661161\n",
      "0.0402617124301\n",
      "0.0322929274416\n",
      "0.0477214675747\n",
      "0.00902845797698\n",
      "0.00451559678059\n",
      "0.0387856159652\n",
      "0.053950509875\n",
      "0.035017150838\n",
      "0.0056435585162\n",
      "0.0307868773591\n",
      "0.0749567276362\n",
      "0.0461103919245\n",
      "0.0293252851432\n",
      "0.0360867643435\n",
      "0.0364749829351\n",
      "0.0221879682622\n",
      "0.0118955175108\n",
      "0.00683862610548\n",
      "0.0496375784316\n",
      "0.0296491148845\n",
      "0.0360018068479\n",
      "0.0342083697068\n",
      "0.00790597324454\n",
      "0.0208905412674\n",
      "0.00714660126506\n",
      "0.0257904240391\n",
      "0.0256676916089\n",
      "0.0152705837683\n",
      "0.00625305857021\n",
      "0.00778002812838\n",
      "0.00887565037216\n",
      "0.00627991280242\n",
      "0.0146866585367\n",
      "0.00785744625167\n",
      "0.010098196167\n",
      "0.00769519367545\n",
      "0.0236237704185\n",
      "0.0116010451803\n",
      "0.01245250567\n",
      "0.00101501315809\n",
      "0.0150067009015\n",
      "0.00720391358094\n",
      "0.0290929565632\n",
      "0.0187175114838\n",
      "0.0215194364275\n",
      "0.0288849373754\n",
      "0.0057581448059\n",
      "0.00580760496927\n",
      "0.00573538613203\n",
      "0.0214781755204\n",
      "0.00205103226352\n",
      "0.040009442954\n",
      "0.00406703896197\n",
      "0.00343966652568\n",
      "0.0047122855645\n",
      "0.0294730714555\n",
      "0.00726625538806\n",
      "0.00642333242476\n",
      "0.0275875927286\n",
      "0.00417555407685\n",
      "0.00286383965564\n",
      "0.0480429525703\n",
      "0.0197833414757\n",
      "0.0189502915444\n",
      "0.000636300851009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05894993502\n",
      "0.0744570695441\n",
      "0.0907648351432\n",
      "0.0494861428225\n",
      "0.0375552231151\n",
      "0.132024535466\n",
      "0.0819437679955\n",
      "0.0895920198377\n",
      "0.0676974287321\n",
      "0.070932403838\n",
      "0.0502569631951\n",
      "0.0808330330727\n",
      "0.0527337500978\n",
      "0.0831121095459\n",
      "0.075720846545\n",
      "0.0627224065409\n",
      "0.0880206129372\n",
      "0.0408896694741\n",
      "0.125016887224\n",
      "0.0964470885874\n",
      "0.0599335720668\n",
      "0.0306892713117\n",
      "0.0660968723527\n",
      "0.0927397618358\n",
      "0.0331971008433\n",
      "0.0511050191726\n",
      "0.0427142858379\n",
      "0.0830301002107\n",
      "0.0433751487045\n",
      "0.0544057077058\n",
      "0.0715896135358\n",
      "0.0549958498407\n",
      "0.0489903436633\n",
      "0.0453079908364\n",
      "0.0654494894245\n",
      "0.037248968523\n",
      "0.0543027275641\n",
      "0.0355107931948\n",
      "0.0869538894295\n",
      "0.0384554583815\n",
      "0.0379305126645\n",
      "0.0701603922297\n",
      "0.040474239546\n",
      "0.016989296276\n",
      "0.0266085307899\n",
      "0.0189146110941\n",
      "0.047067831685\n",
      "0.0501702588849\n",
      "0.080687221857\n",
      "0.0896801935523\n",
      "0.0576456908523\n",
      "0.0444542234849\n",
      "0.0442661083733\n",
      "0.0597043418398\n",
      "0.0595278840356\n",
      "0.0281802283096\n",
      "0.0252715312179\n",
      "0.0361815235544\n",
      "0.0118582022887\n",
      "0.067366582116\n",
      "0.0731787049273\n",
      "0.085988175499\n",
      "0.0433166117307\n",
      "0.08039104151\n",
      "0.0688297519221\n",
      "0.0420572669132\n",
      "0.0553211689075\n",
      "0.077286623406\n",
      "0.0476891245754\n",
      "0.0319908156367\n",
      "0.0356289063528\n",
      "0.0338135799934\n",
      "0.012650634705\n",
      "0.0578705659206\n",
      "0.0338374567662\n",
      "0.0378349824758\n",
      "0.063431876939\n",
      "0.0503588198966\n",
      "0.0177861081268\n",
      "0.0712325196345\n",
      "0.0800558925262\n",
      "0.0613635314215\n",
      "0.027368018856\n",
      "0.0322932732413\n",
      "0.0461719798274\n",
      "0.0163399449604\n",
      "0.0520056000239\n",
      "0.0398899696902\n",
      "0.0584083409689\n",
      "0.0266958592701\n",
      "0.0513613402705\n",
      "0.044407313735\n",
      "0.0376681671609\n",
      "0.0156826130732\n",
      "0.0673085896085\n",
      "0.0517429050846\n",
      "0.046594363277\n",
      "0.0512099567839\n",
      "0.085735600283\n",
      "0.0323725974226\n",
      "0.0647869127896\n",
      "0.0527874875111\n",
      "0.0363778645468\n",
      "0.0409529322329\n",
      "0.0441967141546\n",
      "0.0496809928229\n",
      "0.0736362114788\n",
      "0.0625467817952\n",
      "0.047706813234\n",
      "0.0393248487061\n",
      "0.0147687364809\n",
      "0.0559527479198\n",
      "0.04088808722\n",
      "0.0654708010565\n",
      "0.0678835529785\n",
      "0.05573533021\n",
      "0.0302973630946\n",
      "0.037077563343\n",
      "0.0403423152039\n",
      "0.0317105094332\n",
      "0.0277475812446\n",
      "0.0488290117602\n",
      "0.0208486238564\n",
      "0.039142184301\n",
      "0.0240809859809\n",
      "0.0235948242948\n",
      "0.00352216052335\n",
      "0.0292308913914\n",
      "0.0347072714972\n",
      "0.084983543143\n",
      "0.0376697058667\n",
      "0.037015571376\n",
      "0.0206620029011\n",
      "0.0218588315541\n",
      "0.00850998249134\n",
      "0.0318829932345\n",
      "0.0367791939264\n",
      "0.0145421047396\n",
      "0.0295476020582\n",
      "0.0277911360482\n",
      "0.0401926545893\n",
      "0.0334880798899\n",
      "0.0211351942964\n",
      "0.0140507060328\n",
      "0.0690164674856\n",
      "0.0516526825559\n",
      "0.00721587853919\n",
      "0.016293766137\n",
      "0.0226345918485\n",
      "0.0134055447949\n",
      "0.0222276908822\n",
      "0.0266629906728\n",
      "0.0105424413048\n",
      "0.00537603903205\n",
      "0.0346556348538\n",
      "0.0106125123966\n",
      "0.0141460292551\n",
      "0.000957466731365\n",
      "0.0650485498699\n",
      "0.0564780562636\n",
      "0.0431086009283\n",
      "0.0793576865473\n",
      "0.0992936773843\n",
      "0.0701313725852\n",
      "0.132422725607\n",
      "0.0449484800375\n",
      "0.0827512577004\n",
      "0.0578398310086\n",
      "0.0203471195404\n",
      "0.0905985916898\n",
      "0.082872963066\n",
      "0.103026179723\n",
      "0.0666818771458\n",
      "0.0907544276455\n",
      "0.100589651753\n",
      "0.0934952423129\n",
      "0.0793302314844\n",
      "0.0567598810569\n",
      "0.034143210167\n",
      "0.0484074769208\n",
      "0.0760868425951\n",
      "0.116243576336\n",
      "0.0823278225723\n",
      "0.0471083424509\n",
      "0.0970630594061\n",
      "0.096467375394\n",
      "0.112933784377\n",
      "0.0721494832193\n",
      "0.0685527072904\n",
      "0.103426582597\n",
      "0.0716040629695\n",
      "0.0767689132179\n",
      "0.17211201701\n",
      "0.0253691552206\n",
      "0.100722627678\n",
      "0.0959691235529\n",
      "0.114671049016\n",
      "0.0426610176352\n",
      "0.0287543180343\n",
      "0.0730057700412\n",
      "0.0299457608772\n",
      "0.0452839499939\n",
      "0.0118123297319\n",
      "0.0408322694875\n",
      "0.0579397630624\n",
      "0.0467972595774\n",
      "0.0504211806459\n",
      "0.0636726473791\n",
      "0.0471248169558\n",
      "0.0780879805125\n",
      "0.00969417870222\n",
      "0.0344158484238\n",
      "0.0346769491448\n",
      "0.0480823326334\n",
      "0.039412716557\n",
      "0.0347744727788\n",
      "0.0557012570314\n",
      "0.0548820569266\n",
      "0.0380202231479\n",
      "0.0259478866595\n",
      "0.0234782794621\n",
      "0.0326813737368\n",
      "0.0525433053203\n",
      "0.0627335319929\n",
      "0.0631073954539\n",
      "0.0496834267441\n",
      "0.010937392523\n",
      "0.0472177362103\n",
      "0.0434377789407\n",
      "0.0314474638223\n",
      "0.0405873285408\n",
      "0.0566783987189\n",
      "0.0523646983663\n",
      "0.0419510823326\n",
      "0.0334505538408\n",
      "0.0535467279501\n",
      "0.0671802767585\n",
      "0.037743406807\n",
      "0.0428209465783\n",
      "0.0647650425635\n",
      "0.0255089154778\n",
      "0.0251125768666\n",
      "0.0469576221962\n",
      "0.0458530677648\n",
      "0.0447108813579\n",
      "0.0727431660449\n",
      "0.0378075819192\n",
      "0.0616937932808\n",
      "0.0483210332853\n",
      "0.0500295734782\n",
      "0.0508206751826\n",
      "0.0521201491627\n",
      "0.0614758178727\n",
      "0.0237266331563\n",
      "0.0465808538017\n",
      "0.0324095642753\n",
      "0.0365879631942\n",
      "0.0320622869281\n",
      "0.0268960619013\n",
      "0.0378465661659\n",
      "0.0639027481201\n",
      "0.0392649814015\n",
      "0.028973905661\n",
      "0.0389539160613\n",
      "0.0435866378075\n",
      "0.0332186569744\n",
      "0.0357820886077\n",
      "0.0587271902081\n",
      "0.063069918285\n",
      "0.0332293423864\n",
      "0.0772560894838\n",
      "0.0530339189485\n",
      "0.0306237169781\n",
      "0.0618825386054\n",
      "0.00519497735406\n",
      "0.023891738046\n",
      "0.00394611535853\n",
      "0.0299021730983\n",
      "0.0454048501821\n",
      "0.00468868979385\n",
      "0.0233344844698\n",
      "0.0215488054515\n",
      "0.0111780845827\n",
      "0.0185069095605\n",
      "0.0085919829526\n",
      "0.0290643534024\n",
      "0.0200150636947\n",
      "0.0208766711821\n",
      "0.0284468001436\n",
      "0.0219811963721\n",
      "0.0450793756784\n",
      "0.0153565306489\n",
      "0.0178296532157\n",
      "0.0404362292581\n",
      "0.0304421996849\n",
      "0.0251519579403\n",
      "0.021644475284\n",
      "0.0221894090826\n",
      "0.00775389775458\n",
      "0.0102838869004\n",
      "0.0104266164764\n",
      "0.0131444968224\n",
      "0.00202238731402\n",
      "0.000859706760991\n",
      "0.0669015761246\n",
      "0.056033259334\n",
      "0.0838809260782\n",
      "0.0873387988794\n",
      "0.079853393757\n",
      "0.0712823208951\n",
      "0.0296755680728\n",
      "0.0625779815746\n",
      "0.0712377778533\n",
      "0.0581172349901\n",
      "0.0967210652004\n",
      "0.114611661093\n",
      "0.158618252352\n",
      "0.0644787150977\n",
      "0.0659215924325\n",
      "0.0701122453621\n",
      "0.0613568218176\n",
      "0.0801666246693\n",
      "0.115295789656\n",
      "0.0844639531739\n",
      "0.0419460166345\n",
      "0.0717476047094\n",
      "0.0950369785235\n",
      "0.0657785127109\n",
      "0.0990134666368\n",
      "0.0636609601124\n",
      "0.100200897454\n",
      "0.105220659096\n",
      "0.0607784783406\n",
      "0.0952967469901\n",
      "0.0528469677987\n",
      "0.060761023235\n",
      "0.110038854067\n",
      "0.0966922596487\n",
      "0.123229844533\n",
      "0.0802756715757\n",
      "0.0978137689493\n",
      "0.15843978343\n",
      "0.123782876894\n",
      "0.111724556599\n",
      "0.0498983122931\n",
      "0.0424345128422\n",
      "0.0749909051016\n",
      "0.0632033205191\n",
      "0.133315150383\n",
      "0.0436847824736\n",
      "0.0537692823972\n",
      "0.0262934534022\n",
      "0.0617239302918\n",
      "0.0921530594409\n",
      "0.0321143651618\n",
      "0.0534301312915\n",
      "0.0587440942153\n",
      "0.0807549727124\n",
      "0.0637011367244\n",
      "0.0718999006198\n",
      "0.050348270412\n",
      "0.103902906974\n",
      "0.0891705429267\n",
      "0.0311875708296\n",
      "0.0600924007929\n",
      "0.031258275678\n",
      "0.0610280490742\n",
      "0.112211760061\n",
      "0.0936434798694\n",
      "0.104904411024\n",
      "0.0340911897921\n",
      "0.0460851184758\n",
      "0.031341692893\n",
      "0.0836927118394\n",
      "0.0517437816274\n",
      "0.066148431835\n",
      "0.0660539564232\n",
      "0.0855239762091\n",
      "0.119382363943\n",
      "0.0854739645101\n",
      "0.0630158627442\n",
      "0.0730809143209\n",
      "0.0369884552381\n",
      "0.0867979091841\n",
      "0.0259366473054\n",
      "0.0305559887817\n",
      "0.0412980503892\n",
      "0.0166132283644\n",
      "0.0437401134927\n",
      "0.0326131300423\n",
      "0.0458481852821\n",
      "0.0343438906675\n",
      "0.0550816858578\n",
      "0.0306065467649\n",
      "0.0336732614653\n",
      "0.0621860585073\n",
      "0.0426376959675\n",
      "0.0390366366831\n",
      "0.090051712616\n",
      "0.0279634760883\n",
      "0.0393141530289\n",
      "0.0817259899215\n",
      "0.0441098826908\n",
      "0.0214820267486\n",
      "0.0367012071907\n",
      "0.038349139222\n",
      "0.077005272314\n",
      "0.0241502406602\n",
      "0.0578207003494\n",
      "0.0543182750188\n",
      "0.0386714982387\n",
      "0.0589012798961\n",
      "0.0162129643431\n",
      "0.0481398609829\n",
      "0.0407649779512\n",
      "0.0976375900423\n",
      "0.0765324709998\n",
      "0.0699069218518\n",
      "0.0267735644334\n",
      "0.0733643167039\n",
      "0.0623461326924\n",
      "0.0495208010338\n",
      "0.0317859030569\n",
      "0.0432909725872\n",
      "0.0527165668641\n",
      "0.111927613355\n",
      "0.0384979691863\n",
      "0.00478836541725\n",
      "0.0578666150948\n",
      "0.0471549883724\n",
      "0.0438004075555\n",
      "0.0201561507679\n",
      "0.0547548445616\n",
      "0.02341549101\n",
      "0.0640795926687\n",
      "0.0482499916465\n",
      "0.0217175798389\n",
      "0.0749485999566\n",
      "0.0484660262067\n",
      "0.06658920564\n",
      "0.0639382461788\n",
      "0.0620061014249\n",
      "0.0533376242641\n",
      "0.0393097266186\n",
      "0.0336244754175\n",
      "0.0631615426565\n",
      "0.0253323403323\n",
      "0.0319005713215\n",
      "0.0865280316339\n",
      "0.0603631622736\n",
      "0.0161584807784\n",
      "0.0491774937297\n",
      "0.0792297353161\n",
      "0.0462789675618\n",
      "0.0159734744597\n",
      "0.0529973207318\n",
      "0.0747096614088\n",
      "0.0277284016673\n",
      "0.0645847822072\n",
      "0.0143791100175\n",
      "0.0356875956552\n",
      "0.0199302339284\n",
      "0.0427277177567\n",
      "0.0777847690752\n",
      "0.0327265544727\n",
      "0.0374251684679\n",
      "0.0571335367642\n",
      "0.0161550213465\n",
      "0.0225706706388\n",
      "0.0745677110019\n",
      "0.0344256513906\n",
      "0.0333920473985\n",
      "0.0353646008951\n",
      "0.0284389784812\n",
      "0.0741887466582\n",
      "0.013897616931\n",
      "0.0108987315952\n",
      "0.0246956427644\n",
      "0.021176813978\n",
      "0.0397450137505\n",
      "0.0513109839087\n",
      "0.0193917450824\n",
      "0.0108323326133\n",
      "0.0151824553632\n",
      "0.00373074125842\n",
      "0.0199359623434\n",
      "0.0230130043102\n",
      "0.0130982973444\n",
      "0.0154431379073\n",
      "0.0107141481873\n",
      "0.00729718288309\n",
      "0.00512498315916\n",
      "0.0189290471312\n",
      "0.00716082699111\n",
      "0.0274466615515\n",
      "0.0196743595567\n",
      "0.0182697639625\n",
      "0.0175589695642\n",
      "0.00844592202635\n",
      "0.00410413580034\n",
      "0.0232265724106\n",
      "0.0200427481881\n",
      "0.00264940757703\n",
      "0.0029278139239\n",
      "0.0101852806345\n",
      "0.0138529160598\n",
      "0.0151525594354\n",
      "0.00334663196305\n",
      "0.00883882715423\n",
      "0.0532510431523\n",
      "0.0344667402836\n",
      "0.00414907986742\n",
      "0.0205888370888\n",
      "0.0043675821787\n",
      "0.0205102594195\n",
      "0.03779667995\n",
      "0.00943825444797\n",
      "0.0133522820452\n",
      "0.000886608839529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0291962205964\n",
      "0.0478582100364\n",
      "0.133263025251\n",
      "0.0574022053477\n",
      "0.0549461971434\n",
      "0.0668567454194\n",
      "0.066721040567\n",
      "0.0562758720225\n",
      "0.0686344064254\n",
      "0.0619538029343\n",
      "0.0486069240017\n",
      "0.0459980085159\n",
      "0.0832220724603\n",
      "0.0834313340486\n",
      "0.0576253071207\n",
      "0.0649724904937\n",
      "0.0751778646335\n",
      "0.0398559328021\n",
      "0.0308585771822\n",
      "0.068392551665\n",
      "0.0212118337027\n",
      "0.0478683085713\n",
      "0.0321040993436\n",
      "0.0631872120131\n",
      "0.0715626411695\n",
      "0.0433695803391\n",
      "0.075013301263\n",
      "0.0376969601042\n",
      "0.0273287985302\n",
      "0.0468301205981\n",
      "0.0704220074569\n",
      "0.0776019406775\n",
      "0.0755380841962\n",
      "0.0318569441637\n",
      "0.0386866681611\n",
      "0.0246568167421\n",
      "0.0334604625537\n",
      "0.031406268441\n",
      "0.0393514518897\n",
      "0.0252036218104\n",
      "0.0294729097966\n",
      "0.0215618511687\n",
      "0.072687406422\n",
      "0.0313322953189\n",
      "0.0131030931746\n",
      "0.0630799801937\n",
      "0.0357828360273\n",
      "0.0723612746471\n",
      "0.0349860402376\n",
      "0.0140236416809\n",
      "0.0376305212607\n",
      "0.0437546327968\n",
      "0.0303771604385\n",
      "0.00952781260658\n",
      "0.0311256304187\n",
      "0.0346077938326\n",
      "0.0311591585885\n",
      "0.0469645888386\n",
      "0.0497404740747\n",
      "0.0866615246453\n",
      "0.0407810871768\n",
      "0.023266860585\n",
      "0.0683765439823\n",
      "0.0352738674496\n",
      "0.061725520641\n",
      "0.0223860128223\n",
      "0.027023762424\n",
      "0.0330785991373\n",
      "0.0313880602909\n",
      "0.0117823926242\n",
      "0.0189906643451\n",
      "0.013555969276\n",
      "0.0582554035319\n",
      "0.04418203422\n",
      "0.0358987821809\n",
      "0.0228447430903\n",
      "0.0455891039683\n",
      "0.0238560845841\n",
      "0.0417586766463\n",
      "0.0337197614348\n",
      "0.0659113825164\n",
      "0.0274960033286\n",
      "0.0163835612563\n",
      "0.0353180426396\n",
      "0.0501633912854\n",
      "0.0356314836938\n",
      "0.0236698785312\n",
      "0.0190194328357\n",
      "0.0176336624142\n",
      "0.0284618851437\n",
      "0.0299780432431\n",
      "0.0342543621681\n",
      "0.0423677532777\n",
      "0.0166745923783\n",
      "0.0498163427123\n",
      "0.0354217065957\n",
      "0.0897719555819\n",
      "0.0307579357399\n",
      "0.0144464882355\n",
      "0.0331058097706\n",
      "0.059731513949\n",
      "0.0162853546127\n",
      "0.0528181638127\n",
      "0.0265542071872\n",
      "0.0144787141984\n",
      "0.0435985273449\n",
      "0.0135968476484\n",
      "0.0293146114879\n",
      "0.0140485772142\n",
      "0.0170628179195\n",
      "0.00574528495957\n",
      "0.0170983039546\n",
      "0.00365689411945\n",
      "0.00215290545394\n",
      "0.013057270686\n",
      "0.0016177932808\n",
      "0.0198634502939\n",
      "0.0543086475077\n",
      "0.0044125297371\n",
      "0.0149584896333\n",
      "0.0165169241208\n",
      "0.0175699994373\n",
      "0.0236964598426\n",
      "0.0220541064965\n",
      "0.0200012161008\n",
      "0.014966769441\n",
      "0.00174130285133\n",
      "0.00744220935749\n",
      "0.0330701636562\n",
      "0.00464139848583\n",
      "0.0242541433443\n",
      "0.0422041490662\n",
      "0.0118397072386\n",
      "0.0151845687219\n",
      "0.0162071739491\n",
      "0.00635208409585\n",
      "0.0339839477202\n",
      "0.0180181980983\n",
      "0.000736488276993\n",
      "0.0691483980328\n",
      "0.0159724899769\n",
      "0.0877060720931\n",
      "0.0553060817785\n",
      "0.0435076921783\n",
      "0.0387200561448\n",
      "0.0543417634791\n",
      "0.10627215814\n",
      "0.0341628367861\n",
      "0.0332681580623\n",
      "0.0315674408708\n",
      "0.0889148162755\n",
      "0.0792942507423\n",
      "0.047851230434\n",
      "0.0345621486934\n",
      "0.0288825085688\n",
      "0.0408962197143\n",
      "0.0205609181448\n",
      "0.0482884197049\n",
      "0.04046405378\n",
      "0.0205415937971\n",
      "0.0129609496578\n",
      "0.0639733544452\n",
      "0.073181642081\n",
      "0.0322621933201\n",
      "0.0290292477522\n",
      "0.0368033709295\n",
      "0.0327654994357\n",
      "0.0446056177272\n",
      "0.0252484481393\n",
      "0.0377241907222\n",
      "0.048171246848\n",
      "0.0535036395433\n",
      "0.0542947857175\n",
      "0.0436465984947\n",
      "0.0327532300129\n",
      "0.0496453814098\n",
      "0.0495651911268\n",
      "0.0328980064507\n",
      "0.0844310675177\n",
      "0.0557669489982\n",
      "0.0322631640174\n",
      "0.0756588247759\n",
      "0.0420282527811\n",
      "0.015658995252\n",
      "0.0205160156744\n",
      "0.104505422804\n",
      "0.0394245285879\n",
      "0.0279586215315\n",
      "0.0217621881369\n",
      "0.0770592144055\n",
      "0.0456270350748\n",
      "0.11000041097\n",
      "0.0934962790163\n",
      "0.0647694969943\n",
      "0.0264108073999\n",
      "0.0538964825857\n",
      "0.0367377121774\n",
      "0.053106150009\n",
      "0.071030179653\n",
      "0.0254705536375\n",
      "0.0659893817773\n",
      "0.0473896807805\n",
      "0.055510877468\n",
      "0.0542036440423\n",
      "0.0557625396343\n",
      "0.0357077332764\n",
      "0.0568603177969\n",
      "0.0692131888105\n",
      "0.0449011955701\n",
      "0.0589099828367\n",
      "0.0692091197366\n",
      "0.0291499820462\n",
      "0.0515299139066\n",
      "0.069519020145\n",
      "0.0579030445963\n",
      "0.0565868746666\n",
      "0.0328707299179\n",
      "0.0281760006954\n",
      "0.0199319300792\n",
      "0.00592611000761\n",
      "0.016572162507\n",
      "0.0264056982745\n",
      "0.0124570187848\n",
      "0.00155051329097\n",
      "0.00669054394169\n",
      "0.0107421790357\n",
      "0.00354272537988\n",
      "0.0147767420077\n",
      "0.0113556595787\n",
      "0.018458748968\n",
      "0.00632234618803\n",
      "0.00443012552808\n",
      "0.0278325727981\n",
      "0.00602859099812\n",
      "0.0159774616965\n",
      "0.0128602603117\n",
      "0.0145640238297\n",
      "0.00173790665528\n",
      "0.0183229993205\n",
      "0.0114539122252\n",
      "0.00372476781136\n",
      "0.00457849795821\n",
      "0.0116196604866\n",
      "0.0224769294419\n",
      "0.00971366787074\n",
      "0.00924560948028\n",
      "0.0126163762043\n",
      "0.0113620325022\n",
      "0.00601841309634\n",
      "0.0289866503396\n",
      "0.0147821671339\n",
      "0.00529158596682\n",
      "0.0243936385727\n",
      "0.0371829882841\n",
      "0.0161570192626\n",
      "0.00503048518183\n",
      "0.00450268610432\n",
      "0.0127515130815\n",
      "0.0166586056795\n",
      "0.00868651184224\n",
      "0.00197105862504\n",
      "0.00512503970123\n",
      "0.0173251219598\n",
      "0.0427292379597\n",
      "0.01731629012\n",
      "0.00438696468479\n",
      "0.0207020347688\n",
      "0.0141673589951\n",
      "0.00416272623644\n",
      "0.00669803077555\n",
      "0.0569794454261\n",
      "0.0112725834716\n",
      "0.0111058977521\n",
      "0.00910403732493\n",
      "0.0156500310135\n",
      "0.00212851456217\n",
      "0.0208042247938\n",
      "0.00252232370294\n",
      "0.0209333368019\n",
      "0.0229993579961\n",
      "0.0271501800445\n",
      "0.0136388349869\n",
      "0.0479390129076\n",
      "0.0169474083129\n",
      "0.00834621746583\n",
      "0.0326398494383\n",
      "0.0114348473351\n",
      "0.00567243587065\n",
      "0.00466019943852\n",
      "0.0124035422048\n",
      "0.0162587290241\n",
      "0.0209031766804\n",
      "0.00492696420398\n",
      "0.0262197457666\n",
      "0.0229607386169\n",
      "0.023774540249\n",
      "0.0243576877377\n",
      "0.0150984740653\n",
      "0.0333305592318\n",
      "0.00183007288758\n",
      "0.0267509407079\n",
      "0.0158092208943\n",
      "0.0268119727761\n",
      "0.0164475164164\n",
      "0.030428900719\n",
      "0.00204763337261\n",
      "0.00244253692663\n",
      "0.0256687491848\n",
      "0.0245457388857\n",
      "0.0277196547672\n",
      "0.0268690697413\n",
      "0.0126883258928\n",
      "0.0026871232616\n",
      "0.00290101865651\n",
      "0.0250026470919\n",
      "0.0282457828196\n",
      "0.020409352227\n",
      "0.052426291973\n",
      "0.0228832815379\n",
      "0.0231087026866\n",
      "0.0192385349187\n",
      "0.00174192651\n",
      "0.0245312898858\n",
      "0.0192015459367\n",
      "0.0278134496184\n",
      "0.0176995881749\n",
      "0.0285296883483\n",
      "0.0100619303782\n",
      "0.0107945476325\n",
      "0.0270917747145\n",
      "0.00502722605721\n",
      "0.0278501588805\n",
      "0.0142864021112\n",
      "0.00285665844649\n",
      "0.00408506936112\n",
      "0.0237056015137\n",
      "0.0216175327268\n",
      "0.0181933693767\n",
      "0.00148783831934\n",
      "0.00367263678718\n",
      "0.00788349815325\n",
      "0.0107544961556\n",
      "0.0268147038936\n",
      "0.00580107788775\n",
      "0.0362839848634\n",
      "0.0133759246293\n",
      "0.0100226902467\n",
      "0.0435568997344\n",
      "0.0197700107141\n",
      "0.0179834103175\n",
      "0.00531927455678\n",
      "0.00884466654427\n",
      "0.00419936685981\n",
      "0.0168756184365\n",
      "0.0047608566749\n",
      "0.00888613147195\n",
      "0.0134636872289\n",
      "0.0142601511109\n",
      "0.0333132244506\n",
      "0.0254815020359\n",
      "0.0252193329164\n",
      "0.0234941534291\n",
      "0.0223699379644\n",
      "0.0123335300837\n",
      "0.0334518316468\n",
      "0.0227931154801\n",
      "0.0162910214379\n",
      "0.0145940749096\n",
      "0.00492537019298\n",
      "0.0152035919794\n",
      "0.00644353174445\n",
      "0.00738277694456\n",
      "0.00570888912545\n",
      "0.00448647010527\n",
      "0.0173594320164\n",
      "0.00760046228623\n",
      "0.0297111499699\n",
      "0.00816155574164\n",
      "0.0253848802961\n",
      "0.00580733205016\n",
      "0.0184576550021\n",
      "0.0544383706272\n",
      "0.0365513455304\n",
      "0.0181722615069\n",
      "0.026919883189\n",
      "0.00446035853068\n",
      "0.00177684797607\n",
      "0.015493287567\n",
      "0.0401166921464\n",
      "0.00187650858774\n",
      "0.0245740434168\n",
      "0.0027805776848\n",
      "0.0180455837612\n",
      "0.0253088491546\n",
      "0.0275749774693\n",
      "0.0022667322228\n",
      "0.00501535923696\n",
      "0.00968945155949\n",
      "0.0139982314707\n",
      "0.027084395926\n",
      "0.0232844490095\n",
      "0.0159891037644\n",
      "0.020280347719\n",
      "0.00216253590509\n",
      "0.017588724085\n",
      "0.0186805685563\n",
      "0.0143182878049\n",
      "0.0158909507661\n",
      "0.0314962927616\n",
      "0.00988064086537\n",
      "0.0123101048089\n",
      "0.000539782854553\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e168d400d99413a90108d6d00cb6f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0671119639198\n",
      "0.0625484016621\n",
      "0.0687669324811\n",
      "0.0528483125702\n",
      "0.0381778801525\n",
      "0.043308513339\n",
      "0.0724770257576\n",
      "0.0691076827523\n",
      "0.0777376120283\n",
      "0.0809964318463\n",
      "0.049458925352\n",
      "0.0692068356446\n",
      "0.0955987036974\n",
      "0.0387409584342\n",
      "0.0630697219326\n",
      "0.0745705597822\n",
      "0.0553178245238\n",
      "0.0769541526812\n",
      "0.0951663517555\n",
      "0.0495395615966\n",
      "0.0555618173633\n",
      "0.0483387978278\n",
      "0.0361843645598\n",
      "0.0338052559333\n",
      "0.0731640310356\n",
      "0.0172456025015\n",
      "0.0328168104063\n",
      "0.0543157636642\n",
      "0.0443012203416\n",
      "0.0506407363979\n",
      "0.05869708278\n",
      "0.0528554770161\n",
      "0.0243438942792\n",
      "0.0413156811666\n",
      "0.0286492218062\n",
      "0.0453780198484\n",
      "0.0268062925393\n",
      "0.0301786649581\n",
      "0.0160753111475\n",
      "0.0536392351845\n",
      "0.0484586289829\n",
      "0.0447744296042\n",
      "0.0432363043579\n",
      "0.0463257242869\n",
      "0.0449361832221\n",
      "0.0242422038423\n",
      "0.0299849572099\n",
      "0.0398217915865\n",
      "0.048810044532\n",
      "0.0311966623784\n",
      "0.0505054166683\n",
      "0.0233715598729\n",
      "0.0343795838336\n",
      "0.0381990935905\n",
      "0.0270950175563\n",
      "0.0281086028719\n",
      "0.0535077441619\n",
      "0.0285459964283\n",
      "0.0402653606643\n",
      "0.0190147137913\n",
      "0.0378596938422\n",
      "0.0226707850468\n",
      "0.0257206763117\n",
      "0.0440314768913\n",
      "0.0398193265265\n",
      "0.0554953957836\n",
      "0.0546719091593\n",
      "0.030592676781\n",
      "0.0313401356297\n",
      "0.0180238804037\n",
      "0.0113728732021\n",
      "0.0332608896272\n",
      "0.0556476642372\n",
      "0.0201543231002\n",
      "0.0367082744044\n",
      "0.0250067281743\n",
      "0.0533811290562\n",
      "0.0339007641339\n",
      "0.0192897140547\n",
      "0.0402123897075\n",
      "0.0208338620579\n",
      "0.0164989966202\n",
      "0.0253584609928\n",
      "0.0344299537131\n",
      "0.0352609104721\n",
      "0.0433604612598\n",
      "0.0161785390112\n",
      "0.0261601744509\n",
      "0.0244672187626\n",
      "0.0412502153837\n",
      "0.0454837249901\n",
      "0.0251097981989\n",
      "0.014506572588\n",
      "0.0246395567308\n",
      "0.0179218976417\n",
      "0.0373089590391\n",
      "0.0396312604397\n",
      "0.0268620721275\n",
      "0.0198114063518\n",
      "0.00980515254237\n",
      "0.0172821383494\n",
      "0.0139287212831\n",
      "0.0102872426008\n",
      "0.0135475272306\n",
      "0.0190132092578\n",
      "0.0129678354499\n",
      "0.00821850991756\n",
      "0.0152788428061\n",
      "0.00769865307764\n",
      "0.0254185284517\n",
      "0.0160513633743\n",
      "0.00444281475735\n",
      "0.00472991614798\n",
      "0.0163765670093\n",
      "0.00905655172442\n",
      "0.00873478745196\n",
      "0.0137192006465\n",
      "0.0105897325347\n",
      "0.0135836217186\n",
      "0.0202709456306\n",
      "0.00662380266067\n",
      "0.0319750388581\n",
      "0.00285760302432\n",
      "0.0210727232476\n",
      "0.0137210027102\n",
      "0.00548324851808\n",
      "0.018054024911\n",
      "0.0137935619927\n",
      "0.0261165770345\n",
      "0.0200616352581\n",
      "0.0254734710346\n",
      "0.0103393860939\n",
      "0.0220660261242\n",
      "0.0066865606453\n",
      "0.00885268636149\n",
      "0.0273230898556\n",
      "0.0190583835874\n",
      "0.00974552743224\n",
      "0.0204807364915\n",
      "0.0135936020813\n",
      "0.0179232019969\n",
      "0.0122298048261\n",
      "0.0151481166395\n",
      "0.00511151955825\n",
      "0.0111388375978\n",
      "0.0154055182334\n",
      "0.037471879667\n",
      "0.0173305832687\n",
      "0.011111816395\n",
      "0.0169470856111\n",
      "0.00411412385468\n",
      "0.0119764519511\n",
      "0.0180190374474\n",
      "0.0138912494536\n",
      "0.0084374588327\n",
      "0.0183122301259\n",
      "0.00776201074544\n",
      "0.0140631625226\n",
      "0.0237193885903\n",
      "0.0163882033097\n",
      "0.0109436732205\n",
      "0.0172698190134\n",
      "0.0177196131641\n",
      "0.0280831046026\n",
      "0.0251611311798\n",
      "0.0220666338134\n",
      "0.0181978633563\n",
      "0.0135768052065\n",
      "0.0150535347967\n",
      "0.0133007499909\n",
      "0.0211758308534\n",
      "0.0112019277859\n",
      "0.0151568320527\n",
      "0.0149768106749\n",
      "0.0122695565264\n",
      "0.00602324254331\n",
      "0.0136785715146\n",
      "0.00630933511434\n",
      "0.0164479648161\n",
      "0.0151084901442\n",
      "0.00303776757396\n",
      "0.0169568276874\n",
      "0.0106921524611\n",
      "0.00982122861393\n",
      "0.00158639354578\n",
      "0.00094301427426\n",
      "0.095408398778\n",
      "0.0747982169149\n",
      "0.0501482839228\n",
      "0.077828721458\n",
      "0.0562347624307\n",
      "0.10685102244\n",
      "0.0684566503018\n",
      "0.0547700833044\n",
      "0.10497938412\n",
      "0.0708291809462\n",
      "0.0697893151916\n",
      "0.0697215351259\n",
      "0.0806452301791\n",
      "0.0554308819033\n",
      "0.0644346843693\n",
      "0.063249255466\n",
      "0.0900541553893\n",
      "0.0496533984926\n",
      "0.0751513181011\n",
      "0.054987564275\n",
      "0.0991269582872\n",
      "0.0788197406769\n",
      "0.0706118555968\n",
      "0.0772138607892\n",
      "0.0873690948794\n",
      "0.0774443386393\n",
      "0.0882601145631\n",
      "0.0604414189527\n",
      "0.0626025193217\n",
      "0.075746999813\n",
      "0.0651125871274\n",
      "0.065313372819\n",
      "0.0462357253693\n",
      "0.049462184679\n",
      "0.0776035279662\n",
      "0.0833088066175\n",
      "0.0738520384084\n",
      "0.0676920732144\n",
      "0.0399475439775\n",
      "0.0420438108772\n",
      "0.0538243217468\n",
      "0.0746158287884\n",
      "0.0724145014216\n",
      "0.0710233779889\n",
      "0.106369139327\n",
      "0.10256206588\n",
      "0.0854716247569\n",
      "0.0657710646962\n",
      "0.0547336519462\n",
      "0.0508121921087\n",
      "0.0858852884337\n",
      "0.0495377922978\n",
      "0.0560009809033\n",
      "0.0860590407541\n",
      "0.0408320307386\n",
      "0.0643348285616\n",
      "0.0638913711023\n",
      "0.0699327880884\n",
      "0.0575800091193\n",
      "0.0866599286973\n",
      "0.0916283228762\n",
      "0.0631932038405\n",
      "0.0581964444827\n",
      "0.0434079373771\n",
      "0.0633847268492\n",
      "0.0470363691832\n",
      "0.0497658479319\n",
      "0.0323029062872\n",
      "0.0528141144932\n",
      "0.075100277237\n",
      "0.0553617155417\n",
      "0.0438859243085\n",
      "0.033337951872\n",
      "0.064217240897\n",
      "0.0593163562289\n",
      "0.0652930958121\n",
      "0.0478525876845\n",
      "0.0755042879856\n",
      "0.0598104246034\n",
      "0.0645413677353\n",
      "0.0419913406103\n",
      "0.0517452361928\n",
      "0.0419375837398\n",
      "0.028054777391\n",
      "0.025058864438\n",
      "0.0524044891286\n",
      "0.045765372632\n",
      "0.0278155540803\n",
      "0.052780770359\n",
      "0.0336361023151\n",
      "0.0358906008371\n",
      "0.0581900629052\n",
      "0.0408426312001\n",
      "0.0257943648197\n",
      "0.0355268581032\n",
      "0.0226619592369\n",
      "0.0335446969362\n",
      "0.0365735430227\n",
      "0.0392773252295\n",
      "0.0302310739183\n",
      "0.0615463863171\n",
      "0.0600667457167\n",
      "0.0658708914515\n",
      "0.0267778413696\n",
      "0.0356625764426\n",
      "0.0539600551032\n",
      "0.0436269964145\n",
      "0.0391542597287\n",
      "0.0365297231005\n",
      "0.0353138878911\n",
      "0.0297304731562\n",
      "0.0322837271542\n",
      "0.0285132555541\n",
      "0.0500450348494\n",
      "0.039373480079\n",
      "0.0405789867956\n",
      "0.0121832512717\n",
      "0.0711712707358\n",
      "0.0317389737421\n",
      "0.0279512854115\n",
      "0.0437263527646\n",
      "0.0368239278794\n",
      "0.0421410694206\n",
      "0.0196611834086\n",
      "0.0303213431037\n",
      "0.0221182364255\n",
      "0.027850888892\n",
      "0.0281120799382\n",
      "0.0302705594839\n",
      "0.0166596097792\n",
      "0.0269465342358\n",
      "0.0375589062204\n",
      "0.0181343410048\n",
      "0.0438764847544\n",
      "0.0308072249681\n",
      "0.033957118662\n",
      "0.0414117100219\n",
      "0.014254725943\n",
      "0.00916706141359\n",
      "0.0124544838725\n",
      "0.0433203569865\n",
      "0.0107436781611\n",
      "0.0290420248437\n",
      "0.00739667752186\n",
      "0.0173592453634\n",
      "0.0122882708574\n",
      "0.0167967641198\n",
      "0.00654285770506\n",
      "0.0104020336017\n",
      "0.0344029252363\n",
      "0.0157354748057\n",
      "0.00643113066312\n",
      "0.00513403352979\n",
      "0.00761827471405\n",
      "0.0223957274451\n",
      "0.00582752320931\n",
      "0.0275994779567\n",
      "0.0238155986686\n",
      "0.0183300059544\n",
      "0.0176694098138\n",
      "0.0223219963747\n",
      "0.0248156457631\n",
      "0.013134541102\n",
      "0.00841716437509\n",
      "0.0153572193288\n",
      "0.00639236536192\n",
      "0.0399178191964\n",
      "0.0229276969776\n",
      "0.0541997688288\n",
      "0.00930518828082\n",
      "0.0204594203328\n",
      "0.00547777694638\n",
      "0.0103908658138\n",
      "0.0158081697076\n",
      "0.0157835119174\n",
      "0.00933874281538\n",
      "0.0282029845319\n",
      "0.0134417674492\n",
      "0.00392386798759\n",
      "0.000367191316106\n",
      "0.0545594945271\n",
      "0.0932324149523\n",
      "0.0924509120406\n",
      "0.0533322198624\n",
      "0.0539583908148\n",
      "0.0694050418982\n",
      "0.105640295832\n",
      "0.0641687850255\n",
      "0.0759955329333\n",
      "0.0645962570114\n",
      "0.0692045172769\n",
      "0.0899344362641\n",
      "0.0718725412457\n",
      "0.067252531157\n",
      "0.0767886813056\n",
      "0.0796765116479\n",
      "0.0643228028167\n",
      "0.0780470877462\n",
      "0.0669252854282\n",
      "0.111704649103\n",
      "0.0733463617355\n",
      "0.0658471862222\n",
      "0.0572474760242\n",
      "0.0705728777895\n",
      "0.0661944561262\n",
      "0.112694024141\n",
      "0.0307947719328\n",
      "0.0477724210213\n",
      "0.0942668119386\n",
      "0.0826903741282\n",
      "0.0755819917113\n",
      "0.0923081935923\n",
      "0.117684187587\n",
      "0.060712940001\n",
      "0.117609955689\n",
      "0.0557189238353\n",
      "0.0735078942628\n",
      "0.0766254131467\n",
      "0.0721784929284\n",
      "0.0837898826504\n",
      "0.0752101324047\n",
      "0.0490499312918\n",
      "0.0561800565998\n",
      "0.0791330369994\n",
      "0.10505127249\n",
      "0.094019966041\n",
      "0.0620957605984\n",
      "0.0592891832457\n",
      "0.0566910620022\n",
      "0.0857447901336\n",
      "0.0474154786372\n",
      "0.057300323316\n",
      "0.0796672562609\n",
      "0.0594045238143\n",
      "0.0703093992648\n",
      "0.0572084866354\n",
      "0.0975414235045\n",
      "0.0357912994242\n",
      "0.0678867657631\n",
      "0.0529217487907\n",
      "0.037890053798\n",
      "0.0698993488532\n",
      "0.0522811049081\n",
      "0.0612771806893\n",
      "0.060322922069\n",
      "0.0387413372644\n",
      "0.0585544719887\n",
      "0.057055376727\n",
      "0.0575799152815\n",
      "0.0165774507131\n",
      "0.064912491952\n",
      "0.0179824571781\n",
      "0.0357543887546\n",
      "0.033310614724\n",
      "0.0358909771836\n",
      "0.0147297098117\n",
      "0.044363435512\n",
      "0.0205617247338\n",
      "0.0468281849819\n",
      "0.0516245153062\n",
      "0.0381471040268\n",
      "0.0422576926034\n",
      "0.0291096674955\n",
      "0.036298028542\n",
      "0.028867260986\n",
      "0.0505548062541\n",
      "0.0534330774394\n",
      "0.0190466313355\n",
      "0.019479078291\n",
      "0.0114746949564\n",
      "0.0106910370154\n",
      "0.00436951426747\n",
      "0.0306139953653\n",
      "0.0293732014906\n",
      "0.0249807574681\n",
      "0.0138426049113\n",
      "0.0226636317233\n",
      "0.0213857782356\n",
      "0.0381652782792\n",
      "0.0126325936866\n",
      "0.0130796144077\n",
      "0.0216849880336\n",
      "0.0582561069057\n",
      "0.0323052607901\n",
      "0.0135285790786\n",
      "0.0411088549995\n",
      "0.00941712016776\n",
      "0.0198788616609\n",
      "0.0316589241454\n",
      "0.041791191889\n",
      "0.0128667511057\n",
      "0.0108233853856\n",
      "0.0148975025946\n",
      "0.024429723886\n",
      "0.010233695746\n",
      "0.0212600606885\n",
      "0.0177229602172\n",
      "0.0124947049296\n",
      "0.030818202223\n",
      "0.00751128191671\n",
      "0.0151686052255\n",
      "0.019381381425\n",
      "0.0339456361029\n",
      "0.0197993726419\n",
      "0.0141357409759\n",
      "0.0123548210834\n",
      "0.004534640478\n",
      "0.00917711359076\n",
      "0.000807880259494\n",
      "0.0893834087496\n",
      "0.0709022784938\n",
      "0.0607434466953\n",
      "0.0482456806919\n",
      "0.0549871676622\n",
      "0.0794772973828\n",
      "0.0637973689481\n",
      "0.0701344904932\n",
      "0.0822027655146\n",
      "0.100355768115\n",
      "0.063600798437\n",
      "0.0871906431471\n",
      "0.0404325682273\n",
      "0.0711078847889\n",
      "0.0834532133672\n",
      "0.0407686976003\n",
      "0.0603835480607\n",
      "0.0479961590964\n",
      "0.0597871765666\n",
      "0.0611928829914\n",
      "0.0697793449823\n",
      "0.0628493239085\n",
      "0.0574224764467\n",
      "0.0484222333402\n",
      "0.0703338014437\n",
      "0.0546215226959\n",
      "0.0528326776588\n",
      "0.0688322015789\n",
      "0.0491404247188\n",
      "0.0478366747291\n",
      "0.0347838884248\n",
      "0.0412424226439\n",
      "0.0486129054376\n",
      "0.0513785187134\n",
      "0.0285727358242\n",
      "0.0946027283646\n",
      "0.0365603633977\n",
      "0.0355030798185\n",
      "0.0414172211665\n",
      "0.0506900396353\n",
      "0.0491993895139\n",
      "0.0570634579501\n",
      "0.0667738804699\n",
      "0.0332800561865\n",
      "0.0691480931491\n",
      "0.0458815533486\n",
      "0.0716516898453\n",
      "0.0518031666183\n",
      "0.0285233964233\n",
      "0.0288215730426\n",
      "0.0242995537253\n",
      "0.059781093047\n",
      "0.0442817871572\n",
      "0.0291459395391\n",
      "0.0381800874712\n",
      "0.0221012260728\n",
      "0.0382861402579\n",
      "0.052052805424\n",
      "0.029519954126\n",
      "0.0378064783664\n",
      "0.0294874175882\n",
      "0.0246134637307\n",
      "0.0439937772771\n",
      "0.0213246675529\n",
      "0.0312775901536\n",
      "0.0441324037123\n",
      "0.0612445329834\n",
      "0.0251087152305\n",
      "0.041591961866\n",
      "0.0393679923906\n",
      "0.0248199527144\n",
      "0.0295981830128\n",
      "0.0225782939137\n",
      "0.037891589067\n",
      "0.0576517663726\n",
      "0.0289398315614\n",
      "0.00880572027543\n",
      "0.0343370605594\n",
      "0.0235462351522\n",
      "0.0488542678715\n",
      "0.0521643743756\n",
      "0.0272656884878\n",
      "0.0390141370809\n",
      "0.0404552220242\n",
      "0.0270999520155\n",
      "0.0209977463868\n",
      "0.0430239643009\n",
      "0.0457223796165\n",
      "0.0326226303332\n",
      "0.0287908791892\n",
      "0.0388920578871\n",
      "0.0479128025555\n",
      "0.0437414570486\n",
      "0.0286326509465\n",
      "0.030001645517\n",
      "0.0281749301979\n",
      "0.0282074789164\n",
      "0.0392061185893\n",
      "0.0121465693603\n",
      "0.0352629522854\n",
      "0.0279280494439\n",
      "0.0491947747055\n",
      "0.0370515924137\n",
      "0.0253670405773\n",
      "0.0357156220568\n",
      "0.0188515521661\n",
      "0.0134734254208\n",
      "0.0300984345409\n",
      "0.0197304284025\n",
      "0.0152209091248\n",
      "0.022058627939\n",
      "0.0127196323939\n",
      "0.0129569285103\n",
      "0.0153309588142\n",
      "0.0251546710295\n",
      "0.0190597819761\n",
      "0.0499769647945\n",
      "0.0181421993558\n",
      "0.0195202691615\n",
      "0.016760648021\n",
      "0.0256060614738\n",
      "0.0282632304473\n",
      "0.0160210518732\n",
      "0.0432215859337\n",
      "0.0356125249132\n",
      "0.0221120919489\n",
      "0.0443941181783\n",
      "0.0416339316607\n",
      "0.0287500625563\n",
      "0.0176685343989\n",
      "0.0163261196499\n",
      "0.0301207994476\n",
      "0.0224563522673\n",
      "0.0253468156949\n",
      "0.0314281844603\n",
      "0.0424408784418\n",
      "0.0193773580177\n",
      "0.03196068442\n",
      "0.0291345990898\n",
      "0.00905829959938\n",
      "0.0300583594214\n",
      "0.0198569018522\n",
      "0.0197244614909\n",
      "0.0149338489027\n",
      "0.0171942567227\n",
      "0.0211404035072\n",
      "0.0324274812131\n",
      "0.0105467192372\n",
      "0.0323585761066\n",
      "0.0263392602665\n",
      "0.0262623606437\n",
      "0.0481750951104\n",
      "0.0266814969519\n",
      "0.0317595251719\n",
      "0.0293965149359\n",
      "0.0294467815989\n",
      "0.0256862210898\n",
      "0.0192019617981\n",
      "0.0145535820817\n",
      "0.0254607712636\n",
      "0.0133119097091\n",
      "0.00726294153049\n",
      "0.0128333576075\n",
      "0.0095851394577\n",
      "0.0153136704263\n",
      "0.00433329836117\n",
      "0.0160175130596\n",
      "0.0191618398832\n",
      "0.0164300430982\n",
      "0.0188118926134\n",
      "0.01277866336\n",
      "0.0202158957846\n",
      "0.0079553689213\n",
      "0.0216698616418\n",
      "0.0140618775361\n",
      "0.00884878894441\n",
      "0.0131764015982\n",
      "0.00997275623296\n",
      "0.00913447392939\n",
      "0.0130393733209\n",
      "0.0168812826568\n",
      "0.0135164967562\n",
      "0.0237330816063\n",
      "0.00670015762256\n",
      "0.00727983402031\n",
      "0.0234328998659\n",
      "0.0155837044057\n",
      "0.00584523683933\n",
      "0.0201937464852\n",
      "0.00773986197055\n",
      "0.0183183356788\n",
      "0.00957521794156\n",
      "0.00471243654095\n",
      "0.0139515705336\n",
      "0.00494644568888\n",
      "0.0143791209876\n",
      "0.00548430392877\n",
      "0.00324520520589\n",
      "0.018216474475\n",
      "0.0147627588298\n",
      "0.00928090038137\n",
      "0.0102491571929\n",
      "0.00792699523695\n",
      "0.00584933011512\n",
      "0.00291706374395\n",
      "0.00143098398203\n",
      "0.0060855002637\n",
      "0.010634333985\n",
      "0.0256242654881\n",
      "0.00945016745694\n",
      "0.0141469280687\n",
      "0.00547775415002\n",
      "0.0156406446993\n",
      "0.00843630145223\n",
      "0.00250025379426\n",
      "0.0078559901559\n",
      "0.0192244297009\n",
      "0.00319212305647\n",
      "0.00165293268092\n",
      "0.0149988958929\n",
      "0.0205489369447\n",
      "0.0125836056572\n",
      "0.00318012151351\n",
      "0.00555991103493\n",
      "0.0199717337129\n",
      "0.0158426685692\n",
      "0.00562535179145\n",
      "0.0132837680191\n",
      "0.0150447370299\n",
      "0.0105065322906\n",
      "0.00752326420608\n",
      "0.00494699885416\n",
      "0.00467532116307\n",
      "0.0155071138044\n",
      "0.0078843475113\n",
      "0.00935925054308\n",
      "0.00324245987591\n",
      "0.0108209616877\n",
      "0.00362484307383\n",
      "0.00402251474142\n",
      "0.00713044203522\n",
      "0.00597645225963\n",
      "0.00493479712402\n",
      "0.000608320742521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0764320602054\n",
      "0.117758863504\n",
      "0.100160168691\n",
      "0.0674243357358\n",
      "0.0536950805613\n",
      "0.0671350777356\n",
      "0.0883851877682\n",
      "0.0741087517245\n",
      "0.0777427303405\n",
      "0.069442032118\n",
      "0.0779809886477\n",
      "0.0615458211377\n",
      "0.0742873062996\n",
      "0.0746435450654\n",
      "0.0500802809424\n",
      "0.0757720686316\n",
      "0.0400352783267\n",
      "0.0768256180929\n",
      "0.0967523767001\n",
      "0.115490205781\n",
      "0.0834936349349\n",
      "0.0417478331608\n",
      "0.0494242594697\n",
      "0.0687288572739\n",
      "0.0994850412158\n",
      "0.0395806883841\n",
      "0.0954408462536\n",
      "0.0937853592695\n",
      "0.0620314414741\n",
      "0.0536897675529\n",
      "0.0449297048693\n",
      "0.0699769428957\n",
      "0.0527966513919\n",
      "0.101249649778\n",
      "0.051261337003\n",
      "0.0662268870732\n",
      "0.0509926631048\n",
      "0.0476605098745\n",
      "0.0504059028025\n",
      "0.0593178933539\n",
      "0.0624145500603\n",
      "0.0713646409036\n",
      "0.0648395737895\n",
      "0.0580730316622\n",
      "0.0789237701495\n",
      "0.0940970798653\n",
      "0.0465126698234\n",
      "0.0887398645402\n",
      "0.0756312567944\n",
      "0.0789308628567\n",
      "0.0578353362591\n",
      "0.0727799811387\n",
      "0.0384165669626\n",
      "0.0504056711411\n",
      "0.0690295653279\n",
      "0.0161003997519\n",
      "0.0353221296278\n",
      "0.0324787207459\n",
      "0.0361267526604\n",
      "0.0519075051563\n",
      "0.0506666226691\n",
      "0.0357069825377\n",
      "0.0373569285035\n",
      "0.0337705423527\n",
      "0.0393179941744\n",
      "0.0415935989114\n",
      "0.0705973911092\n",
      "0.0397613602001\n",
      "0.0324809453298\n",
      "0.0499172929075\n",
      "0.0643870249287\n",
      "0.0523227275688\n",
      "0.0485085938235\n",
      "0.0468893947257\n",
      "0.0597960608445\n",
      "0.0643887799925\n",
      "0.0614897291562\n",
      "0.0244359308518\n",
      "0.032423097422\n",
      "0.0296260904426\n",
      "0.03600275585\n",
      "0.0415163365295\n",
      "0.0589709843585\n",
      "0.0437583092849\n",
      "0.0439920336267\n",
      "0.0434757125452\n",
      "0.0376617394806\n",
      "0.0579257477122\n",
      "0.0319413626259\n",
      "0.0285981602427\n",
      "0.0336969057009\n",
      "0.0330281492948\n",
      "0.0775699825484\n",
      "0.0462081968233\n",
      "0.0300833026413\n",
      "0.0444300952377\n",
      "0.0659217897686\n",
      "0.0220877832382\n",
      "0.0703816602241\n",
      "0.0616242434467\n",
      "0.0196740145065\n",
      "0.0499206835866\n",
      "0.0402681143482\n",
      "0.028370651925\n",
      "0.0350928352532\n",
      "0.0305835309578\n",
      "0.0216499588971\n",
      "0.0448774681789\n",
      "0.0327658731187\n",
      "0.0533729721339\n",
      "0.0515938054166\n",
      "0.0301726231191\n",
      "0.0179784597638\n",
      "0.0518463378734\n",
      "0.047565311476\n",
      "0.0591182727964\n",
      "0.0461842597842\n",
      "0.0544558878752\n",
      "0.0488133377328\n",
      "0.0319522648619\n",
      "0.0305182147314\n",
      "0.0587307946863\n",
      "0.0493328130659\n",
      "0.0455989618961\n",
      "0.0302893312157\n",
      "0.0468493591378\n",
      "0.0496239897359\n",
      "0.0336829651103\n",
      "0.049425114309\n",
      "0.0312898040359\n",
      "0.0584372060142\n",
      "0.0420291719329\n",
      "0.0553004908425\n",
      "0.0316361058061\n",
      "0.0548760762651\n",
      "0.0600454425666\n",
      "0.0214292705\n",
      "0.0138545418248\n",
      "0.0594432905609\n",
      "0.0365337708426\n",
      "0.064256200863\n",
      "0.0491263005737\n",
      "0.0417543174432\n",
      "0.0482721604646\n",
      "0.0210550417621\n",
      "0.0523970924432\n",
      "0.0375876914801\n",
      "0.0491350405107\n",
      "0.0412135027679\n",
      "0.045543732553\n",
      "0.0213983527367\n",
      "0.0430930192445\n",
      "0.026477617643\n",
      "0.057290895818\n",
      "0.0281444748995\n",
      "0.0222984267909\n",
      "0.0397271782415\n",
      "0.04312332439\n",
      "0.0157782121617\n",
      "0.0510003866692\n",
      "0.0528898179042\n",
      "0.0381044113185\n",
      "0.0292916507457\n",
      "0.0501446856544\n",
      "0.0159228949069\n",
      "0.0589461357115\n",
      "0.0636145667062\n",
      "0.031672896411\n",
      "0.0424721901083\n",
      "0.0352703354652\n",
      "0.034169269795\n",
      "0.0487639102621\n",
      "0.0819372895866\n",
      "0.0380223983903\n",
      "0.0364167349137\n",
      "0.0158782391045\n",
      "0.0241360238736\n",
      "0.0613357378919\n",
      "0.0415268752609\n",
      "0.0549378796224\n",
      "0.0317683704826\n",
      "0.0468752874673\n",
      "0.0238653194254\n",
      "0.0387022653\n",
      "0.0366871617223\n",
      "0.0328186989373\n",
      "0.0522318159784\n",
      "0.0370413545768\n",
      "0.0489278669203\n",
      "0.0391572392255\n",
      "0.0518389566887\n",
      "0.0391031222952\n",
      "0.0337812376967\n",
      "0.0291497685407\n",
      "0.0451995110728\n",
      "0.0572993739272\n",
      "0.0643130596758\n",
      "0.0426543119381\n",
      "0.0321243872763\n",
      "0.030829064183\n",
      "0.0275712416626\n",
      "0.025518648228\n",
      "0.0353909957074\n",
      "0.0304583896891\n",
      "0.0251078533034\n",
      "0.0405012268832\n",
      "0.0181465902076\n",
      "0.0131572200725\n",
      "0.0418762864817\n",
      "0.0370507218855\n",
      "0.0436052406602\n",
      "0.0406344959445\n",
      "0.0269784439334\n",
      "0.0172395811676\n",
      "0.0605913231435\n",
      "0.0323990274268\n",
      "0.0195119932795\n",
      "0.0270830790256\n",
      "0.0284756774199\n",
      "0.0141680313722\n",
      "0.0312745266474\n",
      "0.0187506558313\n",
      "0.0469583349692\n",
      "0.0345531443362\n",
      "0.0352944213776\n",
      "0.0215984974905\n",
      "0.0500283893857\n",
      "0.0378134431811\n",
      "0.0638552979798\n",
      "0.0278241107716\n",
      "0.0195560348552\n",
      "0.0559839288001\n",
      "0.0313468775685\n",
      "0.0342947226889\n",
      "0.0282061844432\n",
      "0.0284916570713\n",
      "0.037769057274\n",
      "0.0343416134734\n",
      "0.0228056748722\n",
      "0.0460823366062\n",
      "0.0364102272803\n",
      "0.0383506923947\n",
      "0.0453612728459\n",
      "0.0224496470289\n",
      "0.0271479377597\n",
      "0.0310925989829\n",
      "0.0118796265884\n",
      "0.048184770856\n",
      "0.0323543601182\n",
      "0.0343625829923\n",
      "0.0330991217486\n",
      "0.0456008795745\n",
      "0.0175147411773\n",
      "0.0257799245786\n",
      "0.010437214871\n",
      "0.0514743973741\n",
      "0.0427552650615\n",
      "0.0222121704025\n",
      "0.0275770239968\n",
      "0.0495397093931\n",
      "0.0264182951988\n",
      "0.0136366465449\n",
      "0.025492773779\n",
      "0.0249121158352\n",
      "0.024335057234\n",
      "0.0276755069824\n",
      "0.0149112338921\n",
      "0.0522205107357\n",
      "0.0234254304302\n",
      "0.0446124287771\n",
      "0.0236693810822\n",
      "0.0200281453394\n",
      "0.0422364384094\n",
      "0.0443734092736\n",
      "0.0407098581889\n",
      "0.0389572766527\n",
      "0.0237957488314\n",
      "0.0256607983332\n",
      "0.0468980557183\n",
      "0.0398238954843\n",
      "0.0310371503669\n",
      "0.0104991377096\n",
      "0.0705321146752\n",
      "0.0219850562055\n",
      "0.033594464999\n",
      "0.0400501073613\n",
      "0.0269867237397\n",
      "0.0298296873279\n",
      "0.00912819686698\n",
      "0.0202092153094\n",
      "0.0595019304516\n",
      "0.0298380951517\n",
      "0.0457821946885\n",
      "0.0243182456004\n",
      "0.0381418244499\n",
      "0.0176113271562\n",
      "0.0292672738793\n",
      "0.0424215003635\n",
      "0.0321698893688\n",
      "0.0478313082873\n",
      "0.0769252240417\n",
      "0.0470435205785\n",
      "0.0303443745609\n",
      "0.0193251138606\n",
      "0.0335337314498\n",
      "0.035323081114\n",
      "0.0370427257565\n",
      "0.035528473929\n",
      "0.0356992568252\n",
      "0.0497189429502\n",
      "0.0313066539207\n",
      "0.0241615700818\n",
      "0.0176442031891\n",
      "0.00552773689717\n",
      "0.00771105706128\n",
      "0.013336361709\n",
      "0.0138352144351\n",
      "0.006624151278\n",
      "0.0156359278527\n",
      "0.0199809057219\n",
      "0.0168609044254\n",
      "0.0176635232758\n",
      "0.0082637946403\n",
      "0.0138128036507\n",
      "0.0133845287964\n",
      "0.0165023481258\n",
      "0.0238002964084\n",
      "0.0174601239634\n",
      "0.0129545703098\n",
      "0.0181377894407\n",
      "0.0300358768997\n",
      "0.00534507892686\n",
      "0.00280007896913\n",
      "0.0145430127748\n",
      "0.0280211550115\n",
      "0.0121758032835\n",
      "0.0129864136465\n",
      "0.0159516755557\n",
      "0.0223954221716\n",
      "0.00559597885844\n",
      "0.0105417945034\n",
      "0.0032659412889\n",
      "0.014848806116\n",
      "0.0140655024757\n",
      "0.0144388999535\n",
      "0.0203274223992\n",
      "0.0157968448077\n",
      "0.00650798276021\n",
      "0.0148221491175\n",
      "0.0178722630282\n",
      "0.0302653985043\n",
      "0.00253091178093\n",
      "0.0106323152336\n",
      "0.0188492138682\n",
      "0.0205919540811\n",
      "0.0138500873922\n",
      "0.025873813256\n",
      "0.0267921739819\n",
      "0.0168005257935\n",
      "0.0167387570558\n",
      "0.0198658539526\n",
      "0.0114794727085\n",
      "0.0127237004719\n",
      "0.00783342977125\n",
      "0.00992140710907\n",
      "0.023463701856\n",
      "0.0240681218589\n",
      "0.0338188763925\n",
      "0.0246891946079\n",
      "0.0283396657302\n",
      "0.0066419679811\n",
      "0.0226397817558\n",
      "0.0408729744447\n",
      "0.0106025923664\n",
      "0.0263477314822\n",
      "0.0105952330446\n",
      "0.0253085237792\n",
      "0.0130035336147\n",
      "0.00709545289641\n",
      "0.00494221427071\n",
      "0.00894859218353\n",
      "0.0244176604375\n",
      "0.0163936182726\n",
      "0.00731516767039\n",
      "0.00225318189023\n",
      "0.0150993319046\n",
      "0.0246105732855\n",
      "0.010147255135\n",
      "0.0219220159907\n",
      "0.00858589920268\n",
      "0.0266550590042\n",
      "0.0247511870314\n",
      "0.0126896427576\n",
      "0.00406499705188\n",
      "0.0137861622026\n",
      "0.0109086519085\n",
      "0.0134217981629\n",
      "0.0206434296171\n",
      "0.0203519539028\n",
      "0.00585189025391\n",
      "0.00536716082983\n",
      "0.0125211697989\n",
      "0.02415161189\n",
      "0.00884845256376\n",
      "0.0104950850539\n",
      "0.00631750427276\n",
      "0.00597099193101\n",
      "0.0190249262371\n",
      "0.0164441007058\n",
      "0.0206441814026\n",
      "0.00518278742958\n",
      "0.0227517528298\n",
      "0.0136989982798\n",
      "0.0407753248295\n",
      "0.00866376149328\n",
      "0.0358632218441\n",
      "0.0247265905488\n",
      "0.0335019603973\n",
      "0.018030164956\n",
      "0.00530803281287\n",
      "0.00864259905249\n",
      "0.0199138486094\n",
      "0.0189345996493\n",
      "0.00793948364666\n",
      "0.00838170004094\n",
      "0.0172936716543\n",
      "0.0201336106166\n",
      "0.0154973599227\n",
      "0.0144145631863\n",
      "0.00809874697863\n",
      "0.0122155600857\n",
      "0.0290226120062\n",
      "0.0306419054406\n",
      "0.0132197730447\n",
      "0.0317382670401\n",
      "0.0121112846631\n",
      "0.0228943929413\n",
      "0.0075020172773\n",
      "0.0137953584143\n",
      "0.0179870256976\n",
      "0.0208410508629\n",
      "0.0126427051001\n",
      "0.0204052415047\n",
      "0.012079805002\n",
      "0.0224286231827\n",
      "0.0139713576149\n",
      "0.0107784693149\n",
      "0.018447081052\n",
      "0.0118064656246\n",
      "0.00917951528965\n",
      "0.0248486818622\n",
      "0.0104962591918\n",
      "0.0112898074137\n",
      "0.0184855892581\n",
      "0.0157661913733\n",
      "0.0111125523305\n",
      "0.0210510803452\n",
      "0.0128589913419\n",
      "0.0315563709536\n",
      "0.0142777371991\n",
      "0.0236881649524\n",
      "0.0350132778094\n",
      "0.0242929058006\n",
      "0.0130990734572\n",
      "0.0213236661362\n",
      "0.028598840411\n",
      "0.0126252019592\n",
      "0.0246355002834\n",
      "0.0139846723324\n",
      "0.006577951113\n",
      "0.0148929949139\n",
      "0.0113328144223\n",
      "0.0111307562946\n",
      "0.0210746670606\n",
      "0.0109299418584\n",
      "0.0112252010192\n",
      "0.0166154482387\n",
      "0.0085164849933\n",
      "0.00986317216275\n",
      "0.0205333140857\n",
      "0.0133416525456\n",
      "0.0335107615883\n",
      "0.0128656416243\n",
      "0.0213497198581\n",
      "0.00908643273629\n",
      "0.00279790637709\n",
      "0.0203817987202\n",
      "0.017353604918\n",
      "0.00331392478883\n",
      "0.00241960923528\n",
      "0.00108653694028\n",
      "0.0159555566388\n",
      "0.00313953527154\n",
      "0.0231683386757\n",
      "0.0109670833919\n",
      "0.033578071593\n",
      "0.0105456752965\n",
      "0.00343418649559\n",
      "0.00159846024535\n",
      "0.00527266600528\n",
      "0.00346523221992\n",
      "0.00213363325323\n",
      "0.00515642586386\n",
      "0.0110477737371\n",
      "0.00359208700054\n",
      "0.0128852063957\n",
      "0.0135606731571\n",
      "0.000593316273651\n",
      "0.0463454159794\n",
      "0.0942064364718\n",
      "0.0885609214378\n",
      "0.0896836044094\n",
      "0.0690171603934\n",
      "0.0547642589425\n",
      "0.0639821844919\n",
      "0.0755684701199\n",
      "0.101976616655\n",
      "0.0967546176154\n",
      "0.0855866545063\n",
      "0.0648886819861\n",
      "0.0812219447698\n",
      "0.0589220438016\n",
      "0.0632908707438\n",
      "0.0551587226657\n",
      "0.05374271706\n",
      "0.0651356547164\n",
      "0.037509469811\n",
      "0.0567813302644\n",
      "0.0346872584835\n",
      "0.041956219596\n",
      "0.065504411441\n",
      "0.0375044960918\n",
      "0.0553856702692\n",
      "0.0445283018539\n",
      "0.0670065399989\n",
      "0.0340661008694\n",
      "0.0309998170665\n",
      "0.0174341900314\n",
      "0.0500553884796\n",
      "0.039013485523\n",
      "0.0202131454661\n",
      "0.0345298651817\n",
      "0.0175002840109\n",
      "0.0290235911696\n",
      "0.0442876835813\n",
      "0.0182946218685\n",
      "0.0289169550071\n",
      "0.0584923606666\n",
      "0.0408755140173\n",
      "0.022044747213\n",
      "0.0253560564709\n",
      "0.0372129200119\n",
      "0.0223302282329\n",
      "0.0339890544709\n",
      "0.025419575409\n",
      "0.0387293929167\n",
      "0.0430102901914\n",
      "0.0409971595272\n",
      "0.0512689590524\n",
      "0.0228970189984\n",
      "0.0374063864829\n",
      "0.0393851283905\n",
      "0.0392598669698\n",
      "0.0310232766461\n",
      "0.0644713612957\n",
      "0.0270517418766\n",
      "0.0312683195124\n",
      "0.0337517865833\n",
      "0.0353692841728\n",
      "0.0427289998151\n",
      "0.0211149973358\n",
      "0.0428139485199\n",
      "0.0342493829187\n",
      "0.0247826560223\n",
      "0.0452816362555\n",
      "0.0394924378385\n",
      "0.0279669359582\n",
      "0.0283748581131\n",
      "0.0261256149378\n",
      "0.0422060118857\n",
      "0.0223991868579\n",
      "0.0112019462985\n",
      "0.0425605508314\n",
      "0.04720886083\n",
      "0.0259554977121\n",
      "0.0217698043319\n",
      "0.023749181698\n",
      "0.0274564139709\n",
      "0.0300931376548\n",
      "0.0178096994712\n",
      "0.0148551074484\n",
      "0.0325077669298\n",
      "0.0304677842756\n",
      "0.0264215299954\n",
      "0.0155748214917\n",
      "0.0282482447522\n",
      "0.0357155449803\n",
      "0.013745013462\n",
      "0.018903589671\n",
      "0.0255375008686\n",
      "0.0303947833927\n",
      "0.048842802512\n",
      "0.0325028935542\n",
      "0.0306236394911\n",
      "0.0228114766853\n",
      "0.0436200745198\n",
      "0.033803824647\n",
      "0.0542022278931\n",
      "0.0243989367448\n",
      "0.0241851454758\n",
      "0.0362350584188\n",
      "0.029502408512\n",
      "0.0127916166321\n",
      "0.0150542621592\n",
      "0.0234893717191\n",
      "0.0391822584147\n",
      "0.0140433955063\n",
      "0.0244807796746\n",
      "0.0304469145223\n",
      "0.0178739606771\n",
      "0.0090498321809\n",
      "0.00921229012725\n",
      "0.017579937674\n",
      "0.0223475472211\n",
      "0.00528993390131\n",
      "0.0143680535862\n",
      "0.0146812961893\n",
      "0.00822229243852\n",
      "0.0229727393423\n",
      "0.0247346798783\n",
      "0.017769029985\n",
      "0.0276546940929\n",
      "0.041172649476\n",
      "0.00614273251394\n",
      "0.0255263277956\n",
      "0.00687202722595\n",
      "0.00828343980938\n",
      "0.018275535525\n",
      "0.0028762084429\n",
      "0.00523082828497\n",
      "0.0143985319416\n",
      "0.00584649154823\n",
      "0.035892065457\n",
      "0.0120023467257\n",
      "0.0123798374675\n",
      "0.0216448930502\n",
      "0.026429642743\n",
      "0.0252253158027\n",
      "0.0281886245985\n",
      "0.0268538890795\n",
      "0.00398104311545\n",
      "0.0337315326864\n",
      "0.0207036949615\n",
      "0.00575855397132\n",
      "0.00704509577172\n",
      "0.0110376987134\n",
      "0.0246090893398\n",
      "0.0350368426465\n",
      "0.0296668966199\n",
      "0.0162769044096\n",
      "0.00354717362504\n",
      "0.0174704064032\n",
      "0.0110528252188\n",
      "0.011727186025\n",
      "0.0137720256085\n",
      "0.0191908428641\n",
      "0.02699166751\n",
      "0.0105620657995\n",
      "0.0154060066461\n",
      "0.00806447351142\n",
      "0.0316461226473\n",
      "0.0223087450172\n",
      "0.00587165625874\n",
      "0.00334587436056\n",
      "0.0104854469033\n",
      "0.00408176035326\n",
      "0.011550487837\n",
      "0.012161540596\n",
      "0.00894814494972\n",
      "0.0216785310796\n",
      "0.00494390938343\n",
      "0.0125828185712\n",
      "0.0100162292652\n",
      "0.0242199566154\n",
      "0.016405582705\n",
      "0.0222718894864\n",
      "0.0170377947246\n",
      "0.0223248307125\n",
      "0.00502105158608\n",
      "0.0163202477773\n",
      "0.0258639245791\n",
      "0.0324897308982\n",
      "0.01695435713\n",
      "0.0096154154889\n",
      "0.0142225749123\n",
      "0.0310590780099\n",
      "0.00466372040183\n",
      "0.0117010939195\n",
      "0.0352483179814\n",
      "0.0149989782153\n",
      "0.0128344607893\n",
      "0.0203927966244\n",
      "0.00595648013133\n",
      "0.0251406293451\n",
      "0.0186193337021\n",
      "0.00772240445352\n",
      "0.0139832157912\n",
      "0.0211484138338\n",
      "0.0192883598827\n",
      "0.0107159032995\n",
      "0.0196708105557\n",
      "0.0152364061436\n",
      "0.0159086052035\n",
      "0.0130673429765\n",
      "0.0357643738398\n",
      "0.0216143280949\n",
      "0.0261424945261\n",
      "0.00877395242667\n",
      "0.0161791642239\n",
      "0.0233538955549\n",
      "0.0148845430348\n",
      "0.024407011009\n",
      "0.00306293625911\n",
      "0.0169810147634\n",
      "0.0217485258984\n",
      "0.0103645983772\n",
      "0.020641618123\n",
      "0.0147595683385\n",
      "0.0153061732772\n",
      "0.0336713228668\n",
      "0.0198453331604\n",
      "0.0165270144739\n",
      "0.0060200989888\n",
      "0.0174016973253\n",
      "0.0285062749632\n",
      "0.0216043478863\n",
      "0.0142711814597\n",
      "0.00406892398157\n",
      "0.00926259231937\n",
      "0.0125021971764\n",
      "0.0193792756401\n",
      "0.0146736100573\n",
      "0.0209543329736\n",
      "0.0147794806097\n",
      "0.0357286986981\n",
      "0.0158601253211\n",
      "0.0161942188621\n",
      "0.0176781338385\n",
      "0.02491786208\n",
      "0.0133278365991\n",
      "0.0198577181734\n",
      "0.00155126409465\n",
      "0.0232408920895\n",
      "0.0112430782237\n",
      "0.0281893104665\n",
      "0.0155002823641\n",
      "0.01107407569\n",
      "0.0208761109743\n",
      "0.0167704140995\n",
      "0.00987633119378\n",
      "0.00915356402989\n",
      "0.00315346923537\n",
      "0.0103790999111\n",
      "0.0306812625728\n",
      "0.00709778434159\n",
      "0.0154251254075\n",
      "0.032009034458\n",
      "0.0233140259525\n",
      "0.0100933607915\n",
      "0.0122304861324\n",
      "0.0254990112209\n",
      "0.0112003556554\n",
      "0.0180863090187\n",
      "0.00574829129762\n",
      "0.0261292650861\n",
      "0.00605420179665\n",
      "0.0106053221912\n",
      "0.0125802448661\n",
      "0.00260732217645\n",
      "0.00410197963187\n",
      "0.0121319050136\n",
      "0.00856066080066\n",
      "0.000725966173861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0832858289749\n",
      "0.09074200042\n",
      "0.0902070008728\n",
      "0.114889465704\n",
      "0.0767606956548\n",
      "0.0799176166264\n",
      "0.079635731687\n",
      "0.0972697401238\n",
      "0.0676300672336\n",
      "0.104618913667\n",
      "0.0507361224469\n",
      "0.0620164693844\n",
      "0.0444837909544\n",
      "0.0859793665211\n",
      "0.0659970519912\n",
      "0.103555559439\n",
      "0.0760699950768\n",
      "0.0805575383269\n",
      "0.0646279182337\n",
      "0.0345341396902\n",
      "0.0661389225552\n",
      "0.0468573844542\n",
      "0.0698371217313\n",
      "0.0630931382646\n",
      "0.0803043585976\n",
      "0.0991695317034\n",
      "0.0709661659968\n",
      "0.0567521107516\n",
      "0.0686957555047\n",
      "0.0739506903929\n",
      "0.080216434476\n",
      "0.0693230099511\n",
      "0.050610779165\n",
      "0.0631520699516\n",
      "0.0733697667086\n",
      "0.084979512168\n",
      "0.0501460745505\n",
      "0.0474319532169\n",
      "0.0521206099772\n",
      "0.0551959267677\n",
      "0.0659466858247\n",
      "0.0825129799062\n",
      "0.0822380286875\n",
      "0.0571766774434\n",
      "0.0691523661097\n",
      "0.0711985527878\n",
      "0.0672248508841\n",
      "0.0539292381893\n",
      "0.129217158525\n",
      "0.0999002777181\n",
      "0.0928419558044\n",
      "0.0646821206519\n",
      "0.0610455393722\n",
      "0.0589917141648\n",
      "0.0981039111941\n",
      "0.0624157158695\n",
      "0.0551154589727\n",
      "0.0633212480462\n",
      "0.032752328582\n",
      "0.0575979471702\n",
      "0.0771248701668\n",
      "0.0471318963268\n",
      "0.0530926449228\n",
      "0.0677627936588\n",
      "0.0424543795147\n",
      "0.0863552352133\n",
      "0.0294744581095\n",
      "0.0426667157244\n",
      "0.0406332899127\n",
      "0.056792565803\n",
      "0.046558297373\n",
      "0.0472859201014\n",
      "0.0380766566827\n",
      "0.0590982346197\n",
      "0.0512773988667\n",
      "0.0355959006035\n",
      "0.0708672001379\n",
      "0.0574847693945\n",
      "0.0421481318105\n",
      "0.0284087387687\n",
      "0.0277482523431\n",
      "0.0527264462521\n",
      "0.0791789972222\n",
      "0.0445275517716\n",
      "0.054719519126\n",
      "0.047185306813\n",
      "0.0310784730915\n",
      "0.0762537508461\n",
      "0.0481148920524\n",
      "0.0588070068929\n",
      "0.0758250459754\n",
      "0.0878694228492\n",
      "0.0487494089051\n",
      "0.066844248501\n",
      "0.0225833487848\n",
      "0.0217240472089\n",
      "0.0720952908358\n",
      "0.0471837870522\n",
      "0.053362040845\n",
      "0.061887968986\n",
      "0.0714566030265\n",
      "0.0563850952741\n",
      "0.0845063082296\n",
      "0.0738295034865\n",
      "0.0406472737769\n",
      "0.047701204678\n",
      "0.0657142415941\n",
      "0.0291899230107\n",
      "0.0538907907996\n",
      "0.0530060470947\n",
      "0.0371348559837\n",
      "0.059819223511\n",
      "0.0478230009526\n",
      "0.0246401893215\n",
      "0.0496926831304\n",
      "0.0414741635124\n",
      "0.0557535613033\n",
      "0.0460102882457\n",
      "0.0404824656096\n",
      "0.0688005146628\n",
      "0.0331162901029\n",
      "0.0460894478032\n",
      "0.0752849806888\n",
      "0.0334444980823\n",
      "0.043213547512\n",
      "0.0710956553267\n",
      "0.052375185404\n",
      "0.0734591711189\n",
      "0.0545632261073\n",
      "0.0433004325425\n",
      "0.0519331074708\n",
      "0.0597442083827\n",
      "0.0437507277767\n",
      "0.0516367966585\n",
      "0.0510716166836\n",
      "0.0168195876374\n",
      "0.0560416529673\n",
      "0.0316280684778\n",
      "0.0666803638659\n",
      "0.0348377160605\n",
      "0.0737746801773\n",
      "0.0313321253207\n",
      "0.0306152043657\n",
      "0.0381619526726\n",
      "0.0505820272581\n",
      "0.0141083703495\n",
      "0.0580973497702\n",
      "0.038381893224\n",
      "0.0309287121779\n",
      "0.0295156583214\n",
      "0.0444304853644\n",
      "0.0231660557849\n",
      "0.0415499129582\n",
      "0.00933691030877\n",
      "0.0317697933352\n",
      "0.0172377264888\n",
      "0.0216418880379\n",
      "0.0270404997697\n",
      "0.0163105202289\n",
      "0.0230450204461\n",
      "0.0209052596721\n",
      "0.037891115784\n",
      "0.0313304373122\n",
      "0.016226335756\n",
      "0.0186464502026\n",
      "0.0174044914823\n",
      "0.0456870755611\n",
      "0.00534322132091\n",
      "0.045227173955\n",
      "0.0137027233329\n",
      "0.0135674644251\n",
      "0.0401703790159\n",
      "0.0137125669818\n",
      "0.0342822533898\n",
      "0.0103576400214\n",
      "0.0207339150548\n",
      "0.0232855850087\n",
      "0.00836997257843\n",
      "0.00381686831843\n",
      "0.0167650013198\n",
      "0.0299623301285\n",
      "0.0433291272411\n",
      "0.0138275218758\n",
      "0.0322923648102\n",
      "0.0155304101944\n",
      "0.00501458716184\n",
      "0.0215588712496\n",
      "0.0220528362671\n",
      "0.0258343552182\n",
      "0.025385834705\n",
      "0.0259307452233\n",
      "0.0230840512184\n",
      "0.0118847474958\n",
      "0.0441982588629\n",
      "0.0328775619962\n",
      "0.0140948774195\n",
      "0.0205853190766\n",
      "0.0442964749098\n",
      "0.00859984402973\n",
      "0.0171155485745\n",
      "0.0204269146181\n",
      "0.0216536956466\n",
      "0.0290759460898\n",
      "0.0297810554231\n",
      "0.0182690165328\n",
      "0.00438126091491\n",
      "0.0164846365608\n",
      "0.0299393161757\n",
      "0.00844725704762\n",
      "0.0206979712144\n",
      "0.0128333878049\n",
      "0.0259801867091\n",
      "0.014169801112\n",
      "0.0200076936602\n",
      "0.00732353238322\n",
      "0.0417688741107\n",
      "0.0187310303291\n",
      "0.0253452999262\n",
      "0.0229873673263\n",
      "0.0162434066435\n",
      "0.00349016383543\n",
      "0.0164848299106\n",
      "0.0109100034848\n",
      "0.0155518773402\n",
      "0.0114227201139\n",
      "0.00524111796488\n",
      "0.0227261187933\n",
      "0.00263345549855\n",
      "0.00457454113116\n",
      "0.00351115683494\n",
      "0.00464729622647\n",
      "0.00645541903626\n",
      "0.00154424703302\n",
      "0.0133174692635\n",
      "0.00798754404233\n",
      "0.023954235401\n",
      "0.0102404112527\n",
      "0.001440163393\n",
      "0.0104546697049\n",
      "0.00264327292936\n",
      "0.00741392659991\n",
      "0.00327502119622\n",
      "0.00562931200767\n",
      "0.00445206997357\n",
      "0.0157176212049\n",
      "0.00123802965496\n",
      "0.0101170590029\n",
      "0.0200375196663\n",
      "0.0258397619026\n",
      "0.00402106596749\n",
      "0.00509536012141\n",
      "0.000713389627949\n",
      "0.0817627063056\n",
      "0.0966115028395\n",
      "0.092365645017\n",
      "0.0615349555785\n",
      "0.0866123783368\n",
      "0.109436438074\n",
      "0.0698838632752\n",
      "0.0844952170436\n",
      "0.0671901827338\n",
      "0.0674352064809\n",
      "0.116356098679\n",
      "0.047174536591\n",
      "0.066602643189\n",
      "0.0552200771777\n",
      "0.0578451668423\n",
      "0.117236696499\n",
      "0.0707280701621\n",
      "0.071671675156\n",
      "0.0703064793408\n",
      "0.082935221836\n",
      "0.0562042824634\n",
      "0.0875128752647\n",
      "0.0720168052212\n",
      "0.0669626850566\n",
      "0.0840508109985\n",
      "0.0588207740341\n",
      "0.0472600946814\n",
      "0.0521462123555\n",
      "0.0475600672182\n",
      "0.030145559254\n",
      "0.0671718229579\n",
      "0.0259987994844\n",
      "0.0531855159816\n",
      "0.031446312787\n",
      "0.0375750113594\n",
      "0.0407451235699\n",
      "0.0341698461641\n",
      "0.0365891452498\n",
      "0.0486445243102\n",
      "0.0558139125211\n",
      "0.0262213488369\n",
      "0.0569066317459\n",
      "0.049927655925\n",
      "0.0329554698394\n",
      "0.0508754245465\n",
      "0.0311541927247\n",
      "0.0377796419297\n",
      "0.0340207466746\n",
      "0.055914604461\n",
      "0.0459149358271\n",
      "0.0376770572488\n",
      "0.0163455388234\n",
      "0.0607230883809\n",
      "0.025826987006\n",
      "0.0238654438973\n",
      "0.0324148640903\n",
      "0.0352373569524\n",
      "0.0319661866556\n",
      "0.0452043141543\n",
      "0.0121259381949\n",
      "0.0394355675847\n",
      "0.0280843825071\n",
      "0.0445229869147\n",
      "0.0413446985857\n",
      "0.0534040748486\n",
      "0.0528346486397\n",
      "0.0598837358521\n",
      "0.0411075824558\n",
      "0.0293425305452\n",
      "0.0498711560898\n",
      "0.0337109881734\n",
      "0.0408748462661\n",
      "0.0363098926055\n",
      "0.0221348431104\n",
      "0.0380777335628\n",
      "0.0569715646898\n",
      "0.0192032840575\n",
      "0.0436266692436\n",
      "0.0519545869273\n",
      "0.0346665467227\n",
      "0.0208022660815\n",
      "0.0343838429176\n",
      "0.0442179071726\n",
      "0.0319414320742\n",
      "0.0590138871964\n",
      "0.0433916798051\n",
      "0.0547488336183\n",
      "0.0673972472052\n",
      "0.0178024954527\n",
      "0.0488575593509\n",
      "0.0370768752458\n",
      "0.030655069811\n",
      "0.0562247123348\n",
      "0.0321569166916\n",
      "0.0338172572296\n",
      "0.0485624775931\n",
      "0.0336125501698\n",
      "0.0389632801101\n",
      "0.0312349459006\n",
      "0.054295419786\n",
      "0.0356411566811\n",
      "0.0380610166992\n",
      "0.0265238452438\n",
      "0.0214443386547\n",
      "0.0286405278917\n",
      "0.0510764229693\n",
      "0.0150457996032\n",
      "0.0399115590869\n",
      "0.0515546433209\n",
      "0.0363296868368\n",
      "0.0245025457166\n",
      "0.0406088716012\n",
      "0.0322617503328\n",
      "0.0517368705379\n",
      "0.0488553990595\n",
      "0.0179585287087\n",
      "0.0393352340865\n",
      "0.0253338521082\n",
      "0.0339485346622\n",
      "0.042178164645\n",
      "0.0177386000634\n",
      "0.0241660375645\n",
      "0.0295223616541\n",
      "0.0183325544528\n",
      "0.0298758048932\n",
      "0.0103135546473\n",
      "0.0387137400404\n",
      "0.0425149375916\n",
      "0.0330888698742\n",
      "0.065016672687\n",
      "0.041303615101\n",
      "0.0360719042655\n",
      "0.0294535864618\n",
      "0.0178312076609\n",
      "0.0257765253531\n",
      "0.01385774786\n",
      "0.00679545988005\n",
      "0.0295044174045\n",
      "0.0459721616992\n",
      "0.0155270959583\n",
      "0.0240724646555\n",
      "0.0228416218498\n",
      "0.00401647170662\n",
      "0.0340523069128\n",
      "0.0460121441188\n",
      "0.0303462973403\n",
      "0.0249416432679\n",
      "0.0196048309717\n",
      "0.0163232386799\n",
      "0.0201704243359\n",
      "0.0270713495975\n",
      "0.0214741565452\n",
      "0.0204252420011\n",
      "0.0180391949591\n",
      "0.0284148224431\n",
      "0.0155236087388\n",
      "0.0139719354392\n",
      "0.0183119660021\n",
      "0.00684354409275\n",
      "0.0160163305112\n",
      "0.0218614770499\n",
      "0.0511540357443\n",
      "0.0117116164357\n",
      "0.0385607641808\n",
      "0.0294986325958\n",
      "0.027058826867\n",
      "0.0271406278468\n",
      "0.0195001654411\n",
      "0.0396810842658\n",
      "0.022102590984\n",
      "0.0376330914352\n",
      "0.0288095995878\n",
      "0.0289993804046\n",
      "0.0236343335766\n",
      "0.0252062583885\n",
      "0.00894586333118\n",
      "0.0414011109987\n",
      "0.024055866046\n",
      "0.0352550081645\n",
      "0.0256657774689\n",
      "0.0300821430355\n",
      "0.0127454758084\n",
      "0.0253278497793\n",
      "0.0171861444781\n",
      "0.00917079923499\n",
      "0.0346158650312\n",
      "0.0125347533765\n",
      "0.0220141298602\n",
      "0.0383056305747\n",
      "0.0421443335678\n",
      "0.0195572619118\n",
      "0.0197244413022\n",
      "0.0218152179192\n",
      "0.0317428792367\n",
      "0.00826262476791\n",
      "0.025766758814\n",
      "0.00698610470743\n",
      "0.019097785637\n",
      "0.00516211314065\n",
      "0.0133585606428\n",
      "0.0123392191796\n",
      "0.0028380559779\n",
      "0.0116437822444\n",
      "0.00175273858332\n",
      "0.00296235427938\n",
      "0.00403138018942\n",
      "0.00684834936354\n",
      "0.0125577786911\n",
      "0.00272298997954\n",
      "0.0161792584067\n",
      "0.00330652016942\n",
      "0.0170973512672\n",
      "0.013787910338\n",
      "0.00334229630095\n",
      "0.0122275391237\n",
      "0.00426912492234\n",
      "0.00592480964351\n",
      "0.00199490928344\n",
      "0.00255933877065\n",
      "0.00407386798014\n",
      "0.00441718268687\n",
      "0.00308202399186\n",
      "0.00424257962937\n",
      "0.00948475116806\n",
      "0.0213259058643\n",
      "0.00337994399742\n",
      "0.00403464296519\n",
      "0.0182989960573\n",
      "0.0120602074787\n",
      "0.00256373273594\n",
      "0.00521629176519\n",
      "0.0104311024701\n",
      "0.0138238912884\n",
      "0.00223184377439\n",
      "0.000540094143599\n",
      "0.111241672056\n",
      "0.0682494466739\n",
      "0.0665290593615\n",
      "0.0591345019687\n",
      "0.0485257655494\n",
      "0.059768972503\n",
      "0.0634298181979\n",
      "0.08166554618\n",
      "0.0689070998728\n",
      "0.0789795637723\n",
      "0.0821197033857\n",
      "0.075223611153\n",
      "0.0970700843833\n",
      "0.068632548284\n",
      "0.0564611598896\n",
      "0.0524765378926\n",
      "0.0495100169635\n",
      "0.0636476270479\n",
      "0.0677974214521\n",
      "0.0761119370874\n",
      "0.0929839264007\n",
      "0.0946677757861\n",
      "0.0656958273638\n",
      "0.0636050425179\n",
      "0.0780135494195\n",
      "0.0905854727073\n",
      "0.0737870085463\n",
      "0.0940607756468\n",
      "0.0516727120403\n",
      "0.09866035659\n",
      "0.0626552806238\n",
      "0.0612602103395\n",
      "0.11516385037\n",
      "0.10077246993\n",
      "0.056798798008\n",
      "0.0819989214151\n",
      "0.0411523753721\n",
      "0.0855721417158\n",
      "0.0668168738188\n",
      "0.0757084797715\n",
      "0.060224429428\n",
      "0.0703684611614\n",
      "0.074112926829\n",
      "0.10668987847\n",
      "0.0708120519466\n",
      "0.0663702085619\n",
      "0.0740998693796\n",
      "0.120460584445\n",
      "0.0742092202255\n",
      "0.0855224808026\n",
      "0.0671293309371\n",
      "0.0674972216956\n",
      "0.0748632124234\n",
      "0.0525796262632\n",
      "0.0737151680277\n",
      "0.0889176696897\n",
      "0.0510857986606\n",
      "0.0811711418332\n",
      "0.0882740895473\n",
      "0.0648853903381\n",
      "0.0518351778239\n",
      "0.0873985619924\n",
      "0.0834294189413\n",
      "0.0664083504438\n",
      "0.0888788318345\n",
      "0.0463154971564\n",
      "0.0961050486116\n",
      "0.0441356277233\n",
      "0.0625801274339\n",
      "0.0737275330198\n",
      "0.0715073273335\n",
      "0.0735295402106\n",
      "0.0379846332355\n",
      "0.0741285017705\n",
      "0.0864995584373\n",
      "0.0872013807209\n",
      "0.0689995135296\n",
      "0.0732741576777\n",
      "0.111246559967\n",
      "0.063934840064\n",
      "0.0784156901033\n",
      "0.0556936923113\n",
      "0.0614890571828\n",
      "0.0583634845853\n",
      "0.0748753053656\n",
      "0.0698529483831\n",
      "0.116840363771\n",
      "0.0602545590593\n",
      "0.087971780803\n",
      "0.0505568217129\n",
      "0.059389922458\n",
      "0.0320474282309\n",
      "0.0765631959982\n",
      "0.05137591304\n",
      "0.0712270767759\n",
      "0.0248810795847\n",
      "0.0434792274439\n",
      "0.0586377483848\n",
      "0.0504503344798\n",
      "0.0334309918713\n",
      "0.066673074277\n",
      "0.052160152587\n",
      "0.0672039615173\n",
      "0.0393608376334\n",
      "0.0505645607558\n",
      "0.0626851355515\n",
      "0.0336556037643\n",
      "0.0329145940023\n",
      "0.0274750602093\n",
      "0.0363884911157\n",
      "0.0420465782646\n",
      "0.0288408704392\n",
      "0.0266072267774\n",
      "0.060423919905\n",
      "0.0594810307226\n",
      "0.0294719769527\n",
      "0.028357371236\n",
      "0.0788876575853\n",
      "0.0341733197387\n",
      "0.0533835110888\n",
      "0.0459171999677\n",
      "0.0286869602722\n",
      "0.0412466016287\n",
      "0.0353057388367\n",
      "0.0300772371996\n",
      "0.0367816631058\n",
      "0.0270086784254\n",
      "0.0192287846871\n",
      "0.0139645367055\n",
      "0.0572974617336\n",
      "0.0227350398256\n",
      "0.0168002020134\n",
      "0.00782651493836\n",
      "0.0241837568829\n",
      "0.0286508484374\n",
      "0.0314876002088\n",
      "0.0381248846688\n",
      "0.0190469346286\n",
      "0.00266839439596\n",
      "0.0146019480675\n",
      "0.0188202215665\n",
      "0.0200448371535\n",
      "0.0110869227714\n",
      "0.040823507485\n",
      "0.0369109344712\n",
      "0.0276991805572\n",
      "0.0157516026422\n",
      "0.0259809612006\n",
      "0.0104307643752\n",
      "0.0178151050172\n",
      "0.017113228837\n",
      "0.0284640767787\n",
      "0.0144095937228\n",
      "0.0163369490014\n",
      "0.0381129993458\n",
      "0.0232507437104\n",
      "0.0250249382671\n",
      "0.0133368734765\n",
      "0.0294676563761\n",
      "0.0202940912824\n",
      "0.0405526195557\n",
      "0.0134717067335\n",
      "0.0448132274764\n",
      "0.00513697585791\n",
      "0.0110896722222\n",
      "0.0021901859178\n",
      "0.0214037748004\n",
      "0.0208666535839\n",
      "0.0224006366105\n",
      "0.0152851667717\n",
      "0.0120555683378\n",
      "0.0190839397589\n",
      "0.00978106033584\n",
      "0.0167280596082\n",
      "0.0181292332315\n",
      "0.018037299344\n",
      "0.0227475934771\n",
      "0.0140228903717\n",
      "0.0195940947936\n",
      "0.0220959542582\n",
      "0.0277942505925\n",
      "0.0123019859586\n",
      "0.00413811012036\n",
      "0.0344938036018\n",
      "0.0155043473455\n",
      "0.0179753118205\n",
      "0.00584614688305\n",
      "0.0214683587614\n",
      "0.0306115609142\n",
      "0.0118780661695\n",
      "0.0174982775318\n",
      "0.0228503646797\n",
      "0.0130943017734\n",
      "0.00763469990669\n",
      "0.0213946793877\n",
      "0.0282761867821\n",
      "0.0228556942088\n",
      "0.0199793343419\n",
      "0.0204811427703\n",
      "0.0181125070379\n",
      "0.0135598417859\n",
      "0.00651862600304\n",
      "0.0269861394439\n",
      "0.00720801985597\n",
      "0.005143776585\n",
      "0.0141500727312\n",
      "0.0172850732698\n",
      "0.0101824125495\n",
      "0.00901865269033\n",
      "0.0116499517228\n",
      "0.0325111948307\n",
      "0.0099102203447\n",
      "0.00851051674393\n",
      "0.0150204035408\n",
      "0.0205926330412\n",
      "0.0239510900659\n",
      "0.022491329868\n",
      "0.0238099188081\n",
      "0.0210377416862\n",
      "0.0202130359391\n",
      "0.00410518323622\n",
      "0.0208869011477\n",
      "0.0184574394613\n",
      "0.0188978897815\n",
      "0.0061299173473\n",
      "0.00653636696522\n",
      "0.0105154370539\n",
      "0.00916287380348\n",
      "0.00823112419908\n",
      "0.0148944864638\n",
      "0.014437415081\n",
      "0.0268961496364\n",
      "0.00787047641916\n",
      "0.0141692248216\n",
      "0.00983114414449\n",
      "0.00650606483844\n",
      "0.0105070499871\n",
      "0.02593399956\n",
      "0.0100286722537\n",
      "0.0241287275254\n",
      "0.0205290748018\n",
      "0.0134795090114\n",
      "0.0102844617596\n",
      "0.0108477760424\n",
      "0.00846313791839\n",
      "0.0260652016527\n",
      "0.0113486777279\n",
      "0.0307796766137\n",
      "0.0238264323712\n",
      "0.00291581629696\n",
      "0.00664386786335\n",
      "0.0266142687464\n",
      "0.038323672559\n",
      "0.0216818177849\n",
      "0.01733140484\n",
      "0.013699735158\n",
      "0.0130052145142\n",
      "0.00981272076397\n",
      "0.00772359670272\n",
      "0.0122010876254\n",
      "0.0214115338643\n",
      "0.0255661434002\n",
      "0.0131192078402\n",
      "0.0149784060087\n",
      "0.024403084847\n",
      "0.00794777888955\n",
      "0.0258402205905\n",
      "0.0252644825824\n",
      "0.00505235757898\n",
      "0.00891668752229\n",
      "0.0247023679165\n",
      "0.00404056261305\n",
      "0.0249401904463\n",
      "0.00232232832418\n",
      "0.0122033834147\n",
      "0.0131609796816\n",
      "0.0131985146222\n",
      "0.0214663652092\n",
      "0.0311585237986\n",
      "0.00213851056249\n",
      "0.00207369586278\n",
      "0.00288758517728\n",
      "0.00241014487995\n",
      "0.0132926139745\n",
      "0.00503858713241\n",
      "0.00422311907847\n",
      "0.0305728886124\n",
      "0.000923354769311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.102813465923\n",
      "0.069134901541\n",
      "0.0847126931767\n",
      "0.103637177371\n",
      "0.0969175061836\n",
      "0.0764619168871\n",
      "0.0388029077347\n",
      "0.0815499171207\n",
      "0.095091319777\n",
      "0.0715290070043\n",
      "0.0528054835835\n",
      "0.116350504607\n",
      "0.0360650844415\n",
      "0.0836412199602\n",
      "0.0739857675206\n",
      "0.0598718626689\n",
      "0.0450245035129\n",
      "0.0719322437442\n",
      "0.0792609432475\n",
      "0.0567244339984\n",
      "0.0861215830739\n",
      "0.049741675036\n",
      "0.0335622903731\n",
      "0.0531654091239\n",
      "0.0411361942275\n",
      "0.0519836866716\n",
      "0.0625792863591\n",
      "0.0584210031559\n",
      "0.0394729925007\n",
      "0.0578563300691\n",
      "0.0799957142445\n",
      "0.0448550787708\n",
      "0.0394481566613\n",
      "0.071763505004\n",
      "0.0564867108691\n",
      "0.0506661304871\n",
      "0.0884094372286\n",
      "0.0417788854262\n",
      "0.0622831216549\n",
      "0.0612242261689\n",
      "0.0380254904039\n",
      "0.0433379622559\n",
      "0.0430570602096\n",
      "0.0684946219621\n",
      "0.0707898407575\n",
      "0.0649314730684\n",
      "0.0692876795986\n",
      "0.0816534202928\n",
      "0.0555327667953\n",
      "0.0633375651327\n",
      "0.0708648670951\n",
      "0.0589084545937\n",
      "0.0353389189003\n",
      "0.0324951759378\n",
      "0.0431048988636\n",
      "0.0478737353199\n",
      "0.030330494062\n",
      "0.0781655814525\n",
      "0.056915610097\n",
      "0.0430958843582\n",
      "0.0260636767687\n",
      "0.0429865033858\n",
      "0.0379893893558\n",
      "0.0391324826612\n",
      "0.0320777047047\n",
      "0.0422798279846\n",
      "0.0296859093263\n",
      "0.0596904565157\n",
      "0.04596886626\n",
      "0.0478885964129\n",
      "0.036453710115\n",
      "0.0323103474515\n",
      "0.0395234403895\n",
      "0.0167033173961\n",
      "0.0322594155597\n",
      "0.036023539863\n",
      "0.0158750046774\n",
      "0.0178723455021\n",
      "0.03850726942\n",
      "0.0211018754577\n",
      "0.0196474830051\n",
      "0.0472532083393\n",
      "0.0332579056248\n",
      "0.0184559986496\n",
      "0.0370222601662\n",
      "0.0429923243548\n",
      "0.0577861959455\n",
      "0.0428640193526\n",
      "0.017683107272\n",
      "0.031610462358\n",
      "0.0231724730024\n",
      "0.0481982630339\n",
      "0.0285168998888\n",
      "0.0263757847871\n",
      "0.036070060217\n",
      "0.0255358377335\n",
      "0.0378741285784\n",
      "0.025315727444\n",
      "0.033955772785\n",
      "0.0123894955103\n",
      "0.0304035075973\n",
      "0.0455893447729\n",
      "0.0168549109328\n",
      "0.0242918926225\n",
      "0.01131166002\n",
      "0.0438228016087\n",
      "0.0351853573042\n",
      "0.0206622971742\n",
      "0.0148971113504\n",
      "0.0386864680717\n",
      "0.0138640474961\n",
      "0.0159942839574\n",
      "0.0204284734535\n",
      "0.0188479796132\n",
      "0.0325563618965\n",
      "0.0118674474186\n",
      "0.0472027461299\n",
      "0.0182930823297\n",
      "0.0327144401005\n",
      "0.0205211258192\n",
      "0.0326291361171\n",
      "0.010653430654\n",
      "0.0167675247168\n",
      "0.0352470189895\n",
      "0.020507833224\n",
      "0.0162280944048\n",
      "0.0341818032387\n",
      "0.0154009362341\n",
      "0.0366275735447\n",
      "0.020992883798\n",
      "0.0416950931533\n",
      "0.00787832728157\n",
      "0.00652052406478\n",
      "0.00392777151715\n",
      "0.0155861092907\n",
      "0.0167553381196\n",
      "0.0211989986819\n",
      "0.0235273849515\n",
      "0.0167626200085\n",
      "0.0180865960368\n",
      "0.0181599762687\n",
      "0.0213982837573\n",
      "0.0143896429114\n",
      "0.00723064994018\n",
      "0.0217701790856\n",
      "0.0295043814978\n",
      "0.0141948274091\n",
      "0.018707386424\n",
      "0.0115137056487\n",
      "0.0181347513363\n",
      "0.0108602872773\n",
      "0.0123343976151\n",
      "0.0217721922026\n",
      "0.0207555780784\n",
      "0.0299009109089\n",
      "0.0219455112429\n",
      "0.0275213409106\n",
      "0.012364099194\n",
      "0.0238140516442\n",
      "0.0203442496188\n",
      "0.00975321415643\n",
      "0.012010917298\n",
      "0.00575117235963\n",
      "0.00883621157241\n",
      "0.00924280765824\n",
      "0.0398283437644\n",
      "0.0215165036683\n",
      "0.0367048417525\n",
      "0.00834897319532\n",
      "0.0193249271791\n",
      "0.00570638257455\n",
      "0.00495702522735\n",
      "0.00495809124268\n",
      "0.00953376547055\n",
      "0.0276044405279\n",
      "0.00447702190246\n",
      "0.000835892019383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_corrs = [0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01, 0.005, 0.001]\n",
    "k_feats = [5, 10]#, 100, 1000]\n",
    "\n",
    "results = dict(mean_corr=[], k_feat=[], acc=[])\n",
    "\n",
    "for mean_corr in mean_corrs:\n",
    "    results['mean_corr'].extend([mean_corr] * len(k_feats))\n",
    "    # print(\"Meancorr: %.2f\" % mean_corr)\n",
    "    for k_feat in k_feats:\n",
    "        results['k_feat'].append(k_feat)\n",
    "        # print(\"k feat: %i\" % k_feat)\n",
    "        acc = simulate_and_classify_data_constrained(10, 100, k_feat, 10, mean_corr)\n",
    "        results['acc'].append(acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1a22064978>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUYAAAEYCAYAAAAgU193AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22278c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.factorplot(data=pd.DataFrame(results), x='mean_corr', y='acc', hue='k_feat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on how we simulate data\n",
    "In this notebook's simulations, we are going to simulate three variables, with $N$ denoting the number of samples, and $K$ denoting the number of features:\n",
    "\n",
    "- **y**: an $N\\times 1$ vector with values {0, 1}\n",
    "- **c**: an $N\\times 1$ vector with values {0, 1} if continuous, else $c \\sim \\mathcal{N}(0,\\,1)$\n",
    "- **X**: an $N\\times K$ matrix, in each each column $j$ is generated as $X_{j} = \\beta_{1} c + \\beta_{2} y + \\epsilon,\\ \\epsilon \\sim \\mathcal{N}(0, 1)$\n",
    "\n",
    "By tweaking the parameters $\\beta c$ and $\\beta y$, we can specifically \"tune\" how much variance of $y$ is explained by the \"true\" (i.e., unconfounded) signal (which we call $signal\\ R^2$) and how much variance of $y$ is explained by the \"confounded signal\" (i.e., signal in $X$ which is confounded by $c$; we call this $confound\\ R^2$). \n",
    "\n",
    "Basically, in our data-generator (function is defined below) we generate data ($X$, $y$, and $c$) with a specified correlation between $y$ and $c$, $\\rho(cy)$, and subsequently calculate the $confound\\ R^2$ and $signal\\ R^2$ terms. We then check whether these terms correspond to the desired values for these terms; if not, we adjust the generative parameters ($\\beta_{1}$ and $\\beta_{2}$) slightly and try the entire process again until we get the terms that we want.\n",
    "\n",
    "Below, we visualized the $confound\\ R^2$ and $signal\\ R^2$ terms in a Venn-diagram. On the right of the diagram, we describe the steps to get these terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b8fb7ead515e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             '001': ''}\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvenn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "mappings = {'100': 'A', '110': 'D', '010': '',\n",
    "            '101': \"B\\n($signal\\ R^2$)\", '111': 'C\\n($confound\\ R^2$)', '011': 'E',\n",
    "            '001': ''}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "v = venn3(subsets=(1, 1, 0.1, 1, 0.3, 0.1, 0.6), set_labels=('y', 'c', 'X'))\n",
    "\n",
    "for key, value in mappings.items():\n",
    "    v.get_label_by_id(key).set_text(value)\n",
    "    v.get_label_by_id(key).set_size(15)\n",
    "    \n",
    "for label in ['A', 'B', 'C']:\n",
    "    v.get_label_by_id(label).set_size(50)\n",
    "    \n",
    "plt.text(0.7, 0.5, \"$A+B+C+D = 1$\", fontsize=25)\n",
    "plt.text(0.7, 0.4, r\"$B+C+D = \\rho(y.Xc)^2$\", fontsize=25)\n",
    "plt.text(0.7, 0.3, \"$A = 1 - (B+C+D)$\", fontsize=25)\n",
    "plt.text(0.7, 0.2, r\"$C+D = \\rho(y.c)^2$\", fontsize=25)\n",
    "plt.text(0.7, 0.1, \"$B = (B+C+D) - (C+D)$\", fontsize=25)\n",
    "plt.text(0.7, 0.0, r\"$C+E = \\rho(X.c)^2$\", fontsize=25)\n",
    "plt.text(0.7, -.1, \"$D = (B+C+D) - (B+C)$\", fontsize=25)\n",
    "plt.text(0.7, -.2, \"$C = 1 - (A+B+D)$\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some functions\n",
    "Below, we define our main data-generation function - `generate_data` - which generates three variables - $X$, $y$, $c$ - corresponding to our predictors, target, and confound, respectively. (But when setting `c=None`, you can also only simulate $X$ and $y$ *without* $c$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile generate_data.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_r2(iv, dv, stack_intercept=True):\n",
    "    \"\"\" Regress dv onto iv and return r-squared.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    iv : numpy array\n",
    "        Array of shape N (samples) x K (features)\n",
    "    dv : numpy array\n",
    "        Array of shape N (samples) x 1\n",
    "    stack_intercept : bool\n",
    "        Whether to stack an intercept (vector with ones of length N).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r2 : float\n",
    "        R-squared model fit.\n",
    "    \"\"\"\n",
    "    \n",
    "    if iv.ndim == 1:\n",
    "        # Add axis if shape is (N,)\n",
    "        iv = iv[:, np.newaxis]\n",
    "    \n",
    "    if stack_intercept:\n",
    "        iv = np.hstack((np.ones((iv.shape[0], 1)), iv))\n",
    "    \n",
    "    beta = np.linalg.lstsq(iv, dv)[0]\n",
    "    dv_hat = iv.dot(beta).squeeze()\n",
    "    r2 = pearsonr(dv_hat, dv)[0] ** 2\n",
    "    if np.isnan(r2):\n",
    "        r2 = 0\n",
    "    \n",
    "    return r2\n",
    "\n",
    "\n",
    "\n",
    "def generate_data(n_samp, k_feat, c_type, corr_cy, signal_r2, confound_r2=None, verbose=False):\n",
    "    \"\"\" Generate data with known (partial) R2 \"structure\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samp : int\n",
    "        Number of samples (N) in the data (X, y, and c)\n",
    "    k_feat : int\n",
    "        Number of features (K) in the data (X)\n",
    "    c_type : str\n",
    "        Either \"continuous\" or \"categorical\". If categorical,\n",
    "        the data a balanced vector with ones and zeros\n",
    "    corr_cy : float\n",
    "        Number between -1 and 1, specifying the correlation\n",
    "        between the confound (c) and the target (y)\n",
    "    signal_r2 : float\n",
    "        Number between 0 and 1, specifying the explained variance\n",
    "        of y using X, independent of the confound contained in X;\n",
    "        (technically, the semipartial correlation rho(xy.c)\n",
    "    confound_r2 : float or None\n",
    "        Number between 0 and 1 (or None), specifying the shared variance \n",
    "        explained of y of x and c (i.e. the explained variance \n",
    "        of the confound-related information in x). If None,\n",
    "        no confound R2 will be left unspecified (which can be used\n",
    "        to specify a baseline).\n",
    "    verbose : bool\n",
    "        Whether to print (extra) relevant information\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values,\n",
    "        depending on what you set for the `c_type` argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    if n_samp % 2 != 0:\n",
    "        raise ValueError(\"Please select an even number of samples \"\n",
    "                         \"(Makes things easier.)\")\n",
    "\n",
    "    if confound_r2 is not None:\n",
    "        if np.abs(corr_cy) < np.sqrt(confound_r2):\n",
    "            raise ValueError(\"The desired corr_cy value is less than the square \"\n",
    "                             \"root of the desired confound R-squared ... This is \"\n",
    "                             \"impossible to generate.\")\n",
    "        \n",
    "    # Generate y (balanced, 50% class 0, 50% class 1)\n",
    "    y = np.repeat([0, 1], repeats=n_samp / 2)\n",
    "    \n",
    "    # Generate c (confound), with given correlation corr_cy\n",
    "    if c_type == 'categorical':\n",
    "        # Simply shift (\"roll\") y to create correlation using the \"formula\":\n",
    "        # to-shift = N / 4 * (1 - corr_cy)\n",
    "        to_roll = int((n_samp / 4) * (1 - corr_cy))\n",
    "        c = np.roll(y, to_roll)\n",
    "    elif c_type == 'continuous':\n",
    "        # If c is continuous, just sample y + random noise\n",
    "        noise_factor = 100\n",
    "        c = y + np.random.randn(n_samp) * noise_factor\n",
    "        corr = pearsonr(c, y)[0]\n",
    "        \n",
    "        while np.abs(corr - corr_cy) > 0.01:\n",
    "            # Decrease noise if the difference is too big\n",
    "            noise_factor -= 0.01\n",
    "            c = y + np.random.randn(n_samp) * noise_factor\n",
    "            corr = pearsonr(c, y)[0]        \n",
    "    else:\n",
    "        raise ValueError(\"For c_type, please select from {'continuous', \"\n",
    "                         \"'categorical'}\")\n",
    "    \n",
    "    # Define X as a matrix of N-samples by K-features\n",
    "    X = np.zeros((n_samp, k_feat))\n",
    "    \n",
    "    # Pre-allocate arrays for average signal_r2 values and confound_r2 values\n",
    "    signal_r2_values = np.zeros(k_feat)\n",
    "    confound_r2_values = np.zeros(k_feat)\n",
    "    \n",
    "    icept = np.ones((n_samp, 1))\n",
    "    \n",
    "    iterator = tqdm_notebook(np.arange(k_feat)) if verbose else np.arange(k_feat)\n",
    "    for i in iterator:\n",
    "        \n",
    "        # Define generative parameters (gen_beta_y = beta-parameter for y in model of X)\n",
    "        # Upon advice of Steven S., 'reset' generative parameters after each generation\n",
    "        gen_beta_y = 1\n",
    "        gen_beta_c = 1\n",
    "        noise_factor = 1\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            should_continue = False\n",
    "            \n",
    "            # Generate X as a linear combination of y, c, and random noise\n",
    "            this_c = 0 if confound_r2 is None else c\n",
    "            this_X = (gen_beta_y * y + gen_beta_c * this_c + np.random.randn(n_samp) * noise_factor)\n",
    "            this_X = this_X[:, np.newaxis]\n",
    "            \n",
    "            # Fit y = b1X\n",
    "            y_x_r2 = get_r2(iv=this_X, dv=y, stack_intercept=True)  # B + C\n",
    "            \n",
    "            # Increase/decrease noise if difference observed r(yx)**2 is too big/small,\n",
    "            # because if y_x_r2 < (signal_r2 + confound_r2), you won't find proper data anyway\n",
    "            tmp_confound_r2 = 0 if confound_r2 is None else confound_r2\n",
    "            difference_obs_vs_desired = y_x_r2 - (signal_r2 + tmp_confound_r2)\n",
    "            if np.abs(difference_obs_vs_desired) > 0.001:\n",
    "                # If correlation too small/big, adjust noise factor and CONTINUE\n",
    "                if difference_obs_vs_desired < 0:\n",
    "                    noise_factor -= 0.01\n",
    "                else:\n",
    "                    noise_factor += 0.01\n",
    "                continue\n",
    "            \n",
    "            if confound_r2 is None:\n",
    "                # We don't care about confound_r2\n",
    "                unique_var_x = y_x_r2\n",
    "            else:\n",
    "                # Fit y = b1X + b2C\n",
    "                y_xc_r2 = get_r2(iv=np.hstack((this_X, c[:, np.newaxis])), dv=y,\n",
    "                             stack_intercept=True)  # B + C + D\n",
    "                resid_y = 1 - y_xc_r2  # A = 1 - (B + C + D)\n",
    "\n",
    "                # Fit y = b1C\n",
    "                y_c_r2 = get_r2(iv=c, dv=y, stack_intercept=True)  # C + D\n",
    "                unique_var_x = y_xc_r2 - y_c_r2  # B\n",
    "            \n",
    "            # Increase/decrease generative param for y if difference \n",
    "            # r(yx.c) is too small/big ...\n",
    "            difference_obs_vs_desired = unique_var_x - signal_r2\n",
    "            if np.abs(difference_obs_vs_desired) > 0.01:\n",
    "                if difference_obs_vs_desired < 0:\n",
    "                    gen_beta_y += 0.01\n",
    "                else:\n",
    "                    gen_beta_y -= 0.01\n",
    "                \n",
    "                if confound_r2 is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    should_continue = True\n",
    "            else:\n",
    "                if confound_r2 is None:\n",
    "                    break\n",
    "                    \n",
    "            unique_var_c = y_xc_r2 - y_x_r2  # D\n",
    "            shared_var_xc = 1 - resid_y - unique_var_x - unique_var_c  # C\n",
    "\n",
    "            # Also check if shared variance of c and x (component C) is appropriate;\n",
    "            # if not, adjust generative parameter and CONTINUE\n",
    "            difference_obs_vs_desired = shared_var_xc - confound_r2\n",
    "            if np.abs(difference_obs_vs_desired) > 0.01:\n",
    "                if difference_obs_vs_desired < 0:\n",
    "                    gen_beta_c += 0.01\n",
    "                else:\n",
    "                    gen_beta_c -= 0.01\n",
    "                should_continue = True\n",
    "            \n",
    "            if should_continue:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # If we didn't encounter a \"break\" statement, we must have found\n",
    "        # data with the correct specifications ...\n",
    "        X[:, i] = this_X.squeeze()\n",
    "        signal_r2_values[i] = unique_var_x\n",
    "        if confound_r2 is not None:\n",
    "            confound_r2_values[i] = shared_var_xc\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Signal r2: %.3f\" % signal_r2_values.mean())\n",
    "        \n",
    "        if confound_r2 is not None:\n",
    "            print(\"Confound r2: %.3f\" % confound_r2_values.mean())\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(np.corrcoef(X.T), aspect='auto', cmap='RdBu')\n",
    "        plt.title(\"Correlations between features\")\n",
    "        plt.colorbar()\n",
    "        plt.grid('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Signal R2 values\")\n",
    "        plt.hist(signal_r2_values, bins='auto')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Confound R2 values\")\n",
    "        plt.hist(confound_r2_values, bins='auto')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if confound_r2 is None:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X, y, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing!\n",
    "data_args = dict(n_samp=240, k_feat=10, c_type='categorical', corr_cy=0.8,\n",
    "                 signal_r2=0.01, confound_r2=0.0, verbose=True)\n",
    "X, y, c = generate_data(**data_args)\n",
    "res = run_without_confound_control(X, y, c, pipeline, n_splits, data_args)\n",
    "res['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = np.zeros(100)\n",
    "for i in range(100):\n",
    "    x = np.random.normal(0, 1, (100, 26))\n",
    "    corrs[i] = np.max([np.abs(pearsonr(x[:, i], x[:, -1])[0]) for i in range(x.shape[1] - 1)])\n",
    "print(np.abs(corrs).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define several functions to quickly run classification analyses using different confound-strategies we're evaluating in this notebook:\n",
    "\n",
    "- **none**: just run a classification analysis (predict $y$ from $X$) without treating the confound ($c$)\n",
    "- **wdcr** (whole-dataset confound regression): regress out $c$ from $X$ on the whole dataset, then run a (cross-validated) classification analysis\n",
    "- **fwcr** (foldwise confound regression): first regress out $c$ from $X$ per fold, then run a classification analysis (predict $y$ from $X$)\n",
    "- **cb** (counterbalance): first subsample $X$ and $y$ until $\\rho(yc) \\approx 0$, then run a classification analysis (predict $y$ from $X$; also, it's made sure that $\\rho(yc) = 0$ also in *each fold*!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_without_confound_control(X, y, c, pipeline, n_splits, arg_dict):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['none'] * n_splits\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    scores = np.zeros(n_splits)\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        scores[i] = pipeline.score(X_test, y_test)\n",
    "        \n",
    "    results['score'] = scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_with_wholedataset_confound_regression(X, y, c, pipeline, n_splits, arg_dict):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['wdcr'] * n_splits\n",
    "    \n",
    "    # Regress out c from X\n",
    "    cr = ConfoundRegressor(confound=c, fit_idx=np.arange(y.size), cross_validate=True)\n",
    "    X = cr.fit_transform(X)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    scores = np.zeros(n_splits)\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        scores[i] = pipeline.score(X_test, y_test)\n",
    "        \n",
    "    results['score'] = scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_with_confound_regression(X, y, c, pipeline, n_splits, arg_dict):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : nu1mpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    " \n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['fwcr'] * n_splits\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    scores = np.zeros(n_splits)\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        \n",
    "        cr = ConfoundRegressor(confound=c, fit_idx=train_idx, cross_validate=True)\n",
    "        this_pipe = deepcopy(pipeline).steps\n",
    "        this_pipe.insert(0, ('regress', cr))\n",
    "        this_pipe = Pipeline(this_pipe)\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test, y_test = X[test_idx], y[test_idx]\n",
    "        this_pipe.fit(X_train, y_train)\n",
    "        scores[i] = this_pipe.score(X_test, y_test)\n",
    "        \n",
    "    results['score'] = scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_with_counterbalancing(X, y, c, pipeline, n_splits, arg_dict, \n",
    "                              c_type='categorical', metric='corr', threshold=0.01, verbose=False):\n",
    "    \"\"\" Run a classification analysis using without controlling for confounds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array\n",
    "        Array of shape N (samples) x K (features) with floating point numbers\n",
    "    y : numpy array\n",
    "        Array of shape N (samples) x 1 with binary numbers {0, 1}\n",
    "    c : numpy array\n",
    "        Array of shape N (samples) x 1 with either binary {0, 1}\n",
    "        or continuous (from normal dist, 0 mean, 1 variance) values\n",
    "    pipeline : Pipeline-object\n",
    "        A scikit-learn Pipeline-object\n",
    "    n_splits : int\n",
    "        Number of splits to generate in the K-fold routine\n",
    "    arg_dict : dict\n",
    "        Dictionary with arguments used in data generation\n",
    "        (i.e. args fed to generate_data function)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas DataFrame\n",
    "        DataFrame with data parameters (from arg-dict) and fold-wise scores.\n",
    "    \"\"\"\n",
    "\n",
    "    results = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(n_splits)])\n",
    "    results['method'] = ['cb'] * n_splits\n",
    "\n",
    "    results_corr = pd.concat([pd.DataFrame(arg_dict, index=[i]) for i in range(arg_dict['k_feat']*2)])\n",
    "    results_corr['ki'] = np.tile(np.arange(arg_dict['k_feat']), reps=2)\n",
    "    results_corr['before_after'] = ['before'] * arg_dict['k_feat'] + ['after'] * arg_dict['k_feat']\n",
    "    \n",
    "    corrs_xc_before = [pearsonr(c, X[:, i])[0] for i in range(X.shape[1])]\n",
    "    corrs_xy_before = [pearsonr(y, X[:, i])[0] for i in range(X.shape[1])]\n",
    "    \n",
    "    skf = CounterbalancedStratifiedSplit(X, y, c, n_splits=n_splits, c_type=c_type, verbose=verbose)\n",
    "    try:\n",
    "        skf.check_counterbalance_and_subsample()\n",
    "    except ValueError as ve:\n",
    "        results['score'] = np.zeros(n_splits)\n",
    "        corrs_xc_after = np.zeros_like(corrs_xc_before)\n",
    "        corrs_xy_after = np.zeros_like(corrs_xy_before)\n",
    "        results_corr['cx'] = np.concatenate((corrs_xc_before, corrs_xc_after))\n",
    "        results_corr['xy'] = np.concatenate((corrs_xy_before, corrs_xy_after))\n",
    "        return results, results_corr\n",
    "\n",
    "    X, y, c = X[skf.subsample_idx], y[skf.subsample_idx], c[skf.subsample_idx]    \n",
    "    corrs_xc_after = np.array([pearsonr(c, X[:, i])[0] for i in range(X.shape[1])])\n",
    "    corrs_xy_after = np.array([pearsonr(y, X[:, i])[0] for i in range(X.shape[1])])\n",
    "    results_corr['cx'] = np.concatenate((corrs_xc_before, corrs_xc_after))\n",
    "    results_corr['xy'] = np.concatenate((corrs_xy_before, corrs_xy_after))\n",
    "    \n",
    "    scores = np.zeros(n_splits)\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train, y_train, c_train = X[train_idx], y[train_idx], c[train_idx]\n",
    "        X_test, y_test, c_test = X[test_idx], y[test_idx], c[test_idx]\n",
    "        pipeline.fit(np.hstack((X_train, c_train[:, np.newaxis])), y_train)\n",
    "        scores[i] = pipeline.score(np.hstack((X_test, c_test[:, np.newaxis])), y_test)\n",
    "    \n",
    "    results['score'] = scores\n",
    "    \n",
    "    return results, results_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed parameters / settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "n_splits = 10  # i.e. 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: below-chance accuracy in whole dataset confound regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_args = dict(n_samp=240, k_feat=5, c_type='categorical', corr_cy=None,\n",
    "                 signal_r2=0.1, confound_r2=None, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: what happens when we vary *confound $R^2$* and *$\\rho(cy)$*?\n",
    "... or, in other words, what happens when we increase the amount of confounded \"signal\" in X (area *C* in the venn-diagram) and the initial correlation between the confound (c) and the target (y)?\n",
    "\n",
    "We evaluate the following values for the parameter $\\rho(cy)$:\n",
    "- $\\rho(cy) \\in \\{0.25, 0.3, 0.35..., 0.85\\}$\n",
    "\n",
    "We set the range of values (in steps of 0.05) for $confound\\ R^2$ dynamically, because the maximum value for $confound\\ R^2$ depends on the $\\rho(cy)$ parameter. For example, we cannot set $confound\\ R^2 = 0.5$ when $\\rho(cy) = 0.6$, because $\\sqrt{0.5} > 0.6$. Thus:\n",
    "\n",
    "- $confound\\ R^2 \\in \\{0.00, 0.05, ..., \\rho(cy)^2 - (\\rho(cy)^2\\ mod\\ 0.05)\\}$\n",
    "\n",
    "And we'll use the following fixed parameters:\n",
    "- $N = 240$\n",
    "- $K = 5$\n",
    "- $signal\\ R^2=0.1$\n",
    "\n",
    "Importantly, we also track the correlations $\\rho(xy)$ and $\\rho(xc)$ before and after counterbalancing, because there is something weird going on (as we'll explain later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound = categorical\n",
    "Here, the confound is categorical with values {0, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We do it three times for robustness\n",
    "simulations = 1\n",
    "\n",
    "# Specify arguments for data generations (we'll set corr_cy and confound_r2 later)\n",
    "data_args = dict(n_samp=240, k_feat=5, c_type='categorical', corr_cy=None,\n",
    "                 signal_r2=0.1, confound_r2=None, verbose=False)\n",
    "\n",
    "# The values for corr_cy to loop over\n",
    "corr_cy_vec = [0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85]\n",
    "\n",
    "# The confound_r2 values to loop over\n",
    "confound_r2_vecs = [np.arange(0, corr_cy ** 2, 0.05)\n",
    "                    for corr_cy in corr_cy_vec]\n",
    "\n",
    "# The 'baseline scores' to keep track of (scores on data generated without\n",
    "# any influence of the confound)\n",
    "baseline_scores = np.zeros((simulations, len(corr_cy_vec)))\n",
    "\n",
    "results_sce1 = []\n",
    "results_corr_sce1 = []\n",
    "\n",
    "# Loop over simulations\n",
    "for sim in np.arange(simulations):\n",
    "\n",
    "    print(\"Simulation: %i\" % (sim + 1))\n",
    "    \n",
    "    # Loop over values for corr_cy\n",
    "    for i, corr_cy in enumerate(corr_cy_vec):\n",
    "        data_args.update(corr_cy=corr_cy)\n",
    "        data_args.update(confound_r2=None)\n",
    "        Xbase, ybase = generate_data(**data_args) \n",
    "        baseline_scores[sim, i] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "\n",
    "        confound_r2_vec = confound_r2_vecs[i]\n",
    "        print(\"corr_cy: %.3f, confound_r2: %r\" % (corr_cy, np.around(confound_r2_vec, 3).tolist()))\n",
    "        \n",
    "        # Loop over values for confound_r2\n",
    "        for ii, confound_r2 in enumerate(confound_r2_vec):\n",
    "            data_args.update(confound_r2=confound_r2)\n",
    "            X, y, c = generate_data(**data_args)\n",
    "            results_sce1.append(run_without_confound_control(X, y, c, pipeline, n_splits, data_args))\n",
    "            results_sce1.append(run_with_wholedataset_confound_regression(X, y, c, pipeline, n_splits, data_args))\n",
    "            results_sce1.append(run_with_confound_regression(X, y, c, pipeline, n_splits, data_args))    \n",
    "            res, corrs = run_with_counterbalancing(X, y, c, pipeline, n_splits, c_type='categorical',\n",
    "                                                   arg_dict=data_args)\n",
    "            results_corr_sce1.append(corrs)\n",
    "            results_sce1.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it! The plot consists of subplots organized in three columns and six rows. Each *row* contains the results from a different $\\rho(cy)$ setting. The first row shows results from data with $\\rho(cy) = 0.4$, the second from data with $\\rho(cy) = 0.5$, etc. \n",
    "\n",
    "In the first column, the mean accuracy across folds is plotted for a range of $confound\\ R^2$ values. Note that the *specific* range of $confound\\ R^2$ values depends on the $\\rho(cy)$ value, viz,. you cannot generate data with $confound\\ R^2 > \\rho(cy)^2$. These mean accuracies across $confound\\ R^2$ values is plotted for the three methods ('none', 'cb', and 'regress') as well as a (dashed orange) line indicating the performance if there would be no $confound\\ R^2$, only $signal\\ R^2$.\n",
    "\n",
    "The second and third rows provide some extra information about what is happening in the counterbalance method. \n",
    "The second column shows $\\rho(xc)$, i.e. the mean correlation between our features (from X) and the confound (c), both **before** subsampling (blue) and **after** subsampling (green). The third column shows $\\rho(xy)$, i.e. the mean correlation between our features (from X) and the target (y), again both **before** and **after** subsampling.\n",
    "\n",
    "Note that if the counterbalancing yields too few samples (i.e. has to undersample so much that it doesn't leave any samples), then score is set to 0 (also for the correlations $\\rho(cx)$ and $\\rho(xy)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.25)\n",
    "df_sce1 = pd.concat(results_sce1)\n",
    "df_corrs_sce1 = pd.concat(results_corr_sce1)\n",
    "fig, axes = plt.subplots(len(corr_cy_vec), 3, figsize=(15, 35), gridspec_kw={'width_ratios': [1.5, 1, 1]},\n",
    "                         sharex=True, sharey=False)\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    tmp = df_sce1[df_sce1.corr_cy == corr_cy_vec[i]]\n",
    "    axes[i][0].set_title(r\"$\\rho(cy) = %.2f$\" % corr_cy_vec[i])\n",
    "    g = sns.pointplot(x='confound_r2', y='score', hue='method',\n",
    "                   data=tmp, size=5, aspect=2,\n",
    "                   palette=\"muted\", ax=axes[i][0])\n",
    "    \n",
    "    if i != 0:\n",
    "        g.legend_.remove()\n",
    "    \n",
    "    if i != len(axes) - 1:\n",
    "        axes[i][0].set_xlabel('')\n",
    "    \n",
    "    g.set_ylim((-0.1, 1.1))\n",
    "    axes[i][0].axhline(y=0.5, c='black', ls='--')\n",
    "    axes[i][0].axhline(y=baseline_scores[:, i].mean(), c='orange', ls='--')\n",
    "\n",
    "    tmp = df_corrs_sce1[df_corrs_sce1.corr_cy == corr_cy_vec[i]]\n",
    "    axes[i][1].set_title(r'$\\rho(xc)$')\n",
    "    g = sns.barplot(x=\"confound_r2\", y=\"cx\", hue=\"before_after\", data=tmp, ax=axes[i][1])\n",
    "    g.legend_.remove()\n",
    "    axes[i][1].legend(loc='best')\n",
    "    axes[i][2].set_title(r'$\\rho(xy)$')\n",
    "    g.set_ylim((-0.6, 1))\n",
    "    g = sns.barplot(x=\"confound_r2\", y=\"xy\", hue=\"before_after\", data=tmp, ax=axes[i][2])\n",
    "    g.legend_.remove()\n",
    "    g.set_ylim((-0.6, 1))\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the blue line (confound not controlled for) increases when $confound\\ R^2$ increases, as you would expect. You also see that confound regression (green line) nicely follows the orange dashed line, which indicates the scanario in which confound plays no role. \n",
    "\n",
    "Lastly, we an interesting (disturbing?) thing happening with the counterbalancing method: when $\\rho(cy)$ increases, the counterbalancing method starts to show positive bias, i.e., scores higher than you should see when the confound is adequately dealt with. The third column of graphs shows the reason: subsampling seems to increase the correlation between the features (X) and y (i.e. $signal\\ R^2$ seems to increase)!\n",
    "\n",
    "This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {'100': 'A', '110': 'D', '010': '',\n",
    "            '101': \"B\\n($signal\\ R^2$)\", '111': 'C\\n($confound\\ R^2$)', '011': 'E',\n",
    "            '001': ''}\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1, 2, 1)\n",
    "v = venn3(subsets=(1, 1, 0.1, 1, 0.3, 0.1, 0.6), set_labels=('y', 'c', 'X'))\n",
    "\n",
    "for key, value in mappings.items():\n",
    "    v.get_label_by_id(key).set_text(value)\n",
    "    v.get_label_by_id(key).set_size(15)\n",
    "    \n",
    "for label in ['A', 'B', 'C']:\n",
    "    v.get_label_by_id(label).set_size(50)\n",
    "\n",
    "plt.subplot(1, 2, 2)    \n",
    "v = venn3(subsets=(1.5, 1.5, 0.0, 1, 1, 1, 0.0), set_labels=('y', 'c', 'X'))\n",
    "\n",
    "for key, value in mappings.items():\n",
    "    if key in ['111', '110']:\n",
    "        continue\n",
    "    v.get_label_by_id(key).set_text(value)\n",
    "    v.get_label_by_id(key).set_size(15)\n",
    "    \n",
    "for label in ['A', 'B', 'C']:\n",
    "    v.get_label_by_id(label).set_size(50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what must happen is that component **B** (right diagram) must increase with increasing $\\rho(cy)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3: what happens when we vary $\\rho(y.X\\ |\\ c)$?\n",
    "... or, in other words, what happens in we increase the true (unconfounded) signal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound = categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = 1\n",
    "\n",
    "data_args = dict(n_samp=240, k_feat=5, c_type='categorical', corr_cy=0.65,\n",
    "                 signal_r2=None, confound_r2=None, verbose=False)\n",
    "    \n",
    "signal_r2_vec = [0.00, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "confound_r2_vec = [0.00, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "baseline_scores = np.zeros((permutations, len(signal_r2_vec)))\n",
    "\n",
    "results = []\n",
    "results_corr = []\n",
    "for perm in np.arange(permutations):\n",
    "\n",
    "    print(\"Permutation: %i\" % (perm + 1))\n",
    "\n",
    "    for i, signal_r2 in enumerate(signal_r2_vec):\n",
    "        data_args.update(confound_r2=None)\n",
    "        data_args.update(signal_r2=signal_r2)\n",
    "        base_args = deepcopy(data_args)\n",
    "        Xbase, ybase = generate_data(**base_args) \n",
    "        baseline_scores[perm, i] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "        \n",
    "        for ii, confound_r2 in enumerate(confound_r2_vec):\n",
    "            data_args.update(confound_r2=confound_r2)\n",
    "            X, y, c = generate_data(**data_args)\n",
    "            results.append(run_without_confound_control(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_confound_regression(X, y, c, pipeline, n_splits, data_args))\n",
    "            res, corrs = run_with_counterbalancing(X, y, c, pipeline, n_splits, data_args, c_type='categorical',\n",
    "                                                   metric='corr', threshold=0.01)\n",
    "            results_corr.append(corrs)\n",
    "            results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.25)\n",
    "df = pd.concat(results)\n",
    "df_corrs = pd.concat(results_corr)\n",
    "fig, axes = plt.subplots(len(signal_r2_vec), 3, figsize=(15, 25), gridspec_kw={'width_ratios': [1.5, 1, 1]})\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    tmp = df[df.signal_r2 == signal_r2_vec[i]]\n",
    "    axes[i][0].set_title(\"$Signal R^2 = %.3f$\" % signal_r2_vec[i])\n",
    "    g = sns.pointplot(x='confound_r2', y='score', hue='method',\n",
    "                      data=tmp, size=5, aspect=2,\n",
    "                      palette=\"muted\", ax=axes[i][0])\n",
    "    g.legend_.remove()\n",
    "    \n",
    "    if i != len(axes) - 1:\n",
    "        axes[i][0].set_xlabel('')\n",
    "    \n",
    "    g.set_ylim((-0.1, 1.1))\n",
    "    axes[i][0].axhline(y=0.5, c='black', ls='--')\n",
    "    axes[i][0].axhline(y=baseline_scores[:, i].mean(axis=0), c='orange', ls='--')\n",
    "\n",
    "    tmp = df_corrs[df_corrs.signal_r2 == signal_r2_vec[i]]\n",
    "    axes[i][1].set_title(r'$\\rho(xc)$')\n",
    "    g = sns.barplot(x=\"confound_r2\", y=\"cx\", hue=\"before_after\", data=tmp, ax=axes[i][1])\n",
    "    g.legend_.remove()\n",
    "    axes[i][1].legend(loc='best')\n",
    "    axes[i][2].set_title(r'$\\rho(xy)$')\n",
    "    g = sns.barplot(x=\"confound_r2\", y=\"xy\", hue=\"before_after\", data=tmp, ax=axes[i][2])\n",
    "    g.legend_.remove()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same plot, but x-axis and factor (signal r2) reversed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.25)\n",
    "df = pd.concat(results)\n",
    "df_corrs = pd.concat(results_corr)\n",
    "fig, axes = plt.subplots(len(confound_r2_vec), 3, figsize=(15, 55), gridspec_kw={'width_ratios': [1.5, 1, 1]})\n",
    "\n",
    "for i in range(len(axes)):\n",
    "    tmp = df[df.confound_r2 == confound_r2_vec[i]]\n",
    "    axes[i][0].set_title(\"$Confound R^2 = %.3f$\" % confound_r2_vec[i])\n",
    "    g = sns.pointplot(x='signal_r2', y='score', hue='method',\n",
    "                      data=tmp, size=5, aspect=2,\n",
    "                      palette=\"muted\", ax=axes[i][0])\n",
    "    g.legend_.remove()\n",
    "      \n",
    "    axes[i][0].plot(np.arange(len(signal_r2_vec)), baseline_scores.mean(axis=0), color='orange', ls='--')\n",
    "    \n",
    "    if i != len(axes) - 1:\n",
    "        axes[i][0].set_xlabel('')\n",
    "    \n",
    "    g.set_ylim((-0.1, 1.1))\n",
    "    axes[i][0].axhline(y=0.5, c='black', ls='--')\n",
    "    # axes[i][0].plot(x=np.arange(6), y=baseline_scores.mean(axis=0), c='orange', ls='--')\n",
    "\n",
    "    tmp = df_corrs[df_corrs.confound_r2 == confound_r2_vec[i]]\n",
    "    axes[i][1].set_title(r'$\\rho(xc)$')\n",
    "    g = sns.barplot(x=\"signal_r2\", y=\"cx\", hue=\"before_after\", data=tmp, ax=axes[i][1])\n",
    "    g.legend_.remove()\n",
    "    axes[i][1].legend(loc='best')\n",
    "    axes[i][2].set_title(r'$\\rho(xy)$')\n",
    "    g = sns.barplot(x=\"signal_r2\", y=\"xy\", hue=\"before_after\", data=tmp, ax=axes[i][2])\n",
    "    g.legend_.remove()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound = continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "permutations = 1\n",
    "\n",
    "data_args = dict(n_samp=240, k_feat=5, c_type='continuous', corr_cy=0.8,\n",
    "                 signal_r2=None, confound_r2=None, verbose=False)\n",
    "    \n",
    "signal_r2_vec = [0.00, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "confound_r2_vec = [0.00, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]\n",
    "baseline_scores = np.zeros((permutations, len(signal_r2_vec)))\n",
    "\n",
    "results = []\n",
    "results_corr = []\n",
    "for perm in np.arange(permutations):\n",
    "\n",
    "    print(\"Permutation: %i\" % (perm + 1))\n",
    "\n",
    "    for i, signal_r2 in enumerate(signal_r2_vec):\n",
    "        print('Signal r2: %.3f' % signal_r2)\n",
    "        data_args.update(signal_r2=signal_r2)\n",
    "        base_args = deepcopy(data_args)\n",
    "        base_args['c_type'] = 'none'\n",
    "        Xbase, ybase = generate_data(**base_args) \n",
    "        baseline_scores[perm, i] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "        \n",
    "        for ii, confound_r2 in enumerate(confound_r2_vec):\n",
    "            print('Confound r2: %.3f' % confound_r2)\n",
    "            data_args.update(confound_r2=confound_r2)\n",
    "            X, y, c = generate_data(**data_args)\n",
    "            results.append(run_without_confound_control(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_confound_regression(X, y, c, pipeline, n_splits, data_args))\n",
    "            res, corrs = run_with_counterbalancing(X, y, c, pipeline, n_splits, data_args, c_type='continuous',\n",
    "                                                   metric='corr', threshold=0.01)\n",
    "            results_corr.append(corrs)\n",
    "            results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVERYTHING BELOW IS IRRELEVANT FOR NOW (LUKAS' PLAYGROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K_FEAT = 5\n",
    "data_args = dict(n_samp=240, k_feat=K_FEAT, c_type='categorical', corr_cy=0.7,\n",
    "                 signal_r2=0.1, confound_r2=0.49, verbose=False)\n",
    "X, y, c = generate_data(**data_args)\n",
    "\n",
    "corrs_xc_before = [pearsonr(c, X[:, i])[0] for i in range(X.shape[1])]\n",
    "corrs_xy_before = [pearsonr(y, X[:, i])[0] for i in range(X.shape[1])]\n",
    "\n",
    "skf = CounterbalancedStratifiedSplit(X, y, c, n_splits=10, c_type='categorical', verbose=False)\n",
    "skf.check_counterbalance_and_subsample()\n",
    "X, y, c = X[skf.subsample_idx], y[skf.subsample_idx], c[skf.subsample_idx]    \n",
    "corrs_xc_after = np.array([pearsonr(c, X[:, i])[0] for i in range(X.shape[1])])\n",
    "corrs_xy_after = np.array([pearsonr(y, X[:, i])[0] for i in range(X.shape[1])])\n",
    "\n",
    "scores = np.zeros(n_splits)\n",
    "corrs_xy_train = np.zeros((10, K_FEAT))\n",
    "corrs_xy_test = np.zeros((10, K_FEAT))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train, c_train = X[train_idx], y[train_idx], c[train_idx]\n",
    "    X_test, y_test, c_test = X[test_idx], y[test_idx], c[test_idx]\n",
    "    \n",
    "    corrs_xy_train[i, :] = np.array([pearsonr(y_train, X_train[:, i])[0] for i in range(X.shape[1])])\n",
    "    corrs_xy_test[i, :] = np.array([pearsonr(y_test, X_test[:, i])[0] for i in range(X.shape[1])])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    scores[i] = pipeline.score(X_test, y_test)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4: what happens if we vary the number of samples? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound = categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "permutations = 3\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('clf', SVC(kernel='linear'))])\n",
    "\n",
    "data_args = dict(n_samp=None, k_feat=5, c_type='categorical', corr_cy=None,\n",
    "                 signal_r2=0.1, confound_r2=0.1, verbose=False)\n",
    "\n",
    "n_samples_vec = [20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "baseline_scores = np.zeros((permutations, len(n_samples_vec)))\n",
    "\n",
    "results = []\n",
    "for perm in np.arange(permutations):\n",
    "\n",
    "    print(\"Permutation: %i\" % (perm + 1))\n",
    "    \n",
    "    Xbase, ybase = generate_data(**base_args) \n",
    "    baseline_scores[perm] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "\n",
    "    for i, n_samp in enumerate(n_samples_vec):\n",
    "        data_args.update(n_samp=n_samp)\n",
    "        \n",
    "        base_args = deepcopy(data_args)\n",
    "        base_args['c_type'] = 'none'\n",
    "        Xbase, ybase = generate_data(**base_args) \n",
    "        baseline_scores[perm, i] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "        \n",
    "        for corr_cy in [0.4, 0.5, 0.6, 0.7]:\n",
    "            data_args.update(corr_cy=corr_cy)\n",
    "            X, y, c = generate_data(**data_args)\n",
    "            results.append(run_without_confound_control(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_confound_regression(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_counterbalancing(X, y, c, pipeline, n_splits, data_args))\n",
    "            \n",
    "sns.set(font_scale=2)\n",
    "df = pd.concat(results)\n",
    "g = sns.factorplot(x='n_samp', y='score', hue='method', col='corr_cy', col_wrap=2,\n",
    "                   data=df, size=5, aspect=2, legend_out=False,\n",
    "                   palette=\"muted\", legend=True)\n",
    "g = g.map(plt.axhline, y=0.5, c='black', ls='--')\n",
    "g.set_axis_labels('N samples', 'Score')\n",
    "\n",
    "for ax in g.axes:\n",
    "    ax.plot(np.arange(len(n_samples_vec)), baseline_scores.mean(axis=0), color='red', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound = continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "permutations = 2\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('clf', SVC(kernel='linear'))])\n",
    "\n",
    "data_args = dict(n_samp=None, k_feat=5, c_type='continuous', corr_cy=None,\n",
    "                 signal_r2=0.1, confound_r2=0.1, verbose=False)\n",
    "\n",
    "n_samples_vec = [20, 50, 100, 200, 500, 1000]\n",
    "\n",
    "baseline_scores = np.zeros((permutations, len(n_samples_vec)))\n",
    "\n",
    "results = []\n",
    "for perm in np.arange(permutations):\n",
    "\n",
    "    print(\"Permutation: %i\" % (perm + 1))\n",
    "    \n",
    "    for i, n_samp in enumerate(n_samples_vec):\n",
    "        data_args.update(n_samp=n_samp)\n",
    "        \n",
    "        base_args = deepcopy(data_args)\n",
    "        base_args['c_type'] = 'none'\n",
    "        Xbase, ybase = generate_data(**base_args) \n",
    "        baseline_scores[perm, i] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "        \n",
    "        for corr_cy in [0.4, 0.5, 0.6, 0.7]:\n",
    "            data_args.update(corr_cy=corr_cy)\n",
    "            X, y, c = generate_data(**data_args)\n",
    "            results.append(run_without_confound_control(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_confound_regression(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_counterbalancing(X, y, c, pipeline, n_splits, data_args,\n",
    "                                                     c_type='continuous', metric='corr', threshold=0.01))\n",
    "            \n",
    "sns.set(font_scale=2)\n",
    "df = pd.concat(results)\n",
    "g = sns.factorplot(x='n_samp', y='score', hue='method', col='corr_cy', col_wrap=2,\n",
    "                   data=df, size=5, aspect=2, legend_out=False,\n",
    "                   palette=\"muted\", legend=True)\n",
    "g = g.map(plt.axhline, y=0.5, c='black', ls='--')\n",
    "g.set_axis_labels('N samples', 'Score')\n",
    "for ax in g.axes:\n",
    "    ax.plot(np.arange(len(n_samples_vec)), baseline_scores.mean(axis=0), color='red', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 5: what happens when we vary the number of features?\n",
    "We also vary the strength of the confound signal in X (confound_r2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound = categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "permutations = 1\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('clf', SVC(kernel='linear'))])\n",
    "\n",
    "data_args = dict(n_samp=240, k_feat=None, c_type='categorical', corr_cy=0.7,\n",
    "                 signal_r2=0.1, confound_r2=None, verbose=False)\n",
    "\n",
    "k_feat_vec = [1, 5, 10, 20, 50, 100, 500, 1000, 10000]\n",
    "\n",
    "baseline_scores = np.zeros((permutations, len(k_feat_vec)))\n",
    "\n",
    "results = []\n",
    "for perm in np.arange(permutations):\n",
    "\n",
    "    print(\"Permutation: %i\" % (perm + 1))\n",
    "    \n",
    "    for i, k_feat in enumerate(k_feat_vec):\n",
    "        data_args.update(k_feat=k_feat)\n",
    "        print(\"k feat: %i\" % k_feat)\n",
    "        base_args = deepcopy(data_args)\n",
    "        base_args['c_type'] = 'none'\n",
    "        Xbase, ybase = generate_data(**base_args) \n",
    "        baseline_scores[perm, i] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "        \n",
    "        for confound_r2 in [0, 0.1, 0.2, 0.3, 0.4]:\n",
    "            data_args.update(confound_r2=confound_r2)\n",
    "            X, y, c = generate_data(**data_args)\n",
    "            results.append(run_without_confound_control(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_confound_regression(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_counterbalancing(X, y, c, pipeline, n_splits, data_args,\n",
    "                                                     c_type='categorical'))\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "df = pd.concat(results)\n",
    "g = sns.factorplot(x='k_feat', y='score', hue='method', col='confound_r2', col_wrap=2,\n",
    "                   data=df, size=5, aspect=2, legend_out=False,\n",
    "                   palette=\"muted\", legend=True)\n",
    "g = g.map(plt.axhline, y=0.5, c='black', ls='--')\n",
    "g.set_axis_labels('K features', 'Score')\n",
    "\n",
    "for ax in g.axes:\n",
    "    ax.plot(np.arange(len(k_feat_vec)), baseline_scores.mean(axis=0), color='red', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confound = continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "permutations = 1\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('clf', SVC(kernel='linear'))])\n",
    "\n",
    "data_args = dict(n_samp=240, k_feat=None, c_type='continuous', corr_cy=0.7,\n",
    "                 signal_r2=0.1, confound_r2=None, verbose=False)\n",
    "\n",
    "k_feat_vec = [1, 5, 10, 20, 50, 100, 500, 1000, 10000]\n",
    "\n",
    "baseline_scores = np.zeros((permfor ax in g.axes:\n",
    "    ax.plot(np.arange(len(k_feat_vec)), baseline_scores.mean(axis=0), color='red', ls='--')utations, len(k_feat_vec)))\n",
    "\n",
    "results = []\n",
    "for perm in np.arange(permutations):\n",
    "\n",
    "    print(\"Permutation: %i\" % (perm + 1))\n",
    "    \n",
    "    for i, k_feat in enumerate(k_feat_vec):\n",
    "        data_args.update(k_feat=k_feat)\n",
    "        print(\"k feat: %i\" % k_feat)\n",
    "        base_args = deepcopy(data_args)\n",
    "        base_args['c_type'] = 'none'\n",
    "        Xbase, ybase = generate_data(**base_args) \n",
    "        baseline_scores[perm, i] = cross_val_score(pipeline, Xbase, ybase, cv=n_splits).mean()\n",
    "        \n",
    "        for confound_r2 in [0, 0.1, 0.2, 0.3, 0.4]:\n",
    "            data_args.update(confound_r2=confound_r2)\n",
    "            X, y, c = generate_data(**data_args)\n",
    "            results.append(run_without_confound_control(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_confound_regression(X, y, c, pipeline, n_splits, data_args))\n",
    "            results.append(run_with_counterbalancing(X, y, c, pipeline, n_splits, data_args,\n",
    "                                                     c_type='continuous', metric='corr', threshold=0.01))\n",
    "\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "df = pd.concat(results)\n",
    "g = sns.factorplot(x='k_feat', y='score', hue='method', col='confound_r2', col_wrap=2,\n",
    "                   data=df, size=5, aspect=2, legend_out=False,\n",
    "                   palette=\"muted\", legend=True)\n",
    "g = g.map(plt.axhline, y=0.5, c='black', ls='--')\n",
    "g.set_axis_labels('K features', 'Score')\n",
    "for ax in g.axes:\n",
    "    ax.plot(np.arange(len(k_feat_vec)), baseline_scores.mean(axis=0), color='red', ls='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "X, y, c = generate_data(n_samp=240, k_feat=5, c_type='categorical', corr_cy=0.6,\n",
    "                        signal_r2=0.1, confound_r2=0.2, verbose=False)\n",
    "\n",
    "skf = CounterbalancedStratifiedSplit(X, y, c, n_splits=n_splits, c_type='categorical', verbose=True)\n",
    "skf.check_counterbalance_and_subsample()\n",
    "\n",
    "X, y, c = X[skf.subsample_idx], y[skf.subsample_idx], c[skf.subsample_idx]    \n",
    "\n",
    "scores = np.zeros(n_splits)\n",
    "y_c_r2 = np.zeros(n_splits)\n",
    "y_xc_r2 = np.zeros((n_splits, X.shape[1]))\n",
    "y_x_r2 = np.zeros((n_splits, X.shape[1]))\n",
    "unique_var_x = np.zeros((n_splits, X.shape[1]))\n",
    "unique_var_c = np.zeros((n_splits, X.shape[1]))\n",
    "shared_var_xc = np.zeros((n_splits, X.shape[1]))\n",
    "d = np.zeros((n_splits, X.shape[1]))\n",
    "\n",
    "this_pipe = pipeline\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    c_train = c[train_idx][:, np.newaxis]\n",
    "    y_c_r2[i] = get_r2(c_train, y_train)\n",
    "    \n",
    "    for ii in range(X_train.shape[1]):\n",
    "        y_xc_r2[i, ii] = get_r2(np.hstack((X_train[:, ii][:, np.newaxis], c_train)), y_train)\n",
    "        y_x_r2[i, ii] = get_r2(X_train[:, ii], y_train)\n",
    "        \n",
    "        resid_y = 1 - y_xc_r2[i, ii]  # A = 1 - (B + C + D)\n",
    "        unique_var_x[i, ii] = y_xc_r2[i, ii] - y_c_r2[i]  # B\n",
    "        unique_var_c[i, ii] = y_xc_r2[i, ii] - y_x_r2[i, ii]  # D\n",
    "        shared_var_xc[i, ii] = 1 - resid_y - unique_var_x[i, ii] - unique_var_c[i, ii]  # C = \n",
    "        d[i, ii] = y_c_r2[i] - shared_var_xc[i, ii]\n",
    "    \n",
    "    print(\"TRAIN\")\n",
    "    print(\"R2(y_xc)): %r\" % np.round(y_xc_r2.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(y_x): %r\" % np.round(y_x_r2.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(signal): %r\" % np.round(unique_var_x.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(confound): %r\" % np.round(shared_var_xc.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(D): %r\" % np.round(d.mean(axis=0), 4).tolist())\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    c_test = c[test_idx][:, np.newaxis]\n",
    "    for ii in range(X_test.shape[1]):\n",
    "        y_xc_r2[i, ii] = get_r2(np.hstack((X_test[:, ii][:, np.newaxis], c_test)), y_test)\n",
    "        y_x_r2[i, ii] = get_r2(X_test[:, ii], y_test)\n",
    "        \n",
    "        resid_y = 1 - y_xc_r2[i, ii]  # A = 1 - (B + C + D)\n",
    "        unique_var_x[i, ii] = y_xc_r2[i, ii] - y_c_r2[i]  # B\n",
    "        unique_var_c[i, ii] = y_xc_r2[i, ii] - y_x_r2[i, ii]  # D\n",
    "        shared_var_xc[i, ii] = 1 - resid_y - unique_var_x[i, ii] - unique_var_c[i, ii]  # C = \n",
    "        d[i, ii] = y_c_r2[i] - shared_var_xc[i, ii]\n",
    "    \n",
    "    print(\"TEST\")\n",
    "    print(\"R2(y_xc)): %r\" % np.round(y_xc_r2.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(y_x): %r\" % np.round(y_x_r2.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(signal): %r\" % np.round(unique_var_x.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(confound): %r\" % np.round(shared_var_xc.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(D): %r\" % np.round(d.mean(axis=0), 4).tolist())\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    \n",
    "    this_pipe.fit(X_train, y_train)\n",
    "    scores[i] = this_pipe.score(X_test, y_test)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "X, y, c = generate_data(n_samp=240, k_feat=5, c_type='categorical', corr_cy=0.4,\n",
    "                        signal_r2=0.1, confound_r2=0.1, verbose=False)\n",
    "\n",
    "scores = np.zeros(n_splits)\n",
    "y_c_r2 = np.zeros(n_splits)\n",
    "y_xc_r2 = np.zeros((n_splits, X.shape[1]))\n",
    "y_x_r2 = np.zeros((n_splits, X.shape[1]))\n",
    "unique_var_x = np.zeros((n_splits, X.shape[1]))\n",
    "unique_var_c = np.zeros((n_splits, X.shape[1]))\n",
    "shared_var_xc = np.zeros((n_splits, X.shape[1]))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "this_pipe = Pipeline(pipeline)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    cr = ConfoundRegressor(confound=c, fit_idx=train_idx, cross_validate=True)\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    c_train = c[train_idx][:, np.newaxis]\n",
    "    y_c_r2[i] = get_r2(c_train, y_train)\n",
    "\n",
    "    X_train = cr.fit_transform(X_train)\n",
    "    X_test = cr.transform(X_test)\n",
    "    \n",
    "    for ii in range(X_train.shape[1]):\n",
    "        y_xc_r2[i, ii] = get_r2(np.hstack((X_train[:, ii][:, np.newaxis], c_train)), y_train)\n",
    "        y_x_r2[i, ii] = get_r2(X_train[:, ii], y_train)\n",
    "        \n",
    "        resid_y = 1 - y_xc_r2[i, ii]  # A = 1 - (B + C + D)\n",
    "        unique_var_x[i, ii] = y_xc_r2[i, ii] - y_c_r2[i]  # B\n",
    "        unique_var_c[i, ii] = y_xc_r2[i, ii] - y_x_r2[i, ii]  # D\n",
    "        shared_var_xc[i, ii] = 1 - resid_y - unique_var_x[i, ii] - unique_var_c[i, ii]  # C = \n",
    "    \n",
    "    print(\"R2(y_xc)): %r\" % np.round(y_xc_r2.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(y_x): %r\" % np.round(y_x_r2.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(signal): %r\" % np.round(unique_var_x.mean(axis=0), 4).tolist())\n",
    "    print(\"R2(confound): %r\" % np.round(shared_var_xc.mean(axis=0), 4).tolist())\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    this_pipe.fit(X_train, y_train)\n",
    "    scores[i] = this_pipe.score(X_test, y_test)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An old version of the data generation algorithm\n",
    "Didn't really work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data_from_dist(n_samp, k_feat, c_type, corr_cy, signal_r2, confound_r2=None, verbose=False, plot=False):\n",
    "    \"\"\" Similar to above, but draw generative parameters from distribution. \"\"\"\n",
    "    \n",
    "    if n_samp % 2 != 0:\n",
    "        raise ValueError(\"Please select an even number of samples \"\n",
    "                         \"(Makes things easier.)\")\n",
    "\n",
    "    if confound_r2 is not None:\n",
    "        if np.abs(corr_cy) < np.sqrt(confound_r2):\n",
    "            raise ValueError(\"The desired corr_cy value is less than the square \"\n",
    "                             \"root of the desired confound R-squared ... This is \"\n",
    "                             \"impossible to generate.\")\n",
    "        \n",
    "    # Generate y (balanced, 50% class 0, 50% class 1)\n",
    "    y = np.repeat([0, 1], repeats=n_samp / 2)\n",
    "    \n",
    "    # Generate c (confound), with given correlation corr_cy\n",
    "    if c_type == 'categorical':\n",
    "        # Simply shift (\"roll\") y to create correlation using the \"formula\":\n",
    "        # to-shift = N / 4 * (1 - corr_cy)\n",
    "        to_roll = int((n_samp / 4) * (1 - corr_cy))\n",
    "        c = np.roll(y, to_roll)\n",
    "    elif c_type == 'continuous':\n",
    "        # If c is continuous, just sample y + random noise\n",
    "        noise_factor = 100\n",
    "        c = y + np.random.randn(n_samp) * noise_factor\n",
    "        corr = pearsonr(c, y)[0]\n",
    "        \n",
    "        while np.abs(corr - corr_cy) > 0.01:\n",
    "            # Decrease noise if the difference is too big\n",
    "            noise_factor -= 0.01\n",
    "            c = y + np.random.randn(n_samp) * noise_factor\n",
    "            corr = pearsonr(c, y)[0]        \n",
    "    else:\n",
    "        raise ValueError(\"For c_type, please select from {'continuous', \"\n",
    "                         \"'categorical'}\")\n",
    "\n",
    "    noise_factor = 0.1\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        gen_y_mean = np.random.normal(0, 1, size=1)\n",
    "        gen_c_mean = np.random.normal(0, 1, size=1)\n",
    "        gen_y_std = 0.5#np.abs(np.random.normal(0.0, 0.1, size=1))\n",
    "        gen_c_std = 0.5#np.abs(np.random.normal(0.0, 0.1, size=1))\n",
    "        \n",
    "        should_continue = False\n",
    "        gen_beta_y = np.random.normal(gen_y_mean, gen_y_std, size=(n_samp, k_feat))\n",
    "        gen_beta_c = np.random.normal(gen_c_mean, gen_c_std, size=(n_samp, k_feat))\n",
    "\n",
    "        X = (gen_beta_y * y[:, np.newaxis] + \n",
    "             gen_beta_c * c[:, np.newaxis] + \n",
    "             np.random.randn(n_samp, k_feat) * noise_factor)\n",
    "        \n",
    "        y_x_r2 = np.array([get_r2(iv=X[:, i, np.newaxis], dv=y) for i in range(k_feat)])  # B + C\n",
    "        \n",
    "        if confound_r2 is None:\n",
    "            # We don't care about confound_r2\n",
    "            unique_var_x = y_x_r2\n",
    "        else:\n",
    "            # Fit y = b1X + b2C\n",
    "            y_xc_r2 = np.array([get_r2(iv=np.hstack((X[:, i, np.newaxis], c[:, np.newaxis])), dv=y)\n",
    "                                for i in range(k_feat)])  # B + C + D\n",
    "            resid_y = 1 - y_xc_r2  # A = 1 - (B + C + D)\n",
    "\n",
    "            # Fit y = b1C\n",
    "            y_c_r2 = get_r2(iv=c, dv=y)  # C + D\n",
    "            unique_var_x = y_xc_r2 - y_c_r2  # B\n",
    "\n",
    "        # Increase/decrease generative param for y if difference \n",
    "        # r(yx.c) is too small/big ...\n",
    "        difference_obs_vs_desired = unique_var_x.mean() - signal_r2\n",
    "        if np.abs(difference_obs_vs_desired) > 0.01:\n",
    "            if difference_obs_vs_desired < 0:\n",
    "                gen_y_mean += 0.01\n",
    "            else:\n",
    "                gen_y_mean -= 0.01\n",
    "\n",
    "            if confound_r2 is None:\n",
    "                continue\n",
    "            else:\n",
    "                should_continue = True\n",
    "        else:\n",
    "            if confound_r2 is None:\n",
    "                break\n",
    "  \n",
    "        unique_var_c = y_xc_r2 - y_x_r2  # D\n",
    "        shared_var_xc = 1 - resid_y - unique_var_x - unique_var_c  # C\n",
    "\n",
    "        # Also check if shared variance of c and x (component C) is appropriate;\n",
    "        # if not, adjust generative parameter and CONTINUE\n",
    "        difference_obs_vs_desired = shared_var_xc.mean() - confound_r2\n",
    "        if np.abs(difference_obs_vs_desired) > 0.01:\n",
    "            if difference_obs_vs_desired < 0:\n",
    "                gen_c_mean += 0.05\n",
    "            else:\n",
    "                gen_c_mean -= 0.05\n",
    "            should_continue = True\n",
    "\n",
    "        if should_continue:\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Total r2: %.3f\" % y_x_r2.mean())\n",
    "                print(\"Confound r2: %.3f\" % shared_var_xc.mean())\n",
    "                print(\"Signal r2: %.3f\" % unique_var_x.mean())\n",
    "                print(\"gen_y_mean: %.2f\" % gen_y_mean)\n",
    "                print(\"gen_c_mean: %.2f\" % gen_c_mean, end='\\n\\n')\n",
    "            continue\n",
    "\n",
    "        # If we didn't encounter a \"break\" statement, we must have found\n",
    "        # data with the correct specifications ...\n",
    "        signal_r2_values = unique_var_x\n",
    "        if confound_r2 is not None:\n",
    "            confound_r2_values = shared_var_xc\n",
    "        break\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Signal r2: %.3f\" % signal_r2_values.mean())\n",
    "        \n",
    "        if confound_r2 is not None:\n",
    "            print(\"Confound r2: %.3f\" % confound_r2_values.mean())\n",
    "        \n",
    "        print(\"Final gen_y_mean: %.3f\" % gen_y_mean)\n",
    "        print(\"Final gen_c_mean: %.3f\" % gen_c_mean)\n",
    "    \n",
    "    if plot:\n",
    "        kinds = ['signal_r2'] * k_feat + ['confound_r2'] * k_feat\n",
    "        df = dict(corrs=np.hstack((signal_r2_values, confound_r2_values)), kind=kinds)\n",
    "        g = sns.FacetGrid(pd.DataFrame(df), col=\"kind\", margin_titles=True)\n",
    "        g.map(plt.hist, \"corrs\", color=\"steelblue\")\n",
    "        \n",
    "    if confound_r2 is None:\n",
    "        return X, y\n",
    "    else:\n",
    "        return X, y, c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
